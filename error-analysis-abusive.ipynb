{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver_paths = [\"./logs/abusive/hybrid/ckpt\", \"./logs/abusive/hybrid_larger_feature/ckpt\"]\n",
    "checkpoint_files = list(map(tf.train.get_checkpoint_state, saver_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[model_checkpoint_path: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid/ckpt/model-final.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid/ckpt/model-360000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid/ckpt/model-370000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid/ckpt/model-380000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid/ckpt/model-390000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid/ckpt/model-final.ckpt\",\n",
       " model_checkpoint_path: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid_larger_feature/ckpt/model-final.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid_larger_feature/ckpt/model-360000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid_larger_feature/ckpt/model-370000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid_larger_feature/ckpt/model-380000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid_larger_feature/ckpt/model-390000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/abusive/hybrid_larger_feature/ckpt/model-final.ckpt\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create session for evaluation\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Summary:\n",
      "Train: Total Positive Labels=4145 (0.3228)\n",
      "Test: Total Positive Labels=889 (0.3228)\n",
      "\n",
      "dataset passed the assertion test\n"
     ]
    }
   ],
   "source": [
    "from data.hybrid import load_data_from_file\n",
    "\n",
    "(x_train, y_train, x_test, y_test, initW, vocab) = load_data_from_file(\"abusive_valid_binary\")\n",
    "word_text_len = x_train[0][\"word\"].shape[0]\n",
    "word_vocab_size = len(vocab.vocabulary_)\n",
    "char_text_len = x_train[0][\"char\"].shape[0]\n",
    "char_vocab_size = x_train[0][\"char\"].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.hybrid import extract_from_batch\n",
    "\n",
    "batchW, batchC = extract_from_batch(x_test)\n",
    "feed_dicts = []\n",
    "feed_dicts.append({\"input/labels:0\": y_test, \"input/X_word:0\": batchW, \"input/X_char:0\": batchC, \"dropout_keep_prob:0\": 1})\n",
    "feed_dicts.append({\"input/labels:0\": y_test, \"input/X_word:0\": batchW, \"input/X_char:0\": batchC, \"dropout_keep_prob:0\": 1})\n",
    "\n",
    "output_names = [\"output/prediction\", \"output/prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/homes/jhpark/hate-speech/logs/abusive/hybrid/ckpt/model-360000.ckpt\n",
      "(2754,)\n",
      "/home/homes/jhpark/hate-speech/logs/abusive/hybrid_larger_feature/ckpt/model-360000.ckpt\n",
      "(2754,)\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i, ckpt in enumerate(checkpoint_files):\n",
    "    print(ckpt.all_model_checkpoint_paths[0])\n",
    "    tf.reset_default_graph()\n",
    "    saver = tf.train.import_meta_graph(\"{}.meta\".format(ckpt.all_model_checkpoint_paths[0]))\n",
    "\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        graph = tf.get_default_graph()\n",
    "        pred = sess.run(output_names[i] + \":0\", feed_dicts[i])\n",
    "        print(pred.shape)\n",
    "        preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=0.7592 recall=0.6985 f1=0.7276\n",
      "precision=0.7346 recall=0.7255 f1=0.7301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model.helper import calculate_metrics\n",
    "for pred in preds:\n",
    "    precision, recall, f1 = calculate_metrics(y_test, pred)\n",
    "    print(\"precision=%.4f recall=%.4f f1=%.4f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the metrics are same as the final output, we can validate that the pre-trained model has been loaded successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.preprocess import load_from_file\n",
    "data = load_from_file(\"abusive_valid_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['you have more legal rights than i do, is that ok?',\n",
       "       'most women are bad drivers just like most men are douchebag drivers equality notsexist fact latraffic',\n",
       "       'i think anthony is anti-feminazi and so am i',\n",
       "       \"and that's why they invented the term feminazi...\",\n",
       "       'kat is the biggest bitch in mkr history. mkr2015 mkr cow katthebitch',\n",
       "       \"male thor is unworthy of mjolnir so a woman is taking place. i'm not sexist but that is bull sheeeeeit\",\n",
       "       'hashtag killerblondes is the dickiest hashtag in all mkr history... seriously... why not butcherthemodels',\n",
       "       \"the world is enriched by your wisdom. i'm not sexist but i make sexist comments because it makes me lol.\",\n",
       "       '.@brittany_blade barb sheehan never says a word about being abused for 20 years. kills husband. claims abuse w no proof. walks. fems cheer.',\n",
       "       \"pro and anti blameonenotall promoting nothing but division. those of you that think you're facilitating equality, you're most certainly not\"], \n",
       "      dtype='<U148')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"x_train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from data.preprocess import load_from_file\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(pred_scores, target):\n",
    "    pred = list(map(lambda x: 1 if x >= 0.5 else 0, pred_scores))\n",
    "    precision, recall, f1 = calculate_metrics(target, pred)\n",
    "    print(\"Training: Precision=%.2f Recall=%.2f, F1=%.2f\" %(precision, recall, f1))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_with_freq(data):\n",
    "    vect = CountVectorizer(ngram_range=(2,5), analyzer=\"char\")\n",
    "    X_train_counts = vect.fit_transform(data[\"x_train\"])\n",
    "    print(X_train_counts.shape)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    \n",
    "    lr = LinearRegression().fit(X_train_tfidf, data[\"y_train\"])\n",
    "    evaluate(lr.predict(X_train_tfidf), data[\"y_train\"])\n",
    "    \n",
    "    X_test_counts = vect.transform(data[\"x_test\"])\n",
    "    pred_scores = lr.predict(tfidf_transformer.transform(X_test_counts))\n",
    "    evaluate(pred_scores, data[\"y_test\"])\n",
    "    return lr, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12842, 204654)\n",
      "Training: Precision=0.99 Recall=0.99, F1=0.99\n",
      "Training: Precision=0.70 Recall=0.67, F1=0.68\n"
     ]
    }
   ],
   "source": [
    "_, baseline_pred = lr_with_freq(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_pred = list(map(lambda x: 1 if x >= 0.5 else 0, baseline_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2754"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_false_positive(true, pred, x):\n",
    "    assert len(true) == len(pred)\n",
    "    assert len(true) == len(x)\n",
    "    result = []\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == 0 and pred[i] == 1:\n",
    "            result.append(x[i])\n",
    "    return len(result), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_false_negative(true, pred, x):\n",
    "    assert len(true) == len(pred)\n",
    "    assert len(true) == len(x)\n",
    "    result = []\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == 1 and pred[i] == 0:\n",
    "            result.append(x[i])\n",
    "    return len(result), result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2754"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_pred = preds[0]\n",
    "len(hybrid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197,\n",
       " [\"nothin unsexier than some1 that think they're all that. male or female “@the_eccles: awww, bless them, they still think they were hot..#mkr”\",\n",
       "  'your tweet said \"call me sexist,\" followed by sexist statement. i comply, u take issue. men are so illogical and emotional!',\n",
       "  'and remember that israel has only existed for a fraction of the nobel period.',\n",
       "  '3 male indie filmmakers at the table next to me discussing a script for a new movie. it sounds sexist! this is y we need more women in media',\n",
       "  'these 2 delusional, narcissistic hostesses on mkr make me want to vomit my own dinner up.',\n",
       "  'by people, you mean men only. the women are clearly imprisoned in their homes.',\n",
       "  'the translations are on a muslim web site and they say strike.',\n",
       "  'my understanding is free speech means there should never be any consequences when you say revolting things so, check…',\n",
       "  'really!!! really!!! another round!! mkr those butcher promo girls better be nice to sheri and emilie.',\n",
       "  \"the information you are bringing is from the hadiths, not the quran. i don't recall age being discussed in quran\",\n",
       "  'bugger...not... bye bye blondes... mkr',\n",
       "  \"because i'm a feminazi, that's why...\",\n",
       "  'dude we were the best team ever. we made shit happen! aggies coon',\n",
       "  \"did you not watch? she was half his size and there was 80k on the line. don't start with physical equali…\",\n",
       "  'im not sexist but this is gd funny',\n",
       "  'how islam deprives women of education.',\n",
       "  'yazidi girls tell of escape from is',\n",
       "  \"i'm against any jew or christian that wants to live by old testament law as opposed to secular law.\",\n",
       "  \"i don't understand mean girls. and certainly not mean girls on aussie television. mkr mkr2015\",\n",
       "  'who is you. what is an illegal settlement. all muslim settlements were made by violence.',\n",
       "  'i bet the campers vote strategically...at least that is what kat will say mkr',\n",
       "  \"blameonenotall: the 'meninist' rape campaign that spectacularly backfired news\",\n",
       "  \"you are the hate monger murdering babies. i'm defending them.\",\n",
       "  \"iran's plan to exterminate israel is no secret except maybe to obama's administration istandwithisrael pjnet\",\n",
       "  'not surprising that blumenthal is linked with neonazis.',\n",
       "  'yeah, for left wing scum like you all cultures are more valuable than your own. and ...',\n",
       "  'the israelis should demand back the 40% of the arabian peninsula that the muslims stole from them.',\n",
       "  'one can only hope. i see anime avatars, and sadly my experience points at the exact opposite of support.',\n",
       "  'almost all jews are secular and could care less about following the example of moses.',\n",
       "  'you are right. but those leaving the religion are not telling the daesh about it.',\n",
       "  'they are in about 20 verses of the quran. and they are in the hadiths.',\n",
       "  \"i'm betting that not a single idiot at salon has ever read the quran and hadiths.\",\n",
       "  'the per capita production of nobel prize winners for israel is more than 100 times that of muslim states.',\n",
       "  'yesallwomen because once at a louis c.k. show he said \"how do you make an old woman? you take a young woman and just ruin her.\"',\n",
       "  'i don’t need femisnsn because men carry heavy things that i cannot!!! like shopping, boxes, and a huge sense of superiori…',\n",
       "  'omg. all those girl teams and 2 couples. hahaha, poor kats husband and lloyd. mkr',\n",
       "  'the jews are only at war with the muslims who want to wipe them out, microbrain.',\n",
       "  'when 14 year old boys use the term feminazi and u gotta school em in class 😔',\n",
       "  \"isis? in mecca? but i thought they weren't real muslims and had nothing to do with islam?\",\n",
       "  'here, look at the statistics for what muslims believe.',\n",
       "  'say what you will about the promogirls but their apple pie does sound kick-ass. mkr',\n",
       "  'your childhood was a piece of racist and sexist shit and we set it on fire',\n",
       "  'there could be an antiislamophobic, however. and that would be you.',\n",
       "  'thinkin that you have to be christian to be against killing babies is just inbred stupidity',\n",
       "  'deconstructed lemon tart. that would have went with the deconstructed kebab in my toilet on sunday morning. agradehang…',\n",
       "  'and hating islam is rational, and therefore not a phobia.',\n",
       "  '... your own must be attacked and exterminated.',\n",
       "  'bonus: kitzinger was one of the first women to marry a woman in the uk:',\n",
       "  '\"our main course is going to be vacuous and narcissistic, just like us!\" mkr',\n",
       "  \"female devs make me uncomfortable, so let's imply her work was a single commit.\",\n",
       "  'no one ever writes a book called \"men, stop being an asshole in relationships\" or \"she just wants to be treated like a human being for once\"',\n",
       "  'you did not just, in all seriousness, tweet \"i can\\'t be sexist, my mother is a woman\"????',\n",
       "  \"if you compare being feminist with the holocaust, i'm proud of saying that i'm a fucking feminazi\",\n",
       "  'the question is, why are the daesh murderers of innocent civilians complaining about the civilians they hide behind.',\n",
       "  'again, showing you know nothing about islam.',\n",
       "  'the brutality of the old testament is not an excuse for the brutality of islam.',\n",
       "  'british teenager raped 90 times in weekend, report into uk sex slavery finds auspol htt…',\n",
       "  'yesallwomen know men hate us, and try to carve a life out from what remains when you subtract our daily fear, rage, despair.',\n",
       "  'because what you think is islam has no resemblance to the real islam.',\n",
       "  'so come up with a drop of oil from any nation that the us has not paid full market price for, butthead.',\n",
       "  'hot pot? come on girls . about as technical as a ham and cheese toastie. disappointing mkr',\n",
       "  'good idea. your prophet mohammed said camel urine is great medicine. try some.',\n",
       "  'i have no problem with semites, only with islam.',\n",
       "  'there is nothing incorrect about the generalization. look at minority demographics in...',\n",
       "  \"i'm not sexist but\",\n",
       "  'those are isis guns pointing at that baby. and murdering someone for the way they were born is sick like your islam.',\n",
       "  'oh you have no idea what its like to be set upon by the bunnyarmy you really want to see that …',\n",
       "  'kat is showing her true colours mkr',\n",
       "  'those sausages nikki and katie made look like they jumped up onto the bench and laid a turd in their bowls :/ mkr mkr2015',\n",
       "  'kat is a disgrace mkr',\n",
       "  'in wod, you see manrik and his wife as children. come baaaaack.',\n",
       "  'this girl had princess in her name so my friend and i roleplayed that she was our princess all game. her bf got mad.',\n",
       "  'do you realize how dumb you sound? there is one wall with anything on it. tylenol, orajel, and pepto bismol',\n",
       "  'haha! mt this makes me laugh. every single time. feminazi feminism yesallwomen misogyny fem2',\n",
       "  'isis\\' video of kasasbeh execution was broadcast on raqqa\\'s big screens and you can hear ppl yelling \"allahackbar\" htt…',\n",
       "  'kat is a whench boot her off mkr',\n",
       "  'sometimes i retweet things i like that are for real not sexist &amp; awesome. i apologize in advance for confusion',\n",
       "  'understand your point. seems like i heard of a plan to retake baghdadi today.',\n",
       "  'you are a liar. the palestinians never had a country and the jews were settled, not bedouins.',\n",
       "  'the academy awards got a little too intense sunday.',\n",
       "  'the guest judges should deduct points for the catwalk fuckery mkr',\n",
       "  'there were never more than 5 million indians in n. america, microbrain.',\n",
       "  'just change your profile pic/name to a woman, pref w/brightly colored hair. tweet out \"gamers are dead\" or \"games are sexist\"',\n",
       "  \"let's have a protest march against hamas.\",\n",
       "  'segregation of the sexes is just one more of the backwards steps that islam hopes to achieve.',\n",
       "  'yeah, it says that men can beat women. so what is your point?',\n",
       "  'and the muslims claimed that mohammed rose to heaven at now site of al aqsa so they could take it.',\n",
       "  '\"we\\'ve proved we\\'re not just the dumb blondes with pretty faces\". whatevs. mkr',\n",
       "  'these girls are pretty...awful. gohome mkr',\n",
       "  'the three best muslim states you could come up with prove my point.',\n",
       "  'how do crimes of christianity excuse the barbarity of islam? no one here is trying to sell christianity.',\n",
       "  'it is trolling.',\n",
       "  '\"grammar nazi\" and \"feminazi\" are offensive terms. way to be language stalins.',\n",
       "  'the japanese army was a nationalist group, not a religious terror group.',\n",
       "  'is the personification of rape culture. avoid alex valbergs, women.',\n",
       "  'ole annie on that tree!! she was fired up!! barnsidekennelsjk ready to go again. coon hunting…',\n",
       "  'obsurfer=lies, vileislam=truth',\n",
       "  'some ppl promote education to our youth.. promotes the kardashians.. what a coon..',\n",
       "  '“@moralcourage: ever been called a \"feminazi\"? watch our new video! haha many times!',\n",
       "  'gandhi was pretty sexist, but on ladyghostbusters: \"first they ignore you, then they laugh at you, then they fight you, then you win.\"',\n",
       "  'before mohammed, the jews lived on 40% of the arabian peninsula. almost all that land has been stolen by musli',\n",
       "  \"because 40% of arabian penninsula was jewish before mohammed's exterminations.\",\n",
       "  'ever told a sexist joke and a woman teammate laughed along because “she’s cool”? read this now.',\n",
       "  \"in fact, i think i'll go watch some gilmore girls on netflix. goodnight.\",\n",
       "  \"you don't know your jewish tribes. jews of khybar had nothing to do with a treaty against the meccans.\",\n",
       "  'let me spell it out. it is deeply misogynist to propagate wild conspiracy theories suggesting women in gaming fake death or ra…',\n",
       "  'cannot stand kat on mkr ! 😡',\n",
       "  'car is 4/5 christian and the muslims took the government by force. finally the christians fought back.',\n",
       "  \"my views are largely tempered by my former tax bracket. and although i like the idea of flat tax, doesn't work well at pov lvl.\",\n",
       "  \"acting like men who respect women are noteworthy implies that men are inherently deviant. that's insulting to all genders…\",\n",
       "  \"don't like these girls👿 mkr bishes no offence girls 😋😋\",\n",
       "  'gamergate just called a \"copy boy or personal assistant\" on 8chan.',\n",
       "  'yes! get rid of kat! mkr',\n",
       "  'you are either 100% ignorant of your religion or you are a 100% liar. you tell me which.',\n",
       "  'mkr lets see who the producers think are going to be better tv - kat or nikki and katie??',\n",
       "  \"because when young women look at colleges to attended the rape statistics are more important than it's qs rankings. ye…\",\n",
       "  \"chortle that blind date last night is why i don't do blind dates. all she did was call me sexist because i like being chivalrous\",\n",
       "  'what does that have to do with the barbarity of islam?',\n",
       "  \"i don't care who it is, just as long as it's not the promogirls mkr\",\n",
       "  'rape is not a rhetorical device. rape is not a rhetorical device. rape is not a rhetorical device.',\n",
       "  \"if you're still believing that modern feminism doesn't demonize men, read a couple of tweets in blameonenotall.\",\n",
       "  'if katie &amp; nikki go, i hope one of them give kat a left hook on the way out. what a bitch. mkr',\n",
       "  'mkr nikki. you laughing sounds like a dying pig',\n",
       "  'giuliani calls out obama – can’t say ‘radical islam’ because he’s a coward -',\n",
       "  'yesallwomen are not going to just \"let it go.\" there are countless women who are killed every day in single numbers who are forgotten.',\n",
       "  \"you retiring not at a sneaker show smh no need to stunt with kicks you probably ain't pay for rt coon htt…\",\n",
       "  \"who cares. it's about the example that the religion tries to follow that determines if it can be reformed.\",\n",
       "  'if you\\'ll happily use the word \"feminazi\", you don\\'t get to complain about other people causing trouble.',\n",
       "  'these two might not be able to use the churning machine, but they sure churn my stomach. mkr',\n",
       "  'another stupid excuse. isis is islam. not cia or mossad.',\n",
       "  'so your argument is that islam is good because others did bad things in the past. try again.',\n",
       "  'it hurts my brain just trying to come up with sexistfacts. \"the sun is 92 million miles from earth\"?',\n",
       "  'and kat thought her bake was great. so delusional mkr',\n",
       "  'in fact, it was the jews that settled medina before the arabs. then the muslims exterminated them.',\n",
       "  \"why doesn't she know that her prophet said that most dwellers of hell were women?\",\n",
       "  \"the old school is sexist. i'm not sexist im just old school\",\n",
       "  'or that some of my followers would take it upon themselves to \"defend\" me by tweeting at that person w/o my knowledge.',\n",
       "  'this whole harper lee thing kind of grosses me out. so, her sister died, and it sounds like everybody is taking advantage of her.',\n",
       "  \"&amp; blondes scored fairly “@k8ie_mac: devastated that kat is still there, wasn't the biggest fan of blondes but liked them more than kat mkr”\",\n",
       "  'the next person who says feminazi will get a slap. if you think a woman is mistreating a man you can call her sexist too, not feminazi -.-',\n",
       "  'you can call a female a feminist or a \"feminazi\" but that still does not justify you being a complete inconsiderate jerk defending -',\n",
       "  'want 2 get men 2 approach other men on buses, trains, and sidewalks pestering them 2 b best friends, 24/7 and see how they fucking like it.',\n",
       "  'uc irvine, which sits amidst former bastion of ca conservatism, orange county, is a leftist, pro islam, anti-israel radi…',\n",
       "  '\"i believe in equality except when i\\'m drunk and mad at women.\" &lt;-- why i don\\'t trust male feminists',\n",
       "  \"you're a feminazi?!?\",\n",
       "  'declaring war on humanity and murdering 280 million people to \"allahu akbar\" is evil.',\n",
       "  'guess they are running out of those black uniforms.',\n",
       "  'when i see young angry feminists being so much better at this than i was at their age it just makes my heart swell.',\n",
       "  'that is a lie. the prophet mohammed had people executed for criticizing him.',\n",
       "  '“@todclarey: lemon \"tart\" looks like shit mkr” please- don\\'t hold back - tell it how you see it',\n",
       "  'why were innocent civilians in prison?',\n",
       "  'if you talk the talk, you gotta walk the walk!! do the girls know the recipe for humble pie??? mkr',\n",
       "  '... when he hears \"blue haired bitch\", \"gamergate\", and \"freebsdgirl\". oh, dear.',\n",
       "  'not dumb blondes...mmm...perhaps just delusions of adequacy? mkr',\n",
       "  'stop saying sass or i will put my foot up your ass mkr',\n",
       "  'feminazi blog reminds liberals today is “national day of appreciation for abortion providers”… via',\n",
       "  \"let's see, why would there be a movement against islam, but not against any other religion.\",\n",
       "  'wtf kat and andre are cheaters! that is so unfair. they cannot give someone 1. they should be gone. mkr katandandre …',\n",
       "  'all those damn uppity women, demanding a better quality of life, always causing problems.',\n",
       "  \"tell me how old you are. i don't want to be here attacking a child about her religion.\",\n",
       "  \"mkr purposely pick slightly unattractive girls who're besotted with themselves. makes good television lol\",\n",
       "  \"mkr you'd think in her downtime annie would have paid napoléon perdis a visit and learnt not to use the same coloured blush as your hair...\",\n",
       "  'same with feminists though. just saying you are one can lead to instant hostility or stuff like \"ur not a feminazi so ur ok',\n",
       "  '.@slutwalkto agreed, sure but what does that have to do w page3?',\n",
       "  'not really. the radicals follow the religion to the letter.',\n",
       "  'here is a comparison of slavery in islam as compared to the west.',\n",
       "  \"tweetstats.hashtag.notyourshield.mention._roguestar_.rt quickly, create a shield from the person that posted women's addresses on twitter!\",\n",
       "  'halloween is a busy day for sexist assholes.',\n",
       "  'but you keep returning to the stupid idea that christian barbarity justifies muslim barbarity.',\n",
       "  'the destruction of minority populations by islam continues.',\n",
       "  'so these are hadiths that i copied from the usc muslim students hadith database. look them up for yourself.',\n",
       "  \"can't get any more freaky than having sex slaves, beating women, making women slaves.\",\n",
       "  \"when you're a woman and/or an underrepresented minority, you fear people will view you as being hired for something other …\",\n",
       "  \"it's fairly obvious that you are willing to throw women under the bus so that muslims can go on abusing them.\",\n",
       "  \"i'm going to blow your mind: every sentence that contains an adjective signals an opinion sexistfacts (maybe not colors, maybe)\",\n",
       "  'rape is not a punch line.',\n",
       "  \"again, you present your imam's propaganda. you have no hadiths or verses.\",\n",
       "  'people are taking this chris paul thing way to far joke notsexist justcomplaining',\n",
       "  'the jihadists that slaughtered french magazine editor and staff \"men \\'spoke french perfectly\\'',\n",
       "  'the have disgraced our country . first they covered for seville,then they failed with muslims rapes,now this …',\n",
       "  \"the meals weren't great but they didnt deserve 17... mkr\",\n",
       "  'yeah, they appeared as if by magic. are you really that stupid.',\n",
       "  'why not just make a tart? mkr',\n",
       "  'and no one said only women should stick together. non-rapist men can also stick together with their (women) friends',\n",
       "  'i just someone in a youtube comment call himself a feminist and then proceed to describe someone as a \"feminazi\" alright',\n",
       "  'no, they had no rights in a court of law and had to hide the practice of their religion from muslims and had to pay jizya.',\n",
       "  'why do i have to read arabic. heard of translations?',\n",
       "  \"nuland's nazis? i didn't know that the muslims were working for nuland.\",\n",
       "  \"in islam, there's a concept called taqiyya, which dictates that lying is acceptable if it advances the cause of allah http:/…\",\n",
       "  \"blameonenotall you don't assume all muslims are terrorists so why assume all men are rapists?\",\n",
       "  '5 muslims arrested in horrific 4 week long repeated gang rape',\n",
       "  'no, that is what you and islam do.',\n",
       "  'we are talking about what mohammed believed and is therefore a part of islam.',\n",
       "  'he has schools all over the world. what is wrong with a 40 mil estate?',\n",
       "  'says the race obsessionist who lives life to be able to sanctimoniously call others racists.',\n",
       "  'shares australia pm announces new counterterror measures: tony abbott says st... by arab',\n",
       "  'so is watching mkr. i have to say that kat seems like an absolute crazy bitch..'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_false_positive(data[\"y_test\"], hybrid_pred, data[\"x_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268,\n",
       " [\"eva &amp; debra are now my most hated team on mkr after least night's ep. snakes in the grass those two.\",\n",
       "  'stoprush: \"uses words like chickified &amp; feminazi, is a hate-monger.\" how do you actually function in this worl…',\n",
       "  'that face kat just pulled was hideous. mkr',\n",
       "  \"ben i'm open to meeting good feminists like helen, etc. but uh...how can i say nicely, maddy is...not smart...at all. a lemming.\",\n",
       "  'and the secret twist. kat and andre are eliminated for being cunts!!!! mkr',\n",
       "  'mkr kat: the wicked witch from the west',\n",
       "  'personally i am just glad when they use the actual word and not feminazi',\n",
       "  \".@facerealitynow i saw/rt'ed it. too funny. i saw this one today...\",\n",
       "  \"at least we know they're natural blondes mkr\",\n",
       "  \"yes i am a brony and a meninst get over it. you won't have sex with me? what kind of bullcrap is this oppression? femi…\",\n",
       "  'how can there be a biological inequality? that suggests that the process of evolution itself is s…',\n",
       "  \"i see that now...didn't get far enough into his tl.\",\n",
       "  'i\\'m not kidding when i say it: today, feminismishate -- \"nice\" feminists should stop caling themselves that.',\n",
       "  \"i pay the bills in this household it's the least she can do\",\n",
       "  'perturbed by neighborhood strip club, feminist asserts \"women are people 100% of the time\"',\n",
       "  'to some, yeah...',\n",
       "  \"you call difference inequality; that's a loaded word\",\n",
       "  'makeup time for katie &amp; nikki mkr',\n",
       "  'he asked for it. did you? why not find an employer who will value you properly?',\n",
       "  'seriously. crazy-eyes is staying? mkr',\n",
       "  '.@notchrissmith \"not paid the same in hollywood\"? they each have agents who individually negotiate their deals. you want to pay a flat rate?',\n",
       "  'omg that woman is awful!!!!! mkr',\n",
       "  'tokyo hot n1049 endless sex drive - dailyxlover jav asian',\n",
       "  'mkr stop rolling the sausages like that.... giving me a \"soft-on\"!',\n",
       "  'what is that?',\n",
       "  \"i apologise on behalf of my generation for nikki and katie. we aren't all so vapid. mkr nikkiandkatie\",\n",
       "  \"colin was pretty nice to the girls. do you think he's giving them the dick? mkr\",\n",
       "  'mkr killerblondes model? puhlease. did the local thrift shop put on a \"fashion\" show. and should we be able to see her…',\n",
       "  'men are attacked 4 times more, just fyi.',\n",
       "  \"here we go. fucking twitter man i hate this moving around shit. but no better way to find feminazi's lawl.\",\n",
       "  'i have watched several female comedians on several occasions, to my memory i have not laug…',\n",
       "  \"gamergate is only interested in game journalists and sjw's who put politics ahea…\",\n",
       "  \"the format to mkr is stupid. it'll be about 10 weeks until we see the cowboy and his fit daughter cook again.\",\n",
       "  '...there is no beef here.',\n",
       "  'odd that google can find no relevant examples of it actually being enforced then, huh?',\n",
       "  'yay! just when we drowned with the last on-sided feminazi rape (public) video, one mores comes along:',\n",
       "  'throwing shit around because kat is impossible to deal with. mkr mkr2015',\n",
       "  '“@carlyandtresne: really!!! really!!! another round!! mkr those butcher promo girls better be nice to sheri and emilie.” yes or else',\n",
       "  'mkr \"if i could score them a zero then i would because i\\'m a super slut\". cheats irritate ratingsbonanza uglycow',\n",
       "  \"they're staying...just so that the producers can have a showdown between them and the posh bitches mkr bringit\",\n",
       "  'if kat and andre are strategically scoring, they need to be kicked off mkr2015 mkr',\n",
       "  \"just not very appealing to me. then again, it is a tv show that revolves around women in prison. probably why i don't like…\",\n",
       "  'seriously? where exactly do you suggest i put my tits, should i detach them when doing feminist things?',\n",
       "  \"i'm so disgusted that people like kat is allowed in this competition mkr\",\n",
       "  \"catching up on mkr, disgraceful behaviour from kat and andre. strategic scoring won't save them forever.\",\n",
       "  'was on a russian site once. it was either meninism or uva hoax, i think.',\n",
       "  \"why, because she backed straight into your balls? because that's what i would do, with scientific precision.\",\n",
       "  \"they just don't make me laugh. like whip out your tits do something entertaining\",\n",
       "  'fk you kat and andre! mkr',\n",
       "  \"i didn't realize i was supposed to treat you as a child whose opinion is irrelevant, sorry. my bad.\",\n",
       "  \"cos that's all they suck to get their jobs mkr\",\n",
       "  'sooooo true.',\n",
       "  \"wait, that back shot of nikki in the kitchen... she's wearing blue knickers?! honey, no. 🙅🙅🙅 mkr vplonextreme\",\n",
       "  'this guy is an amazing writer regarding feminism.',\n",
       "  'be a man. stop babbling and squirming and admit you were wrong.',\n",
       "  'there goes your attempt at an \"i\\'m such a deep dude\" persona. suck my balls hoe',\n",
       "  'pretty much this.',\n",
       "  'actually i have. the same thing happens to african americans, spanish, and other minorities.',\n",
       "  'dehydrated vaginas - i mean pears - for breakfast.',\n",
       "  \"you're bat-shit insane to *allies*, and then wonder why you have so few. that's why radical feminism has al…\",\n",
       "  \"the motive doesn't lessen the outcome.\",\n",
       "  'stick to your day jobs girls mkr',\n",
       "  'i hate kat so much mkr',\n",
       "  \"aww i was looking forward to ash and camilla butting heads with the promo girls. the drama would've been great mkr\",\n",
       "  \"mkr oh my gawd we are like so hot i can't even...\",\n",
       "  \"irrelevant girl, give it 5 years. she's going to lose sexual market value. then i'll ask her to pass the bag.\",\n",
       "  \"did you find katie and nikki in the line for mt druitt's next top model? mkr\",\n",
       "  'wait, do you realize you just did the very thing the tweet you posted said is wrong...?',\n",
       "  'ugh. fuck off kat. your turns next. mkr',\n",
       "  'already sick of hearing \"pretty\" mkr mkr2015',\n",
       "  'damn kat &amp; andre sneak through, would much rather katie &amp; nikki had gotten through mkr damnukat',\n",
       "  'omg, you guys. they are misusing the word gaslighting repeatedly. must not have gotten to that on their sjw word of the day calendar yet.',\n",
       "  'yeah colin would be looking for more fat in his sausage.... mkr',\n",
       "  '\"we want to stay in this competition because we haven\\'t finished telling everyone how pretty we are\" mkr',\n",
       "  'are kat and andre brother and sister? mkr',\n",
       "  'has there been a more genuinely hate worthy individual than that goblin, kat? mkr',\n",
       "  \"not my fault people feed stereotypes. for example my sister... she's been in multiple accidents\",\n",
       "  'lol, that chick from last night is still raging about me blocking her, i guess.',\n",
       "  \"mkr maybe you girls should go back to cutting up cows and pigs, don't quit your day job!!\",\n",
       "  'notyourscapegoat join the party :)',\n",
       "  'five bucks says drinking.',\n",
       "  'this is the epitome of how to not blameonenotall',\n",
       "  'kats face when the score was announced made me want to puke eww wtf mkr',\n",
       "  \"valentine's day is great and all, but i'm holding out my real enthusiasm for love day:\",\n",
       "  'one should consider we actually get more tax breaks and less intensive jobs even in…',\n",
       "  'what does the \".77 on the dollar\" figure describe?',\n",
       "  \"it's insulting to women to expect them to only be attracted to super easy games. i honestly am insulted as a parent of a girl…\",\n",
       "  'thanks for heads up. i watched it. good stuff.',\n",
       "  'kat needs to calm down with the crazy eyes mkr',\n",
       "  'wrong and wrong.',\n",
       "  'i was fortunate to catch this right when it was posted, 4th reply in. get rekt anita. gamergate',\n",
       "  '.@ilivundrurbed anything but yourself. like every feminist on earth.',\n",
       "  'not me. either guns blazing or waiting quietly in my trojan horse...',\n",
       "  'refined &amp; pretty? looks like something a blind kindergarten student chucked on a plate. mkr',\n",
       "  'katandandre gaaaaah i just want to slap her back to wa mkr',\n",
       "  \"that might be poe, just fyi. i think that was the quote of hers that couldn't be verified.\",\n",
       "  \"it's actually two separate proposals. real. insane.\",\n",
       "  \"when i see people out &amp; about wearing their nfl team shit while that team's game is on i'm judginghard badfans usuall…\",\n",
       "  'kat calls it strategic, i call it being a biatch! mkr',\n",
       "  \"got a 1st for coburn, more than i'd have thought. hawks made a mistake/took big risk on timonen but good deal for hextall.\",\n",
       "  'obviously natural blondes!! mkr',\n",
       "  \"i hate kat's big smug face and her constant split fringe so much. kat and andre australiasmosthatedcouple mkr\",\n",
       "  \"how can you call that inequality? that dynamic exists because it's necessary for the existence of our species\",\n",
       "  'how about next season you hold off on the scores until all the teams have cooked, so strategy plays no part. food does. mkr',\n",
       "  \"you're really fond of lying about everything. i said they aren't isis.\",\n",
       "  'mkr deconstructed by girls that have deconstructed brains ! nearly brought up my dinner when i saw that crap on the plate',\n",
       "  'a genuinely interesting question...of course, the answer is predictable and boring.',\n",
       "  'can you comment on ford doing the very thing she complains of...?',\n",
       "  'why do you judge on genitals, rather than merit? if a man works in an 80% female workplace, should he cry discrimination?',\n",
       "  'and we still get payed equally. that stupid myth bothers me to no end because there…',\n",
       "  \"...it's backdoor communism.\",\n",
       "  '...you have an armpit licking fetish?',\n",
       "  'lately, i get more hate from sjws. but that just goes along with refusing to be used as a platform for abusing their enemies.',\n",
       "  '...like i said. a \"symbol\" is a \"symbol\". jewelry, however, can be pawned later.',\n",
       "  'hey kat, \"gloat\" is not a pretty colour to wear, love. makes you look bitchy. oh that\\'s your normal face? my mistake. mkr',\n",
       "  \"put a bag on your head kat, no one wants to see your glee at other people's misfortune. you're awful please leave thedooristhatway 👉 mkr\",\n",
       "  \"blameonenotall is not because i want to be congratulated by not being a rapist. it's because i don't wanna be categorized with stupid men.\",\n",
       "  'the one thing that could actually end the \"harassment\" of gamergate...',\n",
       "  'i felt so out of place at that party last night. there again so would any man who stands in a kitchen for that long...…',\n",
       "  'you say this based on what?',\n",
       "  \"you mean it shouldn't be? because it is consent to parenthood for men today.\",\n",
       "  'i keep hoping for her brain to engage at some point, but all factual evidence just zips by unnoticed.',\n",
       "  \"howtospotafeminist doesn't want all feminists blamed for feminist lobby for anti-male discriminatry law&gt;freaks out over …\",\n",
       "  'kats the kind of person that would cheat to win and then still think that made her a better cook. so my new baddie mkr',\n",
       "  'an enemy of is good enough for me...f/f',\n",
       "  'what a riveting little lesson in walking the catwalk. thanks ladies mkr',\n",
       "  '\"hey i have a gender studies degree, clearly you all are the brainwashed ones\"',\n",
       "  'there just horrible lemontarts mkr',\n",
       "  'kat is completely class less. mkr',\n",
       "  '“use ruby to get some ladies” at',\n",
       "  'so someone just told me that feminism = \"lying communists who oppose equality\". can you believe people this bs? http:…',\n",
       "  'what is the event?',\n",
       "  'so, no issue with her doing the very thing she decries, then?',\n",
       "  'thought you might like one of my comics. from unpc comix :)',\n",
       "  \"bad seeds grow weeds. must be rooted out first or else you'd foundation cracks.\",\n",
       "  'blameonenotall: do we really need to ask if feminismishate or if misandry is real after reading things like this? http://…',\n",
       "  '.@ilivundrurbed yes. your apathy.',\n",
       "  \"do kat &amp; andre have any shame or integrity? they can't even pretend to be gracious. mkr\",\n",
       "  'we found out quickly most of these \"questions\" were not looking for answers.',\n",
       "  'but greasy haired colin is a judge so messy yuck is in 😂 mkr',\n",
       "  \"can i cry now? i think i'll cry! this is exactly what women should look like! thanks,\",\n",
       "  'loool what goes around comes around, kat should just shut up mkr',\n",
       "  \"i know. you know. my intent is to prove they don't know.\",\n",
       "  'i think drasco just offered colin an old fashioned wristy mkr',\n",
       "  'flyers can never get all 3 phases in sync. have had great forwards. had great d in 2010. but never find the g. if they do',\n",
       "  \"with kats attitude, i think it's time to keep scores quiet until final round mkr\",\n",
       "  'yeah! why do we feed the hungry but not the full? why do we give shelter to the homeless but not the homed? so unequal',\n",
       "  'so she needs to give david $3 million, i guess.',\n",
       "  'fifty shades of grey is romantic only because the guy is a billionaire. if he was living in a trailer it would be a crimin…',\n",
       "  'oh my god, someone punch kat in the face. mkr',\n",
       "  \"mkr can someone punch that smug smirk off kat's face please?\",\n",
       "  'kat looks like medusa tbh + that ugly personality to top it off . mkr',\n",
       "  'whoa. nice, uh, \"impression,\" celine.#elephant nailedit mkr',\n",
       "  'it says women must stay at home and not display themselves.',\n",
       "  'mkr your meatballs were dry, things lloyds never heard before',\n",
       "  \"...many don't have the awareness to operate in what most know to be self interest. they will defy most solutions.\",\n",
       "  'that judge whatever his name needs to stop the fake tan. he is literally orange mkr',\n",
       "  \"how can feminists claim we don't have bodily autonomy? like..being able to kill your unborn kid is not enough for you? …\",\n",
       "  '\"you have the look that every man wants in a woman\\'s eyes - surprised with a bit of fear\" - holy crap.',\n",
       "  \"sorry, i won't use facts next time. i'll just go with the rest of the masses who think there are no differenc…\",\n",
       "  \"these girls say they're butchers, i once worked at a boat hire shop so i must be a ship captain mkr\",\n",
       "  \"i'm sick of you useless ass people in my culture. stfu im sick off you useless ass people in my mentions\",\n",
       "  'kat strikes me as a very repulsive individual... mkr',\n",
       "  'ok. might have used the wrong word, but you see my point',\n",
       "  'the sassy foods feeling mkr2015 mkr these girls are vile people killerblondes basichotdog nobun',\n",
       "  '.@sassnpearls they have no sense of irony or awareness. or shame. or decency. or...well, i could do this all day...',\n",
       "  'i think the hashtag got some traction. cuntandandre mkr',\n",
       "  'im starving. but im so worn the hell out from work i cant bring myself to get up, let alone go cook. single dude probs. n…',\n",
       "  'mkr2015 mkr i think kat thinks she is the best chef ever. ah hello no',\n",
       "  'so kat got a new decent hair cut then mkr',\n",
       "  'but, sorry, who cares that the national organization of women was commie, you were talking about a trucker, right? too funny.',\n",
       "  'i respect your use of sprinkles are for winners.',\n",
       "  'i guess we all hate kat and andrea but they all hated the frauleins more.you bitches cant win nor beat kat the dog mkr sorrybraunhilders',\n",
       "  'good for you. too bad about those hurt by it or unprotected by it, right?',\n",
       "  \"blameonenotall? men don't need to try to be oppressive to be oppressive.\",\n",
       "  \"i just laughed at kane's move. kid is sick. wish they'd used geno over letang, though...\",\n",
       "  \"women drivers, what? i'd say we kick ass.\",\n",
       "  '\"sorry\" for confusing me? what am i looking at?',\n",
       "  '\"i\\'m trying so hard to be charming\" - annie. i have nothing to add. mkr',\n",
       "  \".@tigerclaud called mra's dishonest, then used one untreated mental patient to smear whole mrm.\",\n",
       "  \"have you even seen a women kick a football. i thinks it's probably genetic.\",\n",
       "  \"did you find katie and nikki in the line for mt druitt's next top model? mkr\",\n",
       "  \"kat and andre shouldn't be there they suck dick holes. the moles win my vote with the burnt butter icecream mkr\",\n",
       "  'kat should be eliminated for bad sportsmanship mkr mkr2015',\n",
       "  \"kat is going to be so smug she's going to be declared the particularly smug mayor of smugtown on the isle of smuggy mkr\",\n",
       "  'kat is just plain, fucking awful mkr',\n",
       "  'did u write that urself omg all girls should come with a sign \"you must be this tall to ride my emotional rollercoaster\".',\n",
       "  \"remind me when i said fiction = reality? don't you have a feminist tumblr account to rant on?\",\n",
       "  \"at least we know they're natural blondes mkr\",\n",
       "  'people like you are what give feminists a bad name. gamergate is wrong, but the best defense is not alwa…',\n",
       "  'even in wrestling, it has its share of feminist authority figures! guess who called out on …',\n",
       "  'mkr promo girls are going to bring it in the next round. yeah! bring it! bring your store bought capsicums!',\n",
       "  'neo-nazis and holocaust deniers adore because he thinks just like them. more',\n",
       "  'leave it to some microbrain to get sanctimonious by finding a way to play the race card.',\n",
       "  'take a poll and see if 1/3 of christians think gays should be killed. if they do, christianity should be banned',\n",
       "  'daesh threw a major offensive friday to show they were still strong. they only managed to show their weakness.',\n",
       "  'people confuse empathy with being scared, and that says more about them than anything.',\n",
       "  \"isis quran islam 'no compulsion in religion' (qu'ran 2:256)? think again!\",\n",
       "  'any fool can see that is a photoshop job, goat fucker.',\n",
       "  'and this proved to be true.when there was no one left to steal from in the saudi area they became broken down criminal places',\n",
       "  'isis intend turn war of islamism into ethnic war of pan arabism&amp;kurd in mosul. as evidence show they are conducting pan ara…',\n",
       "  'many of the isis members are walking \"stoners\" ~ even daesh emwazi jihadijohn it\\'s been rumored he\\'s high on khat http…',\n",
       "  \"good for labour and now tory's! encouraging immigrants to enrich us, making the uk a better, freer, nicer place!\",\n",
       "  '2.3% my ass. at least half of them are murderous lunatics.',\n",
       "  'france is on the verge of major cultural backlash against even moderate islam. the tension has built for decades. the line has …',\n",
       "  \"we don't want to teach you. we want to stop your taquiyya lying about it.\",\n",
       "  \"you must have found it in your imagination, because it doesn't exist in islam.\",\n",
       "  'new chant outside the diwan in amman: \"no tahawi, no zarqawi, we want to execute al-rishawi\"',\n",
       "  'i hope so, because in english it comes off as moronic hate mongering.',\n",
       "  'please draw mohammed! just a smiley face will do but draw it and post it. iamcharliehebdo',\n",
       "  \"if we dont ban islam they'll ban us rt new islamic state video features french (cont)\",\n",
       "  'syria: raqqa doctors claim isis fighters sexually brutalize women; many reportedly fearful of rape',\n",
       "  'thanks for admitting that you are murderers. and check your map.',\n",
       "  'daesh price list for yezidi and christian sex slaves.',\n",
       "  'the hate atmosphere is built into islam. so extinguish it.',\n",
       "  '\"he can\\'t be a server at our restaurant, that beard makes him look like a terrorist.\" everyone laughs. fuckthanksgiving',\n",
       "  'islam has never been a resistance to oppression. it has always been source of oppression to both believers and non believer',\n",
       "  'good morning pissed off american patriots! cops in danger?',\n",
       "  \"my review got downvotes because gamergate dogpiled. but knows this. heck, he's friends with nero.\",\n",
       "  'no he\\'s not. he\\'s an idiot. he retweets things like \"nigger hunting licenses\" that never existed. he\\'s a fraud.',\n",
       "  \"i don't see any burning civilians, and the picture is not even of a burning house.\",\n",
       "  'and we would all be worshiping the god of terrorism.',\n",
       "  'islam is the perpetuity of oppression. and i thank isis for helping the world to kill islam.',\n",
       "  \"blameonenotall because the problem is clearly not systemic at all. that's why all problems ended 1000 years ago when the onl…\",\n",
       "  \"if you don't want to read a pedo, you have to stop reading the quran.\",\n",
       "  'dangerous to outsource iraq security to shia militia known for \"disappearances\" &amp; executions.',\n",
       "  'do you have a way to kill the daesh scum that are hiding behind civilians with hurting civilians?',\n",
       "  'the treatment of christians was nearly as bad. and people not of the book were murdered outright.',\n",
       "  'then why do you smelly rag heads keep moving to britian?',\n",
       "  'so, are people who support murder for apostacy guilty of anything?',\n",
       "  \"i'm talking total extermination, bragged about right there in the book.\",\n",
       "  'you will see that loss of men and weapons accelerate land loss in the next few months. the kurds are on the mosul doorstep.',\n",
       "  'islamic documentation is based on belief. historical documentation is based of factual research.',\n",
       "  'the islamist were there the whole time. it was only the dictators that kept them quite.',\n",
       "  'literally, every single approach road or track or open field to mosul is covered by peshmerga outposts now',\n",
       "  'we are in an era where everyone has come to understand what the real islam is all about.',\n",
       "  'wakeuplibs-&gt;.@joenbc: islamic jihadis like “ultrafundamentalist christians” tcot iloveobama',\n",
       "  'a again, give me a verse number that shows that isis is not following islam.',\n",
       "  'roflmao. where is the picture daeshbag?',\n",
       "  'con watermelon and grape soda coon lol rt i need about 50lbs of that 280 lb catfish that was caught',\n",
       "  \"son read blameonenotall. this is why i don't have a problem with generalization about black men. i know i don't do certain things.\",\n",
       "  'this is well-written &amp; worth reading on daily life in mosul today \"caliphatalism?\" via',\n",
       "  'look at this ranting lunatic palestinian. interesting what he tells us.',\n",
       "  'and no doubt your friend was every place where isis could have been murdering children.',\n",
       "  'inciting hatred is central to islam in order to promote imperialism.',\n",
       "  'and if someone risks their life to save me from a mass grave, they are welcome to my tv and toaster.',\n",
       "  'the reason that is newsworthy is because the muslim contribution is so much more often destructive.',\n",
       "  \"stop threatening people to get them to do what you want. that's why people hate religion.\",\n",
       "  '😘😻#cat mainecoon cute kitten best_cats coon relax meow funny fun tiger…',\n",
       "  'quoting from the book of violent fairy tales again?',\n",
       "  'kobane west front ypg forces liberated the village of \" qartl \" 17 km from tell-abyad city',\n",
       "  'and by the way, the entire world hates islam.',\n",
       "  'i know you have religious police telling how to dress.',\n",
       "  \"it can't be separate while those wishing to destroy israel use it as a basis of their genocidal fanaticism.\",\n",
       "  'mohammed loved robbing caravans and looting for a living. isis robs and lots for a living.',\n",
       "  \"it's basically an empty excuse that leftists and muslims use for their anti us bigotry.\",\n",
       "  \"madrassa math. daesh threw manpower at kobane in far disproportion to it's size.\",\n",
       "  'a typical muslim response is to excuse muslim barbarity by pointing elsewhere.',\n",
       "  '---&gt; perfect example of a coon. still can\\'t get over the \"white nubian goddess\" comment.... foh!!!! coon let\\'s talk about that!',\n",
       "  'so now you taquiyya liars deny what is explicit in the quran.',\n",
       "  \"all of you islamists repeat the same brain dead lines that someone else told you to say. i've seen that one a thousand times.\",\n",
       "  'yes, that is why mohammed called blacks \"rasin heads\".',\n",
       "  'there are many supposed americans that work to destroy america.',\n",
       "  \"i dont have to live there to know you can't leave house without male relative\",\n",
       "  \"in fact blumenthal's palestinian friends behead gays.\",\n",
       "  'here is the quran telling women not to leave their house.',\n",
       "  'i know that your husband can beat you there and there is nothing you can do.',\n",
       "  'muhammad was the inventor of thighing. he started molesting aisha when she was 6 years old. cc:'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_false_negative(data[\"y_test\"], hybrid_pred, data[\"x_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Larger Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2754"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_pred_large = preds[1]\n",
    "len(hybrid_pred_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233,\n",
       " [\"nothin unsexier than some1 that think they're all that. male or female “@the_eccles: awww, bless them, they still think they were hot..#mkr”\",\n",
       "  'your tweet said \"call me sexist,\" followed by sexist statement. i comply, u take issue. men are so illogical and emotional!',\n",
       "  'and remember that israel has only existed for a fraction of the nobel period.',\n",
       "  '3 male indie filmmakers at the table next to me discussing a script for a new movie. it sounds sexist! this is y we need more women in media',\n",
       "  'these 2 delusional, narcissistic hostesses on mkr make me want to vomit my own dinner up.',\n",
       "  'by people, you mean men only. the women are clearly imprisoned in their homes.',\n",
       "  \"israel doesn't control syria. that is simply an unrealistic conspiracy theory.\",\n",
       "  'the translations are on a muslim web site and they say strike.',\n",
       "  'wow, must be 5 or 6 of them.',\n",
       "  'bugger...not... bye bye blondes... mkr',\n",
       "  \"blameonenotall is an epistemically violent hashtag, meant to minimize rape culture and structural violence. it's lazy. it's …\",\n",
       "  \"this shows where islam would be if they didn't kill apostates.\",\n",
       "  'kim jong un embraces islam.',\n",
       "  \"because i'm a feminazi, that's why...\",\n",
       "  'dude we were the best team ever. we made shit happen! aggies coon',\n",
       "  \"did you not watch? she was half his size and there was 80k on the line. don't start with physical equali…\",\n",
       "  'im not sexist but this is gd funny',\n",
       "  'how islam deprives women of education.',\n",
       "  'yazidi girls tell of escape from is',\n",
       "  \"i don't understand mean girls. and certainly not mean girls on aussie television. mkr mkr2015\",\n",
       "  \"a. he's a racist and a bigot. b. he wants to sell books and be famous.\",\n",
       "  'who is you. what is an illegal settlement. all muslim settlements were made by violence.',\n",
       "  'i bet the campers vote strategically...at least that is what kat will say mkr',\n",
       "  \"ugh these fake nerd guys that don't even know who wonder woman is.\",\n",
       "  \"iran's plan to exterminate israel is no secret except maybe to obama's administration istandwithisrael pjnet\",\n",
       "  'woah! i just went to go prod sarah jeong to see if i could ask her a legal question, and saw she was gone. hope everything is ok. :(',\n",
       "  'your kindergarten photoshopped images prove conclusively that you are a paid putin troll fighting for fascism.',\n",
       "  'the israelis should demand back the 40% of the arabian peninsula that the muslims stole from them.',\n",
       "  'i wonder if they got hugged often enough as children. (i\\'m only *mostly* facetious. secure people don\\'t use words like \"feminazi\")',\n",
       "  'almost all jews are secular and could care less about following the example of moses.',\n",
       "  'you are right. but those leaving the religion are not telling the daesh about it.',\n",
       "  'they are in about 20 verses of the quran. and they are in the hadiths.',\n",
       "  'what iran won’t say about the bomb...',\n",
       "  \"i'm betting that not a single idiot at salon has ever read the quran and hadiths.\",\n",
       "  'showing that you are a liar since there is zero evidence.',\n",
       "  'the per capita production of nobel prize winners for israel is more than 100 times that of muslim states.',\n",
       "  \"the most bullshit terrible stackoverflow answer ever. i mean, it's technically correct, but... *shudder*\",\n",
       "  'yesallwomen because once at a louis c.k. show he said \"how do you make an old woman? you take a young woman and just ruin her.\"',\n",
       "  'i don’t need femisnsn because men carry heavy things that i cannot!!! like shopping, boxes, and a huge sense of superiori…',\n",
       "  'bottom line you name calling insecure feminazi. get an education, grow a spine and seek professional help.',\n",
       "  \"and if you think it's some random troll asshats, you would be incorrect. it's people pissed off about rape jokes telling him to kill himself\",\n",
       "  'omg. all those girl teams and 2 couples. hahaha, poor kats husband and lloyd. mkr',\n",
       "  'ok i officially dislike kat the most mkr',\n",
       "  'the jews are only at war with the muslims who want to wipe them out, microbrain.',\n",
       "  'when 14 year old boys use the term feminazi and u gotta school em in class 😔',\n",
       "  \"isis? in mecca? but i thought they weren't real muslims and had nothing to do with islam?\",\n",
       "  '\"it\\'s refined and it\\'s pretty. like us!\" seriously, someone get these girls english lessons, they have no idea what they\\'re saying. mkr',\n",
       "  'here, look at the statistics for what muslims believe.',\n",
       "  \"psa: feminazi is a fucking disgusting term that compares fighting for women’s rights to genocide. don't fucking use it.\",\n",
       "  'say what you will about the promogirls but their apple pie does sound kick-ass. mkr',\n",
       "  'your childhood was a piece of racist and sexist shit and we set it on fire',\n",
       "  'and hating islam is rational, and therefore not a phobia.',\n",
       "  '\"our main course is going to be vacuous and narcissistic, just like us!\" mkr',\n",
       "  \"female devs make me uncomfortable, so let's imply her work was a single commit.\",\n",
       "  'no one ever writes a book called \"men, stop being an asshole in relationships\" or \"she just wants to be treated like a human being for once\"',\n",
       "  'you did not just, in all seriousness, tweet \"i can\\'t be sexist, my mother is a woman\"????',\n",
       "  \"toot toot, the karma train stopping at kat's station! mkr\",\n",
       "  \"if you compare being feminist with the holocaust, i'm proud of saying that i'm a fucking feminazi\",\n",
       "  \"i am laughing my ass off at your fauxffense. the original tweet said women shouldn't be allowed to drive, for chrissakes\",\n",
       "  'the question is, why are the daesh murderers of innocent civilians complaining about the civilians they hide behind.',\n",
       "  'again, showing you know nothing about islam.',\n",
       "  'the brutality of the old testament is not an excuse for the brutality of islam.',\n",
       "  'british teenager raped 90 times in weekend, report into uk sex slavery finds auspol htt…',\n",
       "  'yup. and she rarely tells them to stop.',\n",
       "  'yesallwomen know men hate us, and try to carve a life out from what remains when you subtract our daily fear, rage, despair.',\n",
       "  'because what you think is islam has no resemblance to the real islam.',\n",
       "  'hot pot? come on girls . about as technical as a ham and cheese toastie. disappointing mkr',\n",
       "  'good idea. your prophet mohammed said camel urine is great medicine. try some.',\n",
       "  'i have no problem with semites, only with islam.',\n",
       "  'there is nothing incorrect about the generalization. look at minority demographics in...',\n",
       "  \"i'm not sexist but\",\n",
       "  'oh you have no idea what its like to be set upon by the bunnyarmy you really want to see that …',\n",
       "  'kat is showing her true colours mkr',\n",
       "  'kat is a disgrace mkr',\n",
       "  'you are a liar. islam says that a daughter must inherit half of what her brother inherits.',\n",
       "  'in wod, you see manrik and his wife as children. come baaaaack.',\n",
       "  'haha! mt this makes me laugh. every single time. feminazi feminism yesallwomen misogyny fem2',\n",
       "  'kat is a whench boot her off mkr',\n",
       "  'at this point and the way things are going, baghdadi has to know he will be dead before the end of 2015. islam',\n",
       "  \"isis: jihadist police beat up woman for 'exposing eyes' under veiled clothing: women living in mosul who offen...\",\n",
       "  'sometimes i retweet things i like that are for real not sexist &amp; awesome. i apologize in advance for confusion',\n",
       "  'you are a liar. the palestinians never had a country and the jews were settled, not bedouins.',\n",
       "  'mkr kat is defining fair..... hypocrite',\n",
       "  'and what, pray tell, is their \"place\"?',\n",
       "  'the guest judges should deduct points for the catwalk fuckery mkr',\n",
       "  'just change your profile pic/name to a woman, pref w/brightly colored hair. tweet out \"gamers are dead\" or \"games are sexist\"',\n",
       "  \"i find that it's the people who claim others don't know anything about islam that don't know anything.\",\n",
       "  'segregation of the sexes is just one more of the backwards steps that islam hopes to achieve.',\n",
       "  'regardless of who thinks what, there is only one definition of the religion.',\n",
       "  'same applies to 14th waffen ss. they fought for nazis to get rid of soviets. they never had.....',\n",
       "  'yeah, it says that men can beat women. so what is your point?',\n",
       "  'and the muslims claimed that mohammed rose to heaven at now site of al aqsa so they could take it.',\n",
       "  '\"we\\'ve proved we\\'re not just the dumb blondes with pretty faces\". whatevs. mkr',\n",
       "  'these girls are pretty...awful. gohome mkr',\n",
       "  'the three best muslim states you could come up with prove my point.',\n",
       "  'how do crimes of christianity excuse the barbarity of islam? no one here is trying to sell christianity.',\n",
       "  '\"grammar nazi\" and \"feminazi\" are offensive terms. way to be language stalins.',\n",
       "  'is the personification of rape culture. avoid alex valbergs, women.',\n",
       "  'ole annie on that tree!! she was fired up!! barnsidekennelsjk ready to go again. coon hunting…',\n",
       "  'obsurfer=lies, vileislam=truth',\n",
       "  'some ppl promote education to our youth.. promotes the kardashians.. what a coon..',\n",
       "  'stopgamergate \"anita sarkeesian is a feminazi\" because... uh... communism?',\n",
       "  'reddit user realises her boyfriend is gaslighting her, makes him watch gaslight. sweetjustice',\n",
       "  'because if the answer is \"nothing\", then you have blind and irrational belief.',\n",
       "  'exclusive sex offenders too violent to leave uk must stay because risk to overseas countries http…',\n",
       "  '“@moralcourage: ever been called a \"feminazi\"? watch our new video! haha many times!',\n",
       "  'i really hate when ppl who claim to want gender equality are afraid to call themselves feminists because they think feminist…',\n",
       "  'gandhi was pretty sexist, but on ladyghostbusters: \"first they ignore you, then they laugh at you, then they fight you, then you win.\"',\n",
       "  'before mohammed, the jews lived on 40% of the arabian peninsula. almost all that land has been stolen by musli',\n",
       "  \"because 40% of arabian penninsula was jewish before mohammed's exterminations.\",\n",
       "  'ever told a sexist joke and a woman teammate laughed along because “she’s cool”? read this now.',\n",
       "  'what would joan jett do? she would cut her own damn bangs.',\n",
       "  \"in fact, i think i'll go watch some gilmore girls on netflix. goodnight.\",\n",
       "  \"you don't know your jewish tribes. jews of khybar had nothing to do with a treaty against the meccans.\",\n",
       "  'cannot stand kat on mkr ! 😡',\n",
       "  'car is 4/5 christian and the muslims took the government by force. finally the christians fought back.',\n",
       "  \"you guys didn't put up much of a fight in tikrit. the fall of the daesh is on an accelerating curve. islam\",\n",
       "  \"acting like men who respect women are noteworthy implies that men are inherently deviant. that's insulting to all genders…\",\n",
       "  'new life goal',\n",
       "  \"don't like these girls👿 mkr bishes no offence girls 😋😋\",\n",
       "  'checked the gdc2015 tag w/ &amp; w/o in place. like night &amp; day. guess which led to more productive tweets …',\n",
       "  'never go full robot.',\n",
       "  'gamergate just called a \"copy boy or personal assistant\" on 8chan.',\n",
       "  'yes! get rid of kat! mkr',\n",
       "  'you are either 100% ignorant of your religion or you are a 100% liar. you tell me which.',\n",
       "  'mkr lets see who the producers think are going to be better tv - kat or nikki and katie??',\n",
       "  \"because when young women look at colleges to attended the rape statistics are more important than it's qs rankings. ye…\",\n",
       "  'i got them directly off that muslim students website.',\n",
       "  \"chortle that blind date last night is why i don't do blind dates. all she did was call me sexist because i like being chivalrous\",\n",
       "  'what does that have to do with the barbarity of islam?',\n",
       "  'i like these girls, \"we\\'re totes pretty so let\\'s get the men thinking with their spatchcocks, but how... oh!\" mkr',\n",
       "  \"i don't care who it is, just as long as it's not the promogirls mkr\",\n",
       "  'rape is not a rhetorical device. rape is not a rhetorical device. rape is not a rhetorical device.',\n",
       "  \"&gt;'feminazi' &gt;2015 stay classy, steam reviewers. you could at least use terf or something and have vague plausible deniability.\",\n",
       "  \"mkr why do i think jacs really still has no idea where she is or what's she's doing there.\",\n",
       "  \"if you're still believing that modern feminism doesn't demonize men, read a couple of tweets in blameonenotall.\",\n",
       "  'if katie &amp; nikki go, i hope one of them give kat a left hook on the way out. what a bitch. mkr',\n",
       "  'giuliani calls out obama – can’t say ‘radical islam’ because he’s a coward -',\n",
       "  'certainly the paper shows that the idea that drinking water was the only or main health issue in iraq is a lie.',\n",
       "  \"who cares. it's about the example that the religion tries to follow that determines if it can be reformed.\",\n",
       "  'god i hope kats gone soon what are stuck up &amp; -* mkr',\n",
       "  'if you\\'ll happily use the word \"feminazi\", you don\\'t get to complain about other people causing trouble.',\n",
       "  'another stupid excuse. isis is islam. not cia or mossad.',\n",
       "  \"actually, never mind. no clue who you are. really don't care. go be dramatic in someone else's mentions.\",\n",
       "  'so your argument is that islam is good because others did bad things in the past. try again.',\n",
       "  'it hurts my brain just trying to come up with sexistfacts. \"the sun is 92 million miles from earth\"?',\n",
       "  'and kat thought her bake was great. so delusional mkr',\n",
       "  'in fact, it was the jews that settled medina before the arabs. then the muslims exterminated them.',\n",
       "  \"why doesn't she know that her prophet said that most dwellers of hell were women?\",\n",
       "  \"the old school is sexist. i'm not sexist im just old school\",\n",
       "  'i will not be using killer blondes as a hash tag! mkr',\n",
       "  \"men: denigrating women's accomplishments in fields they themselves know nothing about since 100,000,000 bc\",\n",
       "  \"&amp; blondes scored fairly “@k8ie_mac: devastated that kat is still there, wasn't the biggest fan of blondes but liked them more than kat mkr”\",\n",
       "  'the next person who says feminazi will get a slap. if you think a woman is mistreating a man you can call her sexist too, not feminazi -.-',\n",
       "  'you can call a female a feminist or a \"feminazi\" but that still does not justify you being a complete inconsiderate jerk defending -',\n",
       "  'want 2 get men 2 approach other men on buses, trains, and sidewalks pestering them 2 b best friends, 24/7 and see how they fucking like it.',\n",
       "  'uc irvine, which sits amidst former bastion of ca conservatism, orange county, is a leftist, pro islam, anti-israel radi…',\n",
       "  '\"i believe in equality except when i\\'m drunk and mad at women.\" &lt;-- why i don\\'t trust male feminists',\n",
       "  \"you're a feminazi?!?\",\n",
       "  'there is no sovereignty of dictators. only real elections bestow sovereignty',\n",
       "  'declaring war on humanity and murdering 280 million people to \"allahu akbar\" is evil.',\n",
       "  \"nikki's inflection: doing my head in promogirls mkr squeal ugh\",\n",
       "  'that is a lie. the prophet mohammed had people executed for criticizing him.',\n",
       "  \"here's hoping sudden death with kat &amp; annie... how funny would that be mkr\",\n",
       "  'only two verses and we are making progress.',\n",
       "  '“@todclarey: lemon \"tart\" looks like shit mkr” please- don\\'t hold back - tell it how you see it',\n",
       "  '\"they don\\'t relate to these women on social media. these activists, these feminists. they don\\'t relate to them. they are more humble.\"',\n",
       "  \"you run your mouth like a idiot with unsupported assertions. but it's you who knows ziltch bout islam.\",\n",
       "  'coon lenny prince 198 pounds all solid',\n",
       "  'if you talk the talk, you gotta walk the walk!! do the girls know the recipe for humble pie??? mkr',\n",
       "  '... when he hears \"blue haired bitch\", \"gamergate\", and \"freebsdgirl\". oh, dear.',\n",
       "  'that is a lie.',\n",
       "  \"you know you're doing something right when mras are throwing a tantrum feminism yesallwomen feminists feminazi\",\n",
       "  'not dumb blondes...mmm...perhaps just delusions of adequacy? mkr',\n",
       "  \"mkr. kat, you snake. even if you win and open your own restaurant. who's going to go?\",\n",
       "  \"amen! stand your ground-&gt; gun range's ban on muslims draws fire tcot 2a\",\n",
       "  'stop saying sass or i will put my foot up your ass mkr',\n",
       "  'feminazi blog reminds liberals today is “national day of appreciation for abortion providers”… via',\n",
       "  'wrong again asshole. just look at the christian vs muslim population demographic for the last 30 years.',\n",
       "  'the slave girl has no right at all, liar. the owner could kill here and there would be no punishment.',\n",
       "  \"let's see, why would there be a movement against islam, but not against any other religion.\",\n",
       "  'the sassy foods feeling mkr2015 mkr these girls are vile people killerblondes basichotdog nobun',\n",
       "  'all those damn uppity women, demanding a better quality of life, always causing problems.',\n",
       "  \"tell me how old you are. i don't want to be here attacking a child about her religion.\",\n",
       "  \"isis on islam and indoctrination' by peter townsend today. free copy, get yours now\",\n",
       "  \"mkr purposely pick slightly unattractive girls who're besotted with themselves. makes good television lol\",\n",
       "  'when did kat become mrburns??? evil mkr shameonyou',\n",
       "  \"mkr you'd think in her downtime annie would have paid napoléon perdis a visit and learnt not to use the same coloured blush as your hair...\",\n",
       "  'same with feminists though. just saying you are one can lead to instant hostility or stuff like \"ur not a feminazi so ur ok',\n",
       "  'not really. the radicals follow the religion to the letter.',\n",
       "  'here is a comparison of slavery in islam as compared to the west.',\n",
       "  \"tweetstats.hashtag.notyourshield.mention._roguestar_.rt quickly, create a shield from the person that posted women's addresses on twitter!\",\n",
       "  'halloween is a busy day for sexist assholes.',\n",
       "  'but you keep returning to the stupid idea that christian barbarity justifies muslim barbarity.',\n",
       "  'the destruction of minority populations by islam continues.',\n",
       "  'so these are hadiths that i copied from the usc muslim students hadith database. look them up for yourself.',\n",
       "  \"can't get any more freaky than having sex slaves, beating women, making women slaves.\",\n",
       "  \"kat didn't like the canoodles.... mkr mkr2015\",\n",
       "  'wife beating permitted in islam? read the debate and give your opinion',\n",
       "  'arab nations deeply worried by iran nuke deal israel ccot tcot tlot gop terror',\n",
       "  \"it's fairly obvious that you are willing to throw women under the bus so that muslims can go on abusing them.\",\n",
       "  'kahlili wrote a good book explaining quantum theory that i read. but his opinions on muslim scientists are hyperbolic.',\n",
       "  \"i'm going to blow your mind: every sentence that contains an adjective signals an opinion sexistfacts (maybe not colors, maybe)\",\n",
       "  'rape is not a punch line.',\n",
       "  \"again, you present your imam's propaganda. you have no hadiths or verses.\",\n",
       "  'spirituality is a connection with god that is direct and includes no prophets or imams.',\n",
       "  'people are taking this chris paul thing way to far joke notsexist justcomplaining',\n",
       "  'is kat strategic or just a sore loser? mkr',\n",
       "  \"men acting like mythology is static and the characters don't change drastically over generations.\",\n",
       "  '\"it wouldn\\'t be fair\". kat knows nothing of fair! wtf &gt;.&lt; hypocrite mkr',\n",
       "  \"it's sugar and butter and salt\",\n",
       "  'by the way, i have to ask, do you really consider islam to be \"spiritual\"? really?',\n",
       "  'the jihadists that slaughtered french magazine editor and staff \"men \\'spoke french perfectly\\'',\n",
       "  'these white racists are unqualified and were only hired because of affirmative action for whites, otherwise known as life.',\n",
       "  'the have disgraced our country . first they covered for seville,then they failed with muslims rapes,now this …',\n",
       "  \"the meals weren't great but they didnt deserve 17... mkr\",\n",
       "  \"katie and nikki should've called their restaurant conceited not sassy mkr\",\n",
       "  'and no one said only women should stick together. non-rapist men can also stick together with their (women) friends',\n",
       "  'i just someone in a youtube comment call himself a feminist and then proceed to describe someone as a \"feminazi\" alright',\n",
       "  'no, they had no rights in a court of law and had to hide the practice of their religion from muslims and had to pay jizya.',\n",
       "  \"nuland's nazis? i didn't know that the muslims were working for nuland.\",\n",
       "  \"i was called a feminazi. well that's new.. 😜😂✌️\",\n",
       "  'oh no! no churner, good luck to the girls, they are gonna need it mkr',\n",
       "  'that, and the thought that a significant other would be targeted.',\n",
       "  'new life mantra',\n",
       "  \"in islam, there's a concept called taqiyya, which dictates that lying is acceptable if it advances the cause of allah http:/…\",\n",
       "  \"blameonenotall you don't assume all muslims are terrorists so why assume all men are rapists?\",\n",
       "  '5 muslims arrested in horrific 4 week long repeated gang rape',\n",
       "  'no, that is what you and islam do.',\n",
       "  'we are talking about what mohammed believed and is therefore a part of islam.',\n",
       "  'blondes vs blondes mkr',\n",
       "  'shares australia pm announces new counterterror measures: tony abbott says st... by arab',\n",
       "  'so is watching mkr. i have to say that kat seems like an absolute crazy bitch..'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_false_positive(data[\"y_test\"], hybrid_pred_large, data[\"x_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244,\n",
       " [\"eva &amp; debra are now my most hated team on mkr after least night's ep. snakes in the grass those two.\",\n",
       "  'stoprush: \"uses words like chickified &amp; feminazi, is a hate-monger.\" how do you actually function in this worl…',\n",
       "  'a well done, one...must say. fem i was arguing w/ had rt me a few times..so i thought this was her cavalry',\n",
       "  \"ben i'm open to meeting good feminists like helen, etc. but uh...how can i say nicely, maddy is...not smart...at all. a lemming.\",\n",
       "  'and the secret twist. kat and andre are eliminated for being cunts!!!! mkr',\n",
       "  'personally i am just glad when they use the actual word and not feminazi',\n",
       "  \".@facerealitynow i saw/rt'ed it. too funny. i saw this one today...\",\n",
       "  \"at least we know they're natural blondes mkr\",\n",
       "  \"yes i am a brony and a meninst get over it. you won't have sex with me? what kind of bullcrap is this oppression? femi…\",\n",
       "  'how can there be a biological inequality? that suggests that the process of evolution itself is s…',\n",
       "  \"i see that now...didn't get far enough into his tl.\",\n",
       "  \"i pay the bills in this household it's the least she can do\",\n",
       "  'to some, yeah...',\n",
       "  \"you call difference inequality; that's a loaded word\",\n",
       "  'makeup time for katie &amp; nikki mkr',\n",
       "  'he asked for it. did you? why not find an employer who will value you properly?',\n",
       "  'seriously. crazy-eyes is staying? mkr',\n",
       "  '.@notchrissmith \"not paid the same in hollywood\"? they each have agents who individually negotiate their deals. you want to pay a flat rate?',\n",
       "  'omg that woman is awful!!!!! mkr',\n",
       "  'mkr stop rolling the sausages like that.... giving me a \"soft-on\"!',\n",
       "  'what is that?',\n",
       "  \"i apologise on behalf of my generation for nikki and katie. we aren't all so vapid. mkr nikkiandkatie\",\n",
       "  \"colin was pretty nice to the girls. do you think he's giving them the dick? mkr\",\n",
       "  'men are attacked 4 times more, just fyi.',\n",
       "  \"here we go. fucking twitter man i hate this moving around shit. but no better way to find feminazi's lawl.\",\n",
       "  'i have watched several female comedians on several occasions, to my memory i have not laug…',\n",
       "  \"i don't think women can make tough military decisions. notice hilary's face during the bin laden raid h…\",\n",
       "  \"just wanted to point out overseas labour isn't solely women in sweatshops...\",\n",
       "  \"gamergate is only interested in game journalists and sjw's who put politics ahea…\",\n",
       "  \"the format to mkr is stupid. it'll be about 10 weeks until we see the cowboy and his fit daughter cook again.\",\n",
       "  '...there is no beef here.',\n",
       "  'odd that google can find no relevant examples of it actually being enforced then, huh?',\n",
       "  'throwing shit around because kat is impossible to deal with. mkr mkr2015',\n",
       "  '“@carlyandtresne: really!!! really!!! another round!! mkr those butcher promo girls better be nice to sheri and emilie.” yes or else',\n",
       "  'mkr \"if i could score them a zero then i would because i\\'m a super slut\". cheats irritate ratingsbonanza uglycow',\n",
       "  'looks like shit cuntandandre....not surprising mkr',\n",
       "  'if kat and andre are strategically scoring, they need to be kicked off mkr2015 mkr',\n",
       "  \"just not very appealing to me. then again, it is a tv show that revolves around women in prison. probably why i don't like…\",\n",
       "  'seriously? where exactly do you suggest i put my tits, should i detach them when doing feminist things?',\n",
       "  'via anti-feminazi',\n",
       "  \"i'm so disgusted that people like kat is allowed in this competition mkr\",\n",
       "  \"catching up on mkr, disgraceful behaviour from kat and andre. strategic scoring won't save them forever.\",\n",
       "  'was on a russian site once. it was either meninism or uva hoax, i think.',\n",
       "  \"why, because she backed straight into your balls? because that's what i would do, with scientific precision.\",\n",
       "  \"they just don't make me laugh. like whip out your tits do something entertaining\",\n",
       "  'fk you kat and andre! mkr',\n",
       "  \"i didn't realize i was supposed to treat you as a child whose opinion is irrelevant, sorry. my bad.\",\n",
       "  \"cos that's all they suck to get their jobs mkr\",\n",
       "  \"wait, that back shot of nikki in the kitchen... she's wearing blue knickers?! honey, no. 🙅🙅🙅 mkr vplonextreme\",\n",
       "  'this guy is an amazing writer regarding feminism.',\n",
       "  \"but 'inequality' is not defined as that. its defined as unequal. we're unequal in physical strength, for example.\",\n",
       "  'be a man. stop babbling and squirming and admit you were wrong.',\n",
       "  'there goes your attempt at an \"i\\'m such a deep dude\" persona. suck my balls hoe',\n",
       "  'did stevie wonder choose these \"models\"? mkr',\n",
       "  'pretty much this.',\n",
       "  'actually i have. the same thing happens to african americans, spanish, and other minorities.',\n",
       "  'blameonenotall because i wount hold all feminists and progressives responsible for the \"white genocide is good\" article',\n",
       "  'dehydrated vaginas - i mean pears - for breakfast.',\n",
       "  \"you're bat-shit insane to *allies*, and then wonder why you have so few. that's why radical feminism has al…\",\n",
       "  'who said anything about hitting?',\n",
       "  \"the motive doesn't lessen the outcome.\",\n",
       "  'is that wrong? is forcing them into parenthood wrong?',\n",
       "  'chrysanthemum is a source of natural pyrethrum. i had some growing but our libtard feminazi apt owner at sfarts killed it',\n",
       "  'stick to your day jobs girls mkr',\n",
       "  'i hate kat so much mkr',\n",
       "  \"aww i was looking forward to ash and camilla butting heads with the promo girls. the drama would've been great mkr\",\n",
       "  \"mkr oh my gawd we are like so hot i can't even...\",\n",
       "  \"did you find katie and nikki in the line for mt druitt's next top model? mkr\",\n",
       "  'wait, do you realize you just did the very thing the tweet you posted said is wrong...?',\n",
       "  'already sick of hearing \"pretty\" mkr mkr2015',\n",
       "  'damn kat &amp; andre sneak through, would much rather katie &amp; nikki had gotten through mkr damnukat',\n",
       "  'omg, you guys. they are misusing the word gaslighting repeatedly. must not have gotten to that on their sjw word of the day calendar yet.',\n",
       "  'yeah colin would be looking for more fat in his sausage.... mkr',\n",
       "  '\"we want to stay in this competition because we haven\\'t finished telling everyone how pretty we are\" mkr',\n",
       "  'are kat and andre brother and sister? mkr',\n",
       "  'has there been a more genuinely hate worthy individual than that goblin, kat? mkr',\n",
       "  'lol, that chick from last night is still raging about me blocking her, i guess.',\n",
       "  \"mkr maybe you girls should go back to cutting up cows and pigs, don't quit your day job!!\",\n",
       "  'notyourscapegoat join the party :)',\n",
       "  'five bucks says drinking.',\n",
       "  'this is the epitome of how to not blameonenotall',\n",
       "  'kats face when the score was announced made me want to puke eww wtf mkr',\n",
       "  \"valentine's day is great and all, but i'm holding out my real enthusiasm for love day:\",\n",
       "  'one should consider we actually get more tax breaks and less intensive jobs even in…',\n",
       "  'what does the \".77 on the dollar\" figure describe?',\n",
       "  \"it's insulting to women to expect them to only be attracted to super easy games. i honestly am insulted as a parent of a girl…\",\n",
       "  'thanks for heads up. i watched it. good stuff.',\n",
       "  'kat needs to calm down with the crazy eyes mkr',\n",
       "  'wrong and wrong.',\n",
       "  \"because it's a known lie. mostly.\",\n",
       "  'i was fortunate to catch this right when it was posted, 4th reply in. get rekt anita. gamergate',\n",
       "  'not me. either guns blazing or waiting quietly in my trojan horse...',\n",
       "  'refined &amp; pretty? looks like something a blind kindergarten student chucked on a plate. mkr',\n",
       "  'katandandre gaaaaah i just want to slap her back to wa mkr',\n",
       "  \"hmmm...that's a good point. bathing in male tears is more of a feminist thing, huh?\",\n",
       "  \"when i see people out &amp; about wearing their nfl team shit while that team's game is on i'm judginghard badfans usuall…\",\n",
       "  'kat calls it strategic, i call it being a biatch! mkr',\n",
       "  \"got a 1st for coburn, more than i'd have thought. hawks made a mistake/took big risk on timonen but good deal for hextall.\",\n",
       "  'obviously natural blondes!! mkr',\n",
       "  \"i hate kat's big smug face and her constant split fringe so much. kat and andre australiasmosthatedcouple mkr\",\n",
       "  \"how can you call that inequality? that dynamic exists because it's necessary for the existence of our species\",\n",
       "  'how about next season you hold off on the scores until all the teams have cooked, so strategy plays no part. food does. mkr',\n",
       "  '.@usdnobody i was sure it was fake...until it went too long...no one works that hard on a poe...just vapid childish pc trash.',\n",
       "  \"you're really fond of lying about everything. i said they aren't isis.\",\n",
       "  'mkr deconstructed by girls that have deconstructed brains ! nearly brought up my dinner when i saw that crap on the plate',\n",
       "  'a genuinely interesting question...of course, the answer is predictable and boring.',\n",
       "  'can you comment on ford doing the very thing she complains of...?',\n",
       "  'why do you judge on genitals, rather than merit? if a man works in an 80% female workplace, should he cry discrimination?',\n",
       "  'will ever post something positive i say about women? or just the negative things? spreading just hate or no?',\n",
       "  'and we still get payed equally. that stupid myth bothers me to no end because there…',\n",
       "  \"...it's backdoor communism.\",\n",
       "  '...you have an armpit licking fetish?',\n",
       "  'lately, i get more hate from sjws. but that just goes along with refusing to be used as a platform for abusing their enemies.',\n",
       "  '...like i said. a \"symbol\" is a \"symbol\". jewelry, however, can be pawned later.',\n",
       "  'hey kat, \"gloat\" is not a pretty colour to wear, love. makes you look bitchy. oh that\\'s your normal face? my mistake. mkr',\n",
       "  \"blameonenotall is not because i want to be congratulated by not being a rapist. it's because i don't wanna be categorized with stupid men.\",\n",
       "  'the one thing that could actually end the \"harassment\" of gamergate...',\n",
       "  'i felt so out of place at that party last night. there again so would any man who stands in a kitchen for that long...…',\n",
       "  'you say this based on what?',\n",
       "  'i keep hoping for her brain to engage at some point, but all factual evidence just zips by unnoticed.',\n",
       "  'kats the kind of person that would cheat to win and then still think that made her a better cook. so my new baddie mkr',\n",
       "  'an enemy of is good enough for me...f/f',\n",
       "  'what a riveting little lesson in walking the catwalk. thanks ladies mkr',\n",
       "  'there just horrible lemontarts mkr',\n",
       "  'kat is completely class less. mkr',\n",
       "  '“use ruby to get some ladies” at',\n",
       "  'are there no scruples left coon votewithyourwallet',\n",
       "  'what is the event?',\n",
       "  'thought you might like one of my comics. from unpc comix :)',\n",
       "  \"bad seeds grow weeds. must be rooted out first or else you'd foundation cracks.\",\n",
       "  'blameonenotall: do we really need to ask if feminismishate or if misandry is real after reading things like this? http://…',\n",
       "  'whatfeminismgaveme irony',\n",
       "  '.@ilivundrurbed yes. your apathy.',\n",
       "  \"do kat &amp; andre have any shame or integrity? they can't even pretend to be gracious. mkr\",\n",
       "  'we found out quickly most of these \"questions\" were not looking for answers.',\n",
       "  'but greasy haired colin is a judge so messy yuck is in 😂 mkr',\n",
       "  'i think drasco just offered colin an old fashioned wristy mkr',\n",
       "  'flyers can never get all 3 phases in sync. have had great forwards. had great d in 2010. but never find the g. if they do',\n",
       "  'mkr - i am seriously considering not watching just because i have to see kats face. god. i want to slap it with a spatula!',\n",
       "  \"with kats attitude, i think it's time to keep scores quiet until final round mkr\",\n",
       "  'yeah! why do we feed the hungry but not the full? why do we give shelter to the homeless but not the homed? so unequal',\n",
       "  \"i'm all for equal rights, just annoying that they complain about things that don't matter is …\",\n",
       "  'so she needs to give david $3 million, i guess.',\n",
       "  'fifty shades of grey is romantic only because the guy is a billionaire. if he was living in a trailer it would be a crimin…',\n",
       "  'oh my god, someone punch kat in the face. mkr',\n",
       "  'whoa. nice, uh, \"impression,\" celine.#elephant nailedit mkr',\n",
       "  'it says women must stay at home and not display themselves.',\n",
       "  'mkr your meatballs were dry, things lloyds never heard before',\n",
       "  \"...many don't have the awareness to operate in what most know to be self interest. they will defy most solutions.\",\n",
       "  'that judge whatever his name needs to stop the fake tan. he is literally orange mkr',\n",
       "  \"how can feminists claim we don't have bodily autonomy? like..being able to kill your unborn kid is not enough for you? …\",\n",
       "  \"mmm bitter-sweet victory. the 'models' are toast but we still have kat and andre. mkr\",\n",
       "  \"sorry, i won't use facts next time. i'll just go with the rest of the masses who think there are no differenc…\",\n",
       "  \"i'm sick of you useless ass people in my culture. stfu im sick off you useless ass people in my mentions\",\n",
       "  'kat strikes me as a very repulsive individual... mkr',\n",
       "  'ok. might have used the wrong word, but you see my point',\n",
       "  '.@sassnpearls they have no sense of irony or awareness. or shame. or decency. or...well, i could do this all day...',\n",
       "  'i think the hashtag got some traction. cuntandandre mkr',\n",
       "  'im starving. but im so worn the hell out from work i cant bring myself to get up, let alone go cook. single dude probs. n…',\n",
       "  'i respect your use of sprinkles are for winners.',\n",
       "  'good for you. too bad about those hurt by it or unprotected by it, right?',\n",
       "  \"blameonenotall? men don't need to try to be oppressive to be oppressive.\",\n",
       "  \"i just laughed at kane's move. kid is sick. wish they'd used geno over letang, though...\",\n",
       "  \"women drivers, what? i'd say we kick ass.\",\n",
       "  '\"sorry\" for confusing me? what am i looking at?',\n",
       "  '\"i\\'m trying so hard to be charming\" - annie. i have nothing to add. mkr',\n",
       "  \".@tigerclaud called mra's dishonest, then used one untreated mental patient to smear whole mrm.\",\n",
       "  \"have you even seen a women kick a football. i thinks it's probably genetic.\",\n",
       "  \"did you find katie and nikki in the line for mt druitt's next top model? mkr\",\n",
       "  \"kats face. i want to slap her seriously i hope they go but it doesn't look good. if they pass they won't last. they can't cook. mkr\",\n",
       "  'you are too pussy to handle kurdish girls. western boots would turn you to worm meat.',\n",
       "  \"kat and andre shouldn't be there they suck dick holes. the moles win my vote with the burnt butter icecream mkr\",\n",
       "  'kat should be eliminated for bad sportsmanship mkr mkr2015',\n",
       "  \"kat is going to be so smug she's going to be declared the particularly smug mayor of smugtown on the isle of smuggy mkr\",\n",
       "  'kat is just plain, fucking awful mkr',\n",
       "  'not concerned w/ equal here...',\n",
       "  \"remind me when i said fiction = reality? don't you have a feminist tumblr account to rant on?\",\n",
       "  \"she already stated that it's not going to happen lol .. any day now she will come out saying that editing i'd to blame mkr\",\n",
       "  \"at least we know they're natural blondes mkr\",\n",
       "  'people like you are what give feminists a bad name. gamergate is wrong, but the best defense is not alwa…',\n",
       "  'even in wrestling, it has its share of feminist authority figures! guess who called out on …',\n",
       "  'mkr promo girls are going to bring it in the next round. yeah! bring it! bring your store bought capsicums!',\n",
       "  'leave it to some microbrain to get sanctimonious by finding a way to play the race card.',\n",
       "  'take a poll and see if 1/3 of christians think gays should be killed. if they do, christianity should be banned',\n",
       "  'daesh threw a major offensive friday to show they were still strong. they only managed to show their weakness.',\n",
       "  'people confuse empathy with being scared, and that says more about them than anything.',\n",
       "  'and this proved to be true.when there was no one left to steal from in the saudi area they became broken down criminal places',\n",
       "  'isis intend turn war of islamism into ethnic war of pan arabism&amp;kurd in mosul. as evidence show they are conducting pan ara…',\n",
       "  \"it's not about any specific individuals, but about an ideaology that will always produce terrorists.\",\n",
       "  'many of the isis members are walking \"stoners\" ~ even daesh emwazi jihadijohn it\\'s been rumored he\\'s high on khat http…',\n",
       "  \"good for labour and now tory's! encouraging immigrants to enrich us, making the uk a better, freer, nicer place!\",\n",
       "  \"we don't want to teach you. we want to stop your taquiyya lying about it.\",\n",
       "  \"you must have found it in your imagination, because it doesn't exist in islam.\",\n",
       "  'new chant outside the diwan in amman: \"no tahawi, no zarqawi, we want to execute al-rishawi\"',\n",
       "  'i hope so, because in english it comes off as moronic hate mongering.',\n",
       "  \"already proven that islam abuses women &amp; i showed statistics to back it up. i'm bored with repetition.\",\n",
       "  'syria: raqqa doctors claim isis fighters sexually brutalize women; many reportedly fearful of rape',\n",
       "  'thanks for admitting that you are murderers. and check your map.',\n",
       "  'but islam hates free speech.',\n",
       "  'here, butthead, this gives a good summary of the muslim persecution of christians in palestine.',\n",
       "  'daesh price list for yezidi and christian sex slaves.',\n",
       "  '\"he can\\'t be a server at our restaurant, that beard makes him look like a terrorist.\" everyone laughs. fuckthanksgiving',\n",
       "  'good morning pissed off american patriots! cops in danger?',\n",
       "  \"my review got downvotes because gamergate dogpiled. but knows this. heck, he's friends with nero.\",\n",
       "  'no he\\'s not. he\\'s an idiot. he retweets things like \"nigger hunting licenses\" that never existed. he\\'s a fraud.',\n",
       "  \"i don't see any burning civilians, and the picture is not even of a burning house.\",\n",
       "  \"if you don't want to read a pedo, you have to stop reading the quran.\",\n",
       "  'dangerous to outsource iraq security to shia militia known for \"disappearances\" &amp; executions.',\n",
       "  \"it's great that you provided arraf's name. now i understand how these anti semitic orgs. function.\",\n",
       "  'do you have a way to kill the daesh scum that are hiding behind civilians with hurting civilians?',\n",
       "  'then why do you smelly rag heads keep moving to britian?',\n",
       "  'so, are people who support murder for apostacy guilty of anything?',\n",
       "  \"i'm talking total extermination, bragged about right there in the book.\",\n",
       "  'you will see that loss of men and weapons accelerate land loss in the next few months. the kurds are on the mosul doorstep.',\n",
       "  'islamic documentation is based on belief. historical documentation is based of factual research.',\n",
       "  'the islamist were there the whole time. it was only the dictators that kept them quite.',\n",
       "  'literally, every single approach road or track or open field to mosul is covered by peshmerga outposts now',\n",
       "  'wakeuplibs-&gt;.@joenbc: islamic jihadis like “ultrafundamentalist christians” tcot iloveobama',\n",
       "  'a again, give me a verse number that shows that isis is not following islam.',\n",
       "  'roflmao. where is the picture daeshbag?',\n",
       "  \"son read blameonenotall. this is why i don't have a problem with generalization about black men. i know i don't do certain things.\",\n",
       "  'this is well-written &amp; worth reading on daily life in mosul today \"caliphatalism?\" via',\n",
       "  'look at this ranting lunatic palestinian. interesting what he tells us.',\n",
       "  'the zionist label is just something scum like you use to attack jews.',\n",
       "  'and no doubt your friend was every place where isis could have been murdering children.',\n",
       "  'and if someone risks their life to save me from a mass grave, they are welcome to my tv and toaster.',\n",
       "  '😘😻#cat mainecoon cute kitten best_cats coon relax meow funny fun tiger…',\n",
       "  'and you can only get highly educated people out of a free modern society, not a 7th century one based on superstition.',\n",
       "  'quoting from the book of violent fairy tales again?',\n",
       "  'yeap, this is a big problem. the shia militia is only slightly less fanatical and islamofascist than the daesh they are replacing.',\n",
       "  'kobane west front ypg forces liberated the village of \" qartl \" 17 km from tell-abyad city',\n",
       "  'get on my level, son! funrun2 pepcman coon yomama',\n",
       "  'i know you have religious police telling how to dress.',\n",
       "  \"madrassa math. daesh threw manpower at kobane in far disproportion to it's size.\",\n",
       "  '---&gt; perfect example of a coon. still can\\'t get over the \"white nubian goddess\" comment.... foh!!!! coon let\\'s talk about that!',\n",
       "  'so now you taquiyya liars deny what is explicit in the quran.',\n",
       "  'are there no scruples left coon votewithyourwallet',\n",
       "  \"all of you islamists repeat the same brain dead lines that someone else told you to say. i've seen that one a thousand times.\",\n",
       "  'there are many supposed americans that work to destroy america.',\n",
       "  \"i dont have to live there to know you can't leave house without male relative\",\n",
       "  \"in fact blumenthal's palestinian friends behead gays.\",\n",
       "  'compare a netanyahu speech to this genocidal palestinian lunatic.',\n",
       "  'guys in guantanamo are militants captured in war. people in isis cages are women and children, microbrain.',\n",
       "  'muhammad was the inventor of thighing. he started molesting aisha when she was 6 years old. cc:'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_false_negative(data[\"y_test\"], hybrid_pred_large, data[\"x_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_true_positive = 0\n",
    "count = 0\n",
    "for i in range(len(hybrid_pred)):\n",
    "    if hybrid_pred[i] == 1 and y_test[i] == 1:\n",
    "        total_true_positive += 1\n",
    "        if hybrid_pred_large[i] == 1:\n",
    "            count += 1\n",
    "    \n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420289855072463"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/total_true_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_true_negative = 0\n",
    "count = 0\n",
    "for i in range(len(hybrid_pred)):\n",
    "    if hybrid_pred[i] == 0 and y_test[i] == 0:\n",
    "        total_true_negative += 1\n",
    "        if hybrid_pred_large[i] == 0:\n",
    "            count += 1\n",
    "    \n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9514388489208633"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/total_true_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## final_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2754"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred = np.zeros(len(y_test))\n",
    "len(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_positives_idx = []\n",
    "pred_positives = []\n",
    "for i in range(len(final_pred)):\n",
    "    if hybrid_pred_large[i] == 1:\n",
    "        pred_positives_idx.append(i)\n",
    "        pred_positives.append(data[\"x_test\"][i])\n",
    "len(pred_positives_idx)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver_paths = [\"./logs/combined/word/ckpt\"]\n",
    "checkpoint_files = list(map(tf.train.get_checkpoint_state, saver_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[model_checkpoint_path: \"/home/homes/jhpark/hate-speech/logs/combined/word/ckpt/model-final.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/combined/word/ckpt/model-360000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/combined/word/ckpt/model-370000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/combined/word/ckpt/model-380000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/combined/word/ckpt/model-390000.ckpt\"\n",
       " all_model_checkpoint_paths: \"/home/homes/jhpark/hate-speech/logs/combined/word/ckpt/model-final.ckpt\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Summary:\n",
      "Train: Total Positive Labels=1441 (0.3476)\n",
      "Test: Total Positive Labels=309 (0.3476)\n",
      "\n",
      "dataset passed the assertion test\n",
      "31\n",
      "2880\n"
     ]
    }
   ],
   "source": [
    "from data.hybrid import load_data_from_file\n",
    "\n",
    "(_x_train, _y_train, _x_test, _y_test, _initW, _vocab) = load_data_from_file(\"combined_valid_binary\")\n",
    "_word_text_len = _x_train[0][\"word\"].shape[0]\n",
    "_word_vocab_size = len(_vocab.vocabulary_)\n",
    "print(_word_text_len)\n",
    "print(_word_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['or', 'you', 'ending', 'your', 'feminazi', 'reign', 'on', 'this', 'earth']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['james', 'are', 'you', 'being', 'a', 'pc', 'feminazi']\n",
      "changed data 37 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yeah', 'would', 'never', 'im', 'not', 'sexist', 'but', 'women', 'please', 'just', 'stop', 'tweeting', 'about', 'football', 'we', \"don't\", 'tweet', 'about', 'cooking']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'believe', 'in', 'equal', 'rights', 'there', 'for', 'am', 'not', 'sexist', 'but', \"women's\", 'reaction', 'times', 'general', 'awareness', '2', 'whats', 'going', 'on', 'whilst', 'd']\n",
      "changed data 127 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'face', 'kat', 'just', 'pulled', 'was', 'hideous', 'mkr']\n",
      "changed data 42 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'or', 'a', 'scientist', 'or', 'researcher', 'but', 'the', 'stuff', 'girls', 'write', 'and', 'put', 'on', 'social', 'networks', 'is', 'evidence', 'that', 'men', 'are', 'sm']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ok', 'men', 'have', 'fewer', 'legal', 'rights', 'in', 'every', 'western', 'nation', 'today', 'get', 'to', 'work']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'kat', 'the', 'wicked', 'witch', 'from', 'the', 'west']\n",
      "changed data 39 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['rape', 'by', 'fraud', 'feminazi', 'strike', 'again', 'via']\n",
      "changed data 42 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'is', 'actually', 'a', 'bitch', 'but', 'i', 'had', 'to', 'laugh', 'at', 'her', 'response', 'mkr']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['steven', 'crowder', 'obliterates', 'gender', 'gap', 'myth', 'with', 'simple', 'economics', 'video', 'feminism', 'equal', 'pay', 'feminazi']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['should', 'women', 'be', 'united', 'or', 'honest']\n",
      "changed data 33 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'know', \"we're\", '50', 'of', 'the', 'population', 'right', 'im', 'not', 'sexist', 'but', 'i', 'do', 'feel', 'there', 'are', 'too', 'many', 'women', 'on', 'espn', 'not', 'the', 'same']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['this', 'is', 'a', 'woman', 'she', 'is', 'flat', 'out', 'obsessed']\n",
      "changed data 42 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'always', 'fun', 'watching', 'sports', 'w', 'a', 'woman', 'in', 'the', 'room', 'my', 'aunt', 'not', 'sexist']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mr', 'fanta', 'pants', 'how', 'long', 'until', 'kat', 'heats', 'up', 'her', 'crack', 'pipe', 'over', 'the', 'bbq', 'mkr', 'think', 'she', 'already', 'has', 'crazy', 'bitch']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['when', 'i', 'criticised', 'the', 'he', 'for', 'she', 'campaign', 'and', 'a', 'feminist', 'responded', '12', 'women', 'against', 'feminism']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'olympics', 'etc', 'considers', 'marijuana', 'a', 'performance', 'enhancing', 'drug', 'yet', 'you', 'can', 'get', 'a', 'dui', 'or', 'dwi', 'or', 'w', 'e', 'from', 'it', 'con']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['she', 'is', 'painfully', 'stupid', 'ben', 'sorry', \"it's\", 'now', 'hurting', 'my', 'brain']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'paras', 'here', 'are', 'the', 'elite', 'of', 'the', 'british', 'army', 'most', 'applicants', 'fail', 'p', 'company', 'im', 'not', 'sexist', 'but', 'women', 'will', 'nev']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'if', 'there', 'is', 'house', 'fire', 'and', 'your', 'in', 'it', 'you', 'better', 'finish', 'the', 'dishes', 'dear']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'only', 'corrie', 'has', 'not', 'run', 'in', 'front', 'of', 'it', 'what', 'can', 'we', 'expect', 'from', 'a', 'liberal', 'feminazi', 'but', 'idiocy']\n",
      "changed data 96 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'just', 'saying', 'guys', 'are', 'tough', 'all', 'month', 'women', 'are', 'tough', 'for', '25', 'days', 'of', 'the', 'month', 'and', 'then', 'complain', 'they', 'are']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'kidding', 'when', 'i', 'say', 'it', 'today', 'feminism', 'is', 'hate', 'nice', 'feminists', 'should', 'stop', 'ca', 'ling', 'themselves', 'that']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'worried', 'about', 'a', 'bitch', 'that', 'live', 'with', 'her', 'mother', 'you', \"can't\", 'even', 'get', 'a', 'place', 'in', 'your', 'name', 'you', 'got', 'rules', 'stay', 'in', 'a', 'childs', 'place', 'coon']\n",
      "changed data 138 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['perturbed', 'by', 'neighborhood', 'strip', 'club', 'feminist', 'asserts', 'women', 'are', 'people', '100', 'of', 'the', 'time']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"there's\", 'a', 'platform', 'that', 'could', 'easily', 'collapse', 'under', 'her', 'anti', 'sjw', 'women', 'against', 'feminism']\n",
      "changed data 84 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['but', 'but', 'women', 'can', 't', 'rape', 'no', 'this', 'breaks', 'ma', 'feminazi', 'brains', 'z']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['look', 'im', 'not', 'sexist', 'but', 'tell', 'me', 'you', \"don't\", 'get', 'at', 'least', 'a', 'little', 'nervous', 'when', 'you', 'see', 'a', 'woman', 'truck', 'driver']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'drivers', 'are', 'bad', 'and', 'when', 'i', 'mean', 'bad', 'i', 'mean', 'bad']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', 'hate', 'female', 'vocalists', 'normally', 'except', 'haley', 'williams', 'from', 'paramore', 'chicks', 'got', 'some', 'pipes', 'on', 'her']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['time', 'machine', 'broken', 'again', 'socrates', 'you', 'can', 'call', 'me', 'sexist', 'and', 'maybe', 'thats', 'true', 'but', 'i', 'dont', 'think', 'a', 'woman', 'should', 'be', 'president']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['women', 'r', 'more', 'pleasing', 'to', 'look', 'upon', 'not', 'sexist', 'sex', 'honest']\n",
      "changed data 55 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'was', 'out', 'shopping', 'so', 'i', 'just', 'caught', 'up', 'on', 'the', 'ep', 'does', 'anyone', 'think', 'bobble', 'head', 'when', 'they', 'see', 'lloyd', 'kat', 'still', 'a', 'cunt', 'still', \"can't\", 'cook']\n",
      "changed data 139 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['has', 'to', 'be', 'a', 'parody', 'account', 'im', 'not', 'sexist', 'but', 'women', 'just', \"shouldn't\", 'be', 'sports', 'announcers']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['these', 'girls', 'are', 'fucktards', 'mkr']\n",
      "changed data 31 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['marys', 'just', 'a', 'cunt', 'in', 'general', 'tbh', 'like', 'i', \"don't\", 'need', 'your', 'feminazi', 'shit', 'all', 'over', 'my', 'timeline', 'and', 'dashboard']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['maybe', 'the', 'girls', 'should', 'have', 'less', 'tickets', 'on', 'themselves', 'and', 'worry', 'about', 'the', 'cooking', 'mkr']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['who', 'was', 'responsible', 'for', 'convincing', 'these', 'girls', 'they', 'were', 'so', 'pretty', 'mkr']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['faq', 'in', 'hell', 'why', 'the', 'hell', 'did', 'you', 'break', 'it', 'feminazi', 'fake', 'case', 'false', 'rape', '498aorg', 'law', 'misuse', 'men', 'suicide', 'fatherless', 's']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'swear', 'im', 'not', 'sexist', 'but', 'all', 'girl', 'drummers', 'look', 'really', 'awkward', 'whilst', 'playing']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['pretty', 'sure', 'reverse', 'moves', 'the', 'bus', 'not', 'the', 'land', 'im', 'not', 'sexist', 'but', 'this', 'female', 'bus', 'driver', \"can't\", 'reverse', 'property']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['trolling', 'you', 'cuz', 'you', 'used', 'more', 'than', '4', 'words', 'sentenced', 'to', 'incessant', 'needlepoint', 'in', 'a', 'neo', 'communist', 'feminazi', 'gula']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'drivers', 'are', 'terrible']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'or', 'whatever', 'but', 'i', 'do', 'not', 'like', 'female', 'play', 'by', 'play', 'announcers', 'for', 'football', 'just', \"doesn't\", 'work', 'for', 'me']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'every', 'time', 'i', 'get', 'road', 'rage', \"it's\", 'because', 'of', 'a', 'woman']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'idea', 'what', 'that', 'is']\n",
      "changed data 23 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['why', \"wouldn't\", 'he', 'because', 'some', 'sjw', 'feminazi', 'starts', 'crying', 'about', 'how', 'bad', 'of', 'a', 'person', 'he', 'is']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['male', 'opinion', 'presented', 'as', 'fact', 'alert', 'call', 'me', 'sexist', 'but', 'there', 'has', 'never', 'been', 'a', 'funny', 'woman', 'stand', 'up', 'comedian']\n",
      "changed data 109 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['consider', 'all', 'these', 'man', 'hating', 'people', 'as', 'feminists', 'they', 'do', 'nothing', 'but', 'ruin', 'the', 'word']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"we'd\", 'be', 'ranting', 'feminazi', 'bitches', 'during', 'the', 'wrong', 'time', 'of', 'the', 'month']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['tokyo', 'hot', 'n1049', 'endless', 'sex', 'drive', 'daily', 'xl', 'over', 'jav', 'asian']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'dislike', 'kat', 'more', 'than', 'the', 'blondes', 'who', 'are', 'going', 'to', 'tell', 'us', \"they're\", 'hot', 'another', '50000', 'times', 'mkr', 'not', 'australias', 'next', 'top', 'model']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['is', 'she', 'just', 'less', 'bad', 'that', 'other', 'feminists', 'or', 'genuinely', 'advocates', 'equal', 'equality', 'a', 'la', 'chs', 'or', 'young']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', \"it's\", 'her', 'choice', 'alone', 'yes', 'do', 'you', 'oppose', 'single', 'mothers']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'if', 'cunt', 'and', 'arsehole', 'remain', 'you', 'are', 'a', 'bunch', 'of', 'rigged', 'cunts', 'fuck', 'off', 'shit', 'cooks', 'cant', 'cook', 'kat', 'and', 'andre']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['when', 'i', 'criticised', 'the', 'he', 'for', 'she', 'campaign', 'and', 'a', 'feminist', 'responded', '22', 'women', 'against', 'feminism']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"she's\", 'crying', 'over', 'how', 'poorly', 'she', 'raised', 'you', 'wheres', 'my', 'mother', 'im', 'in', 'need', 'of', 'a', 'sandwich', 'not', 'sexist']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['girls', 'should', 'not', 'i', 'repeat', 'should', 'not', 'participate', 'in', 'no', 'shave', 'november']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'may', 'stop', 'watching', 'the', 'nba', 'if', 'o', 'phra', 'has', 'any', 'ownership', 'of', 'a', 'professional', 'basketball', 'team', 'not', 'sexist', 'but', 'plenty', 'of', 'other', 'qu']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['espn', \"doesn't\", 'care', 'what', 'you', 'think', 'im', 'not', 'sexist', 'but', 'female', 'sports', 'reporters', 'need', 'to', 'come', 'to', 'an', 'end', 'immediately']\n",
      "changed data 112 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['also', 'for', 'the', 'record', 'im', 'not', 'some', 'feminazi', 'just', 'tired', 'of', 'the', 'lack', 'of', 'equality', 'in', 'every', 'aspect', 'of', 'the', 'world', 'right', 'now']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'what', 'ferrari', 'was', 'it', 'a', 'red', 'one']\n",
      "changed data 55 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'killer', 'blondes', 'model', 'puh', 'lease', 'did', 'the', 'local', 'thrift', 'shop', 'put', 'on', 'a', 'fashion', 'show', 'and', 'should', 'we', 'be', 'able', 'to', 'see', 'her']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'a', 'dumb', 'bitch', 'mkr']\n",
      "changed data 21 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'females', 'now', 'are', 'trying', 'to', 'say', 'that', \"they're\", 'better', 'than', 'men', 'when', 'they', 'went', 'through', 'so', 'much', 'history', 'saying']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', 'do', 'not', 'like', 'one', 'woman', 'rapper', 'out']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', \"woman's\", 'football', 'makes', 'me', 'feel', 'sick']\n",
      "changed data 54 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'yet', 'somehow', 'they', 'thrive', 'without', 'your', 'ignorant', 'laughter', 'call', 'me', 'sexist', 'but', 'i', 'just', \"don't\", 'find', 'women', 'funny']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['algerian', 'sheikh', 'we', 'have', 'no', 'need', 'for', \"women's\", 'rights', 'islam', 'sharia', 'womens', 'rights']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'let', 'me', 'clarify', 'all', 'feminists', 'are', 'not', 'femi', 'nazis']\n",
      "changed data 51 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['questions', 'for', 'men', 'do', 'you', 'ever', 'find', 'it', 'hard', 'to', 'take', 'a', 'seriously', 'when', 'it', 'is', 'designed', 'by', 'a', 'hardcore', 'sexist']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'think', \"you'd\", 'better', 'lie', 'down', '8', 'replies', 'that', 'are', 'all', 'caked', 'in', 'feminazi', 'bullshit', 'tbh']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yay', 'just', 'when', 'we', 'drowned', 'with', 'the', 'last', 'on', 'sided', 'feminazi', 'rape', 'public', 'video', 'one', 'more', 's', 'comes', 'along']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'quoted', 'it', 'you', 'feminazi']\n",
      "changed data 24 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['huh', 'feminism', 'is', 'for', 'fucking', 'women', 'sign', 'me', 'up']\n",
      "changed data 49 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['bro', 'my', 'icon', 'is', 'literally', 'feminazi', 'm', 'is', 'an', 'andrist', 'hyun', 'ae', 'pick', 'axing', 'a', 'snoot', 'his', 'might', 'be', 'the', 'weakest', 'burn', 'ever', 'males', 'arent', 'funny']\n",
      "changed data 127 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'just', \"can't\", 'drive']\n",
      "changed data 41 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'women', 'ruled', 'the', 'world', 'feminism', 'is', 'awful', 'feminazi', 'women', 'against', 'feminism']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['abnormal', 'cy', 'bias', 'women', 'who', 'are', 'actually', 'charged', 'convicted', 'of', 'it', \"it's\", 'almost', 'never', 'prosecuted', 'cops', 'just', 'close', 'the', 'case']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'boring', 'im', 'not', 'sexist', 'but', 'women', 'should', 'not', 'drive', 'ever']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hey', 'adelaide', 'fringe', '2015', 'only', '8', 'shows', 'left', 'of', 'feminazi', 'at', '950', 'pm', 'come', 'la', 'rf', 'heaps', 'about', 'misogyny', 'see', 'you']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'female', 'football', 'commentators', 'are', 'just', 'not', 'a', 'good', 'idea']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['as', 'soon', 'as', 'we', 'cancel', 'all', 'holidays', 'honoring', 'men', 'im', 'not', 'sexist', 'but', 'can', 'we', 'stop', 'making', 'up', 'holidays', 'for', 'women']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['back', 'then', 'men', 'were', 'in', 'charge', 'and', 'women', 'were', 'obligated', 'to', 'them', 'today', 'women', 'are', 'just', 'as', 'equal', 'and', 'even', 'better', 'not', 'sex']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['michael', 'mutes', 'all', 'women', 'sportscasters', 'not', 'sexist']\n",
      "changed data 47 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"they're\", 'staying', 'just', 'so', 'that', 'the', 'producers', 'can', 'have', 'a', 'showdown', 'between', 'them', 'and', 'the', 'posh', 'bitches', 'mkr', 'bring', 'it']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['stop', 'being', 'a', 'feminazi', 'because', 'you', 'struggle', 'at', 'attracting', 'the', 'other', 'sex', 'stop', 'starting', 'dr']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', 'hate', 'watching', 'women', 'play', 'sports']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['shut', 'up', 'kat', 'mkr']\n",
      "changed data 16 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'menus', 'look', 'like', 'they', 'were', 'made', 'by', 'a', '5', 'year', 'old', 'little', 'girl', 'in', 'this', 'case', 'just', 'the', 'mental', 'age', 'of', 'a', '5', 'year', 'old', 'girl', 'i', 'guess', 'mkr']\n",
      "changed data 130 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'joking', 'when', 'i', 'tell', 'a', 'woman', 'to', 'make', 'me', 'a', 'sandwich', 'but', 'if', 'i', 'want', 'spaghetti', 'bitch', 'better', 'make', 'it', 'not', 'sexist', 'spaghetti']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['pray', 'tell', 'when', 'will', 'ready', 'be', 'guessing', 'never', 'im', 'not', 'sexist', 'but', 'i', \"don't\", 'think', 'america', 'is', 'ready', 'for', 'a', 'female', 'president']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['but', 'what', 'about', 'that', 'one', 'feminazi', 'i', 'saw', 'suplex', 'ing', 'a', 'teen', 'boy', 'for', 'a', 'flippant', 'remark', 'misandry']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'in', 'the', 'slightest', 'but', 'women', 'comedians', 'should', 'not', 'be', 'allowed', 'they', \"aren't\", 'funny', 'at', 'all']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', 'mg', 'to', 'w', 'men', 'going', 'their', 'own', 'way']\n",
      "changed data 34 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kind', 'of', 'want', 'to', 'jump', 'through', 'my', 'tv', 'and', 'punch', 'kat', 'in', 'the', 'face', 'hope', 'other', 'contestants', 'vote', 'strategically', 'for', 'her', 'now', 'bitch', 'mkr']\n",
      "changed data 126 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', 'am', 'i', 'the', 'only', 'person', 'who', \"doesn't\", 'find', 'any', 'female', 'comedian', 'funny', 'in', 'any', 'way', 'at', 'all']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'i', 'hate', 'female', 'sports', 'announcers']\n",
      "changed data 46 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['out', 'of', 'the', 'two', 'blondes', 'which', 'one', 'is', 'beavis', 'and', 'which', 'is', 'butthead', 'mkr']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'liked', 'a', 'video', 'from', 'feminazi', 'fail']\n",
      "changed data 34 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['fat', 'amy', \"isn't\", 'funny', 'because', \"she's\", 'a', 'women', 'i', 'personally', \"don't\", 'think', 'women', 'are', 'funny', 'at', 'all', 'not', 'sexist', 'real', 'talk']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['girls', 'should', 'not', 'care', 'about', 'your', 'opinion', 'girls', 'should', 'not', 'participate', 'in', 'no', 'shave', 'november']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', 'feel', 'like', 'she', 'should', 'be', 'in', 'her', 'own', 'level', 'clearly', 'referring', 'men', 'is', 'not', 'your', 'calling']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['sooo', 'true']\n",
      "changed data 12 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"here's\", 'another', 'good', 'one', 'never', 'trust', 'a', 'feminazi']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kate', 'you', 'stupid', 'woman', 'mkr']\n",
      "changed data 26 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'do', 'cook', 'nd', 'clean', 'better', 'then', 'us', 'its', 'a', 'compliment']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['remember', 'gaga', 'what', 'would', 'you', 'guess', 'her', 'iq', 'to', 'be']\n",
      "changed data 51 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"doesn't\", 'make', 'me', 'sexist', 'i', 'have', 'a', 'right', 'to', 'an', 'opinion', 'i', 'believe', 'in', 'equality', 'for', 'those', 'who', 'are', 'equal', 'man', 'or', 'wo']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', 'blog', 'reminds', 'liberals', 'today', 'is', 'national', 'day', 'of', 'appreciation', 'for', 'abortion', 'providers', 'via']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', 'actively', 'avoid', 'supporting', 'women', 'athletes', 'and', 'then', 'act', 'defensive', 'about', 'it']\n",
      "changed data 96 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['dumb', 'blondes', 'with', 'pretty', 'faces', \"you're\", 'definitely', 'right', 'on', 'one', 'of', 'those', 'statements', 'guess', 'which', 'one', 'mkr']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'all', 'you', 'like', 'love', 'i', \"wasn't\", 'the', 'one', 'driving', 'at', 'speed', 'the', 'wrong', 'way', 'around', 'a', 'very', 'tight', 'car', 'park', 'with', 'a']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'that', 'guy', 'is', 'never', 'going', 'to', 'live', 'down', 'losing', 'to', 'that', 'girl', 'in', 'a', 'football', 'throwing', 'contest', 'for', '100k']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', \"it's\", 'just', 'funny', 'to', 'attack', \"women's\", 'rights', 'because', 'they', 'all', 'get', 'so', 'defensive']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['classic', 'im', 'not', 'sexist', 'but', 'i', 'do', 'believe', 'that', 'women', 'are', 'inferior', 'to', 'men']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', 'i', \"can't\", 'stand', 'females', 'being', 'announcers', 'for', 'male', 'sports']\n",
      "changed data 74 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', '2', 'girl', 'refs', 'really']\n",
      "changed data 38 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'women', 'skaters', \"can't\", 'fall', 'and', 'make', 'it', 'look', 'graceful', 'like', 'the', 'men', 'not', 'sexist']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'sound', 'dumb', 'end', 'of', 'story', 'quit', 'crying', 'and', 'making', 'a', 'big', 'deal', 'of', 'little', 'things', 'tf']\n",
      "changed data 82 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', 'feminazi']\n",
      "changed data 17 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['omg', 'crazy', 'eyes', 'aka', 'kat', 'is', 'crazy', 'karma', 'got', 'her', 'back', 'after', 'she', 'laughed', 'at', 'annies', 'dish', 'mkr', 'mkr', '15', 'crazy', 'eyes']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'tired', 'of', 'all', 'you', 'femi', 'nazis']\n",
      "changed data 31 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['as', 'a', 'mom', 'of', 'daughters', 'im', 'asking', 'if', 'u', 'would', 'call', 'them', 'sluts', 'sex', 'retary', 'or', 'feminazi', 'rush', 'does', 'you', 'pay']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hmm', 'cathy', 'young', 'and', 'who', 'are', 'other', 'two']\n",
      "changed data 39 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['is', 'this', 'lang', 'not', 'sexist', 'but', 'sports', 'women', 'is', 'the', 'worst', 'thing', \"what's\", 'ever', 'happened', 'to', 'sky', 'sports', 'news', 'thats', 'what', 'loose', 'womens', 'for']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'hope', 'twitter', 'rips', 'you', 'a', 'new', 'one', 'you', 'make', 'me', 'sick', 'how', 'to', 'discribe', 'rape', 'his', 'dick', 'was', 'hungry', 'not', 'sexist']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['please', 'answer']\n",
      "changed data 16 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['everything', 'is', 'deconstructed', \"they're\", 'no', 'good', 'at', 'erections', 'then', 'mkr']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['for', 'halloween', 'most', 'or', 'you', 'girls', 'should', 'just', 'go', 'without', 'makeup']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['someone', 'that', 'will', 'be', 'like', 'randi', 'put', 'down', 'the', 'toffee', \"you're\", 'going', 'to', 'get', 'a', 'stomach', 'ache', 'i', 'have', 'a', 'stomach', 'ache']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['wonder', 'how', 'many', 'feminazi', 'social', 'justice', 'warrior', 'heads', 'explode', 'when', 'hillary', 'makes', 'villaraigosa', 'her', 'running', 'mate', 'hopefully']\n",
      "changed data 129 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['now', 'im', 'definitely', 'not', 'sexist', 'but', 'if', \"you're\", 'gonna', 'be', 'awkwardly', 'feminist', 'i', 'will', 'call', 'you', 'poppet', 'every', 'time', 'i', 'see', 'you']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'hate', 'when', 'women', 'talk', 'football', 'but', \"don't\", 'know', 'what', \"they're\", 'talking', 'about', \"it's\", 'only', 'cute', 'when', 'you', 'know', 'your', 'shit', 'no', 'sexist']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['now', 'im', 'not', 'sexist', 'but', \"it's\", 'evident', 'a', 'woman', \"can't\", 'do', 'a', 'mans', 'job', 'when', 'it', 'comes', 'to', 'tennis', 'coaching', 'mauresmo', 'out']\n",
      "changed data 109 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'there', 'are', 'women', 'everywhere']\n",
      "changed data 45 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['questions', 'for', 'men', 'women', 'against', 'feminism', 'feminism', 'my', 'body', 'my', 'choice']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'seem', 'like', 'a', 'nice', 'girl', 'i', 'just', 'ask', 'that', 'u', 'dig', 'a', 'little', 'deeper', 'into', 'what', 'feminism', 'actually', 'says', 'does', 'vs', 'what', 'they', 'claim']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminism', 'is', 'okay', 'believing', 'in', 'a', 'woman', 'overman', 'feminazi', 'world', 'is', 'not', 'women', 'men', 'men', 'women']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['irrelevant', 'girl', 'give', 'it', '5', 'years', \"she's\", 'going', 'to', 'lose', 'sexual', 'market', 'value', 'then', 'ill', 'ask', 'her', 'to', 'pass', 'the', 'bag']\n",
      "changed data 109 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['correction', 'mkr', 'katie', 'and', 'nikki', 'are', 'really', 'the', 'dumb', 'blonde', 'ones']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ugh', 'fuck', 'off', 'kat', 'your', 'turns', 'next', 'mkr']\n",
      "changed data 39 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hahahah', 'fuck', 'you', 'call', 'me', 'sexist', 'but', 'girls', 'just', \"shouldn't\", 'curse']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['cunt', 'and', 'andre', 'cunt', 'and', 'andre', 'cunt', 'and', 'andre', 'mkr', 'biggest', 'flogs', 'the', 'show', 'has', 'ever', 'had', 'on', 'it', 'ever', 'cant', 'cook', 'fuck', 'off', 'kat', 'and', 'andre']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'you', 'want', 'to', 'talk', 'about', 'buzz', 'terms', 'then', 'feminazi', 'is', 'one', \"that's\", 'used', 'more', 'to', 'decry', 'any', 'attempt', 'to', 'promote', 'women', 'in', 'society']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'goes', 'the', 'man', 'woman', 'wage', 'gap', 'sham', 'in', 'shambles', 'like', 'all', 'other', 'feminazi', 'falsities']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'liked', 'a', 'video', 'male', 'rights', 'activist', 'owns', 'feminazi']\n",
      "changed data 50 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', 'admins']\n",
      "changed data 15 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['gonna', 'miss', 'nikki', 's', 'resting', 'bitch', 'face', 'much', 'better', 'than', 'looking', 'at', 'kats', 'sour', 'c', 't', 'face', 'mkr']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['oh', 'snap', \"it's\", 'the', 'masculinity', 'police', 'chief', 'also', 'on', 'that', 'note', 'any', 'man', 'who', 'wears', 'fake', 'tan', \"isn't\", 'a', 'man', 'not', 'sexist', 'just', 'honest']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'cannot', 'drive', 'life', 'facts']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['priceless', 'that', 'bring', 'back', 'clarkson', 'got', 'more', 'petitioners', 'in', 'a', 'day', 'than', 'feminazi', 'led', 'no', 'more', 'page', '3', 'got', 'in', '3', 'years', 'oh', 'the', 'delicious', 'irony']\n",
      "changed data 137 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'my', 'fault', 'people', 'feed', 'stereotypes', 'for', 'example', 'my', 'sister', \"she's\", 'been', 'in', 'multiple', 'accidents']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['nope', 'fact', 'women', 'talk', '3', 'times', 'more', 'than', 'men', 'not', 'sexist']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['are', 'you', 'going', 'to', 'go', 'full', 'feminazi', 'on', 'me']\n",
      "changed data 40 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'swear', 'im', 'not', 'sexist', 'but', 'about', '90', 'of', 'girls', 'are', 'terrible', 'at', 'video', 'games', 'maybe', \"it's\", 'just', 'my', 'sister', 'though']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['radical', 'lol', 'no', 'call', 'me', 'sexist', 'but', 'a', 'girl', 'playing', 'bf4', 'lol', 'no']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'gags29', 'if', 'feminists', 'knew', 'how', 'many', 'women', 'internalized', 'my', 'misogyny', 'they', 'd', 'need', 'safe', 'space', 'women', 'against', 'feminism']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'did', 'it', 'i', 'made', 'a', 'feminist', 'admit', \"she's\", 'been', 'beat', 'feminazi', 'got', 'told']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['half', 'the', 'time', 'when', 'i', 'read', 'a', 'girls', 'tweet', 'i', 'read', 'like', 'the', 'dude', 'in', 'sh', 't', 'sorority', 'girls', 'say', 'video', 'on', 'youtube', 'not', 'sexist', 'j']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['nope', 'call', 'me', 'sexist', 'but', 'gender', 'roles', 'are', 'definitely', 'to', 'be', 'accepted', 'respected']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', 'social', 'justice', 'warriors', 'are', 'truly', 'the', 'overripe', 'gross', 'bananas', 'of', 'the', \"women's\", 'movement']\n",
      "changed data 96 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', \"shouldn't\", 'everyone', 'on', 'the', 'panel', 'have', 'either', 'played', 'or', 'coached', 'at', 'the', 'collegiate', 'level']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'im', 'really', 'glad', 'im', 'a', 'guy', 'lol']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"there's\", 'no', 'real', 'technique', 'to', 'the', 'dishes', 'being', 'served', 'up', 'by', 'these', 'blonde', 'bimbos', 'on', 'mkr', 'chuck', 'together', 'hope', 'for', 'the', 'best']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['need', 'to', 'learn', 'comedic', 'timing', 'so', 'it', \"doesn't\", 'come', 'off', 'as', 'psychotic', 'feminazi', 'ranting']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'need', 'men', 'in', 'ism', 'because', 'watching', 'feminists', 'their', 'lapdogs', 'have', 'conniption', 's', 'mock', 'a', 'movement', 'created', 'solely', 'to', 'mock', 'their', 'movement', 'too', 'funny']\n",
      "changed data 143 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['get', 'gordon', 'ramsay', 'on', 'there', 'hell', 'sort', 'cunt', 'and', 'andre', 'out', 'mkr']\n",
      "changed data 58 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'some', 'of', 'these', 'women', 'act', 'just', 'plain', 'stupid', '100s', 'of', 'years', 'fighting', 'for', 'equality', 'and', \"you're\", 'gonna', 'play', 'dumb']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feelings', 'mutual', 'mate', 'just', 'gonna', 'give', 'up', 'on', 'femi', 'nazis', 'like', 'yourself']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['driverless', 'cars', 'that', 'would', 'be', 'an', 'improvement', 'for', 'some', 'of', 'the', 'clowns', 'on', 'the', 'road', 'not', 'sexist']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['lol', 'edgy', 'im', 'not', 'sexist', 'but', 'why', 'are', 'women', 'allowed', 'opinions', 'lol']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'her', 'mind', 'sure']\n",
      "changed data 21 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'no', 'sexist', 'but', 'to', 'pay', 'the', \"men's\", 'and', \"women's\", 'wimbledon', 'champions', 'the', 'same', 'prize', 'money', 'is', 'farcical', 'as', 'jobs', 'go', \"they're\", 'ent']\n",
      "changed data 126 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['name', '3', 'prominent', 'feminists', 'please', 'no', 'google']\n",
      "changed data 46 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'people', 'at', 'work', 'are', 'childish', 'lol', 'its', 'run', 'by', 'women', 'im', 'not', 'sexist', 'but', 'when', 'women', 'dont', 'agree', 'to', 'something', 'oh', 'man']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'sorry', 'even', 'the', 'rosie', 'st', 'definition', 'of', 'feminism', 'is', 'still', 'unequal']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['right', 'listen', 'to', 'founding', 'mother', 'of', 'feminism', 'advice', 'for', 'young', 'feminists', '13']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', \"can't\", 'deal', 'with', 'women', 'football', 'announcers', 'im', 'not', 'sexist', 'it', 'just', \"doesn't\", 'sound', 'right']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'if', 'you', 'will', 'but', 'men', 'are', 'far', 'more', 'civilised', 'during', 'these', 'debates', 'than', 'women', 'debt', 'debate']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'one', 'cares', 'what', 'u', 'think', 'call', 'me', 'sexist', 'but', 'heels', 'no', 'heels', 'sorry', 'ladies', 'also', 'nasty', 'nails', 'vomit', 'look', 'clean', 'at', 'least']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'one', 'cares', 'call', 'me', 'sexist', 'but', 'i', 'hate', 'to', 'hear', 'girls', 'argue', 'bout', 'sports']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'liv', 'und', 'rur', 'bed', 'anything', 'but', 'yourself', 'like', 'every', 'feminist', 'on', 'earth']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'i', \"don't\", 'care', 'but', 'nascar', 'is', 'a', 'mans', 'sport', 'sorry', 'not', 'sorry']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['news', 'flash', \"it's\", 'not', 'for', 'you', 'a', 'word', 'to', 'all', 'the', 'ladies', 'in', 'very', 'short', 'skirts', 'or', 'shorts', 'this', 'summer', 'thanks', 'not', 'sexist', 'grateful']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'might', 'be', 'poe', 'just', 'fyi', 'i', 'think', 'that', 'was', 'the', 'quote', 'of', 'hers', 'that', \"couldn't\", 'be', 'verified']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['stick', 'it', 'up', 'em', 'kat', \"you're\", 'the', 'sort', 'of', 'bitch', 'that', 'really', 'makes', 'the', 'show', 'mkr']\n",
      "changed data 75 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'the', 'cunt', 'face', 'bitch', 'mkr', 'lives', 'to', 'fight', 'on']\n",
      "changed data 44 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hearing', 'these', 'girls', 'in', 'my', 'class', 'talk', 'about', 'cars', 'is', 'some', 'of', 'the', 'lowest', 'iq', 'discussion', 'ive', 'ever', 'heard', 'not', 'sexist', 'just', 'an', 'obs']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['please', 'tell', 'me', 'cunt', 'and', 'andre', 'go', 'to', 'sudden', 'death', 'tonight', 'cant', 'cook', 'cunt', 'cook', 'mkr', 'wont', 'stop', 'till', 'they', 'drop']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['someone', 'recommend', 'me', 'a', 'good', 'book', 'not', 'sexist', 'but', 'prefer', 'not', 'having', 'main', 'character', 'female', \"can't\", 'connect', 'ya', 'know']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'an', 'atheist', 'but', 'god', 'bless', 'hot', 'women', 'who', 'embrace', 'enjoy', 'their', 'own', 'sexuality']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'i', 'think', 'male', 'comedians', 'are', 'funnier', 'than', 'female', 'ones']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['promo', 'models', 'are', 'the', 'dangle', 'berry', 'of', 'the', 'modeling', 'world', 'shit', 'mkr']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yeah', 'uh', 'women', 'against', 'feminism', \"aren't\", 'bigots', 'who', 'hate', 'men', 'and', \"don't\", 'mind', 'sharing', 'equality', 'w', 'them', 'like', 'you', 'hypocrites', 'fyi']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['these', 'nsw', 'promo', 'girls', 'think', 'way', 'too', 'highly', 'of', 'themselves', \"they're\", 'not', 'even', 'attractive', 'mkr', 'mkr', '2015']\n",
      "changed data 100 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'actually', 'two', 'separate', 'proposals', 'real', 'insane']\n",
      "changed data 51 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminists', 'believe', 'that', 'the', 'existence', 'of', 'a', 'rape', 'culture', 'is', 'solely', 'down', 'to', 'the', 'patria', 'rc']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'i', 'meant', 'can', 'you', 'explain', 'why', 'telling', 'a', 'person', 'you', 'are', 'arguing', 'w', 'they', 'cannot', 'get', 'laid', 'is', 'wrong']\n",
      "changed data 100 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['a', 'ghostbusters', 'remake', 'with', 'women', 'taking', 'up', 'the', 'roles', 'of', 'ghoul', 'slaying', 'saviours', 'im', 'no', 'sexist', 'but', \"let's\", 'not', 'be', 'stupid', 'eh']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminists', 'think', 'very', 'little', 'of', 'women', 'it', 'turns', 'out']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['she', 'is', 'blond', 'what', 'do', 'you', 'want', 'not', 'sexist']\n",
      "changed data 39 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['a', 'feminazi', 'is', 'a', 'feminist', 'who', 'thinks', 'she', 'has', 'a', 'penis', 'my', 'pre', 'cal', 'teacher']\n",
      "changed data 72 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['tweet', 'like', 'a', 'feminist', 'gamer', 'gate', 'an', 'outlandish', 'satirical', 'piece', 'of', 'writing', 'that', 'takes', 'a', 'hearty', 'stab', 'at', 'new', 'feminism', 'http']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'will', 'go', 'fuck', 'yourself', 'because', 'no', 'guy', 'wants', 'to', 'fuck', 'you', 'lol', 'feminazi', 'dont', 'need', 'a', 'man']\n",
      "changed data 84 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['sucks', 'to', 'have', 'the', 'smile', 'wiped', 'off', 'your', 'own', 'face', 'huh', 'kat', 'she', 'in', 'a', 'glass', 'house', 'should', 'not', 'throw', 'stones', 'mkr', 'mkr', '2015']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['black', 'opal80', 'but', 'not', 'a', 'joke', 'a', 'lot', 'of', 'her', 'writing', 'created', 'the', 'false', 'rape', 'culture', 'crisis', 'whose', 'theories', 'were', 'drivers', 'of', 'new', 'ca', 'rape', 'law']\n",
      "changed data 144 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['twitter', 'feminism', \"can't\", 'convince', 'three', 'teen', 'women', 'to', 'take', 'up', 'computer', 'science', 'meanwhile', 'isis', 'is', 'sooo', 'christian', 'grey', 'righ']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ahh', 'manu', 'well', 'said', 'just', 'cook', 'good', 'food', 'and', \"you'll\", 'be', 'ok', 'simples', 'back', 'in', 'ya', 'box', 'bitch', 'kat', 'mkr']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'no', 'sexist', 'but', 'ion', 'know', 'how', 'id', 'react', 'if', 'my', 'son', 'came', 'down', 'the', 'stairs', 'in', 'heels', 'prolly', 'throw', 'a', 'football', 'at', 'his', 'head']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['foods', 'not', 'for', 'you', 'lol', 'coming', 'downstairs', 'and', 'four', 'girls', 'are', 'cooking', 'dinner', 'not', 'sexist', 'but', 'not', 'complaining']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['men', 'are', 'physically', 'stronger', 'women', 'g', 'estate', 'babies', 'pointless', 'for', 'any', '1', 'to', 'whine', 'that', 'this', 'denies', 'them', 'equal', 'rights']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['women', 'should', 'never', 'participate', 'in', 'no', 'shave', 'november', 'you', 'should', 'only', 'have', 'hair', 'on', 'your', 'head']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'and', 'ill', 'probably', 'get', 'in', 'trouble', 'for', 'this', 'but', 'women', 'werent', 'designed', 'or', 'created', 'to', 'be', 'alone']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['only', '2', 'oppressions', 'im', 'not', 'sexist', 'but', 'i', 'just', 'remembered', 'why', 'i', 'hate', 'feminists', 'unless', \"you're\", 'black', 'or', 'jewish', 'please', 'stop', 'whining']\n",
      "changed data 129 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['depends', 'if', 'these', '2', 'cesspools', 'came', 'from', 'a', 'house', 'that', 'taught', 'them', 'integrity', 'and', 'morals', 'as', 'a', 'child', 'sadly', 'not', 'toxic', 'kat', 'mkr']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'have', 'their', 'feminazi', 'heads', 'shoved', 'firmly', 'up', 'their', 'rotting', 'asses']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'is', 'it', 'that', 'women', 'lack', 'that', 'requires', 'two', 'women', 'witnesses', 'to', 'replace', 'one', 'man']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'how', 'much', 'can', 'this', 'blonde', 'airhead', 'twirl', 'her', 'hair', 'idiots', 'shutup']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'females', 'lately', 'are', 'just', 'dumbing', 'down', 'please', 'focus', 'on', 'your', 'brain', 'and', 'your', 'soul', 'than', 'you', 'do', 'on', 'your', 'body']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', 'believe', 'women', 'should', 'hold', 'themselves', 'to', 'a', 'different', 'standard', 'as', 'guys', 'like', 'how', 'they', 'carry', 'themselves']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'i', 'meant', 'tht', 'to', 'u', 'relax', 'i', 'm', 'nt', 'a', 'feminazi', 'so', 'u', 'dnt', 'hv', 'to', 'abuse', 'me', 'indias', 'daughter']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['are', 'these', 'girls', 'for', 'real', \"you're\", 'promo', 'models', 'and', 'struggling', '6', 's', 'at', 'best', 'opposite', 'of', 'killin', 'it', 'pipedown', 'mkr']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', \"wouldn't\", 'recognize', 'my', 'icon', 'because', \"you're\", 'a', 'metrosexual', 'sissy', 'boy', 'living', 'in', 'feminazi', 'sweden', 'i', 'see', 'you', 'everyday', 'in', 'gothenburg']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'in', 'letterman', 'a', 'jackets', 'look', 'so', 'weird', 'to', 'me']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['put', 'a', 'bag', 'on', 'your', 'head', 'kat', 'no', 'one', 'wants', 'to', 'see', 'your', 'glee', 'at', 'other', \"people's\", 'misfortune', \"you're\", 'awful', 'please', 'leave', 'the', 'door', 'is', 'that', 'way', 'mkr']\n",
      "changed data 136 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['please', 'explain', 'what', 'the', '77', 'figure', 'cited', 'in', 'wage', 'gap', 'discussions', 'compares', 'thanks']\n",
      "changed data 82 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'or', 'anything', 'but', 'i', 'hate', 'when', 'women', 'do', 'commentary', 'on', 'sports']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['tbh', 'you', 'can', 'call', 'me', 'sexist', 'but', 'the', 'sight', 'of', 'girls', 'smoking', 'is', 'unpleasant']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'like', 'the', 'forward', 'group', 'now', 'and', 'maf', 'has', 'been', 'great', 'but', 'our', 'defense', 'will', 'get', 'bitch', 'slapped', 'like', 'a', 'mean', 'pimps', 'junkie', 'hooker']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['do', 'you', 'get', 'it', 'you', 'blondes', 'mkr']\n",
      "changed data 42 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'there', 'are', 'certain', 'things', 'that', 'a', 'woman', \"can't\", 'tell', 'me', 'nothing', 'about']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['emotional', 'terrorism', 'twitter', 'feminism']\n",
      "changed data 35 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'i', \"don't\", 'like', 'weak', 'women', 'pov', 'i', 'like', 'kick', 'ass', 'women', 'pov', 'like', 'cer', 'sei', 'you', 'know']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'there', 'is', 'a', 'double', 'standard', 'with', 'men', 'women', 'you', 'want', 'the', 'guy', 'everybody', 'wants', 'but', 'we', \"don't\", 'want', 'the', 'femal']\n",
      "changed data 130 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['oh', 'my', 'god', 'if', 'kat', 'stays', 'in', 'i', 'quit', 'life', 'she', 'doesnt', 'deserve', 'it', 'so', 'hard', 'mkr']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'mean', 'it', \"shouldn't\", 'be', 'because', 'it', 'is', 'consent', 'to', 'parenthood', 'for', 'men', 'today']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', 'bler', 'gh']\n",
      "changed data 15 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['how', 'to', 'spot', 'a', 'feminist', \"doesn't\", 'want', 'all', 'feminists', 'blamed', 'for', 'feminist', 'lobby', 'for', 'anti', 'male', 'discrimina', 'try', 'law', 'freaks', 'out', 'over']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['this', 'is', 'not', 'sexist', 'but', 'my', 'opposite', 'sex', \"can't\", 'drive', 'for', 'shit']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'honestly', \"can't\", 'believe', 'they', 'added', 'female', 'soldiers', 'to', 'call', 'of', 'duty', 'ghosts', 'im', 'not', 'sexist', 'but', \"it's\", 'kinda', 'weird', 'and', 'a', 'l']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'any', 'serious', 'forum', 'i', 'will', 'fully', 'support', 'feminism', 'femi', 'nazis', 'however', 'are', 'a', 'real', 'thing', 'and', 'in', 'their', 'presence', 'i', 'will', 'be', 'a', 'men', 'in', 'ist']\n",
      "changed data 133 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'all', 'you', 'want', 'but', 'from', 'stories', 'seeing', 'this', 'kind', 'of', 'stuff', 'go', 'down', 'women', 'as', 'a', 'whole', 'need', 'to', 'change', 'how', 'they', 'co']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'most', 'women', 'today', 'will', 'take', 'up', 'all', 'the', 'best', 'seat', 'in', 'the', 'pubs', 'clubs', 'when', 'they', 'have', 'no', 'clue', \"what's\", 'going', 'on']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', 'im', 'just', 'not', 'comfortable', 'with', 'ladies', 'r', 'effing', 'football']\n",
      "changed data 74 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hey', 'i', 'have', 'a', 'gender', 'studies', 'degree', 'clearly', 'you', 'all', 'are', 'the', 'brainwashed', 'ones']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['keep', 'ranting', 'attacking', 'people', 'you', 'are', 'making', 'no', 'points', 'bye', 'feminazi', 'what', 'a', 'fool']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"let's\", 'be', 'civil', \"let's\", 'be', 'acquaintances', 'cunt', 'feminazi', 'bitch']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'national', 'kit', 'kat', 'day', 'in', 'belarus', 'go', 'on', 'treat', 'ur', 'girlfriend', 'or', 'wife', 'and', 'ask', 'them', 'if', 'they', 'fancy', 'a', 'finger', 'not', 'sexist']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hate', 'these', 'blonde', 'bitches', 'already', 'mkr']\n",
      "changed data 37 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['shhh', 'no', 'hints', \"they'll\", 'copy', 'off', 'your', 'paper']\n",
      "changed data 45 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['but', 'i', 'guess', 'the', 'wall', 'street', 'journal', 'is', 'a', 'feminazi', 'newspaper', 'pay', 'equality', 'wage', 'gap', 'women', 'against', 'feminism']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yup', 'there', 'still', 'hot', 'blondes', 'mkr']\n",
      "changed data 31 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['please', 'do', \"women's\", 't20', 'cricket', 'im', 'not', 'sexist', 'but', 'id', 'rather', 'kick', 'myself', 'in', 'the', 'eye']\n",
      "changed data 84 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['best', 'not', 'to', 'engage', 'with', 'unpleasant', 'people', 'she', 'can', 'say', 'that', 'again', 'feminism', 'is', 'awful', 'women', 'against', 'feminism', 'ga']\n",
      "changed data 112 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'someone', 'just', 'told', 'me', 'that', 'feminism', 'lying', 'communists', 'who', 'oppose', 'equality', 'can', 'you', 'believe', 'people', 'this', 'bs', 'http']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'feminism', 'is', 'not', 'about', 'equal', 'rights', 'but', 'a', 'socio', 'polit', 'al', 'movement']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['any', 'other', 'whiny', 'male', 'wanna', 'get', 'blocked', 'blame', 'one', 'not', 'all', 'men', 'are', 'violent']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'swear', 'im', 'not', 'sexist', 'but', 'women', 'can', 'not', 'drive']\n",
      "changed data 47 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['fems', 'for', 'the', 'love', 'of', 'god', 'please', 'stop', 'propagating', 'the', 'wage', 'gap', 'myth', 'fem', 'free', 'friday']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"here's\", 'the', 'trick', 'when', 'she', 'says', 'she', 'was', 'six', 'years', 'old', 'it', 'means', 'that', 'she', 'was', 'six', 'years', 'old', 'got', 'it', 'cunt']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['questions', 'for', 'men', 'are', 'you', 'aware', 'that', 'feminism', 'is', 'teaching', 'women', 'that', 'men', 'cannot', 'experience', 'sexism', 'or', 'discrimination', 'ht']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['all', 'femal', 'ghostbusters', 'sorry', 'im', 'not', 'sexist', 'but', 'that', \"ain't\", 'gonna', 'work', 'and', 'nobody', 'is', 'gonna', 'go', 'and', 'watch', 'it', 'flop']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'no', 'issue', 'with', 'her', 'doing', 'the', 'very', 'thing', 'she', 'decries', 'then']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['go', 'fuck', 'yourself', 'kat', 'stupid', 'slut', 'mkr', 'no', 'fucks', 'given']\n",
      "changed data 50 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['anita', 'sar', 'kees', 'ian', 'is', 'a', 'feminazi', 'via']\n",
      "changed data 35 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['femi', 'nazis', 'favorite', 'hang', 'out']\n",
      "changed data 28 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['stop', 'fucking', 'objectifying', 'strangers', 'call', 'me', 'sexist', 'but', 'i', 'love', 'to', 'watch', 'women', 'with', 'fat', 'asses', 'walk', 'away']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['gamer', 'gate', 'kristi', 'thinks', 'anita', 'sar', 'kees', 'ian', 'is', 'a', 'feminazi']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', 'i', 'do', 'not', 'like', 'girl', 'refs', 'for', 'football']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['live', 'brothers', 'at', 'arms', 'obama', 'law', 'and', 'feminazis', 'on', 'coast', '2', 'coast', 'feminazi', 'feminism', 'liberty', 'obama']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'girl', 'is', 'hideous', 'in', 'every', 'way']\n",
      "changed data 36 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['saying', 'that', 'anti', 'feminist', 'is', 'being', 'anti', 'women', 'is', 'like', 'saying', 'being', 'anti', 'kkk', 'is', 'being', 'anti']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"don't\", 'count', 'any', 'chickens', 'most', 'of', 'the', 'gop', 'candidates', 'suck', 'and', 'could', 'well', 'lose', 'to', 'uneducated', 'history', 'vagina', 'voters']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'fuck', 'me', 'half', 'of', 'the', 'women', 'y', 'ids', 'on', 'twitter', 'are', 'fucking', 'clueless']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', 'just', \"can't\", 'watch', 'football', 'with', 'a', 'female', 'commentator', 'or', 'announcer', 'even', 'my', 'mom', \"can't\", 'stand', 'it']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'fuck', 'that', 'lady', \"she's\", 'awful']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'even', 'women', 'prefer', 'to', 'watch', \"men's\", 'sports', 'over', 'women', 'playing', 'because', \"it's\", 'played', 'at', 'a', 'higher', 'level']\n",
      "changed data 112 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"there's\", 'that', 'p600', 'spike', 'mmn', 'sexism', 'only', 'females', 'recognise', 'the', 'final', 'spin', 'on', 'a', 'washing', 'machine', 'full', 'of', 'clothes', 'not', 'sexist', 'fact']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'you', \"don't\", 'like', 'the', 'constitution', 'and', 'you', 'vote', 'for', 'a', 'person', 'like', 'hillary', 'then', \"you're\", 'a', 'feminazi']\n",
      "changed data 96 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ghostbusters', '3', 'with', 'an', 'all', 'female', 'lead', 'cast', 'will', 'not', 'work', 'im', 'not', 'sexist', 'but', \"it's\", 'simply', 'a', 'bad', 'idea', 'lost', 'interest', 'now']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['judging', 'by', 'your', 'username', 'alone', 'yeah', 'you', 'are', 'im', 'not', 'sexist', 'but', 'i', 'really', 'dislike', 'the', 'majority', 'of', 'teenage', 'girls']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['two', 'people', 'went', 'to', 'jail', 'in', 'the', 'uk', 'for', 'criticizing', 'feminists']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['femi', 'nazis', 'want', 'to', 'replace', 'the', 'andrew', 'jackson', 'on', 'the', '20', 'bill', 'with', 'this', 'hose', 'beast', 'wi', 'union']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"here's\", 'a', 'tip', \"you're\", 'sexist', 'i', 'swear', 'im', 'not', 'sexist', 'but', 'sometimes', 'i', 'think', 'i', 'act', 'that', 'way', 'and', 'idk', 'how', 'to', 'explain', 'it', 'v', 'v']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kats', 'not', 'a', 'morning', 'person', 'or', 'a', 'midday', 'person', 'or', 'an', 'afternoon', 'person', 'or', 'an', 'evening', 'person', 'wait', 'is', 'she', 'even', 'a', 'person', 'mkr', 'mkr', '2015']\n",
      "changed data 134 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'absolutely', 'crystal', 'clear', 'now', 'that', 'with', 'the', 'amount', 'of', 'sjw', 'feminists', 'on', 'campuses', 'that', 'guys', 'must', 'tape', 'every', 'sexual', 'en']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'it', 'seems', 'like', 'females', 'never', 'have', 'valid', 'opinions', 'when', 'it', 'comes', 'to', 'sports']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['can', 'i', 'cry', 'now', 'i', 'think', 'ill', 'cry', 'this', 'is', 'exactly', 'what', 'women', 'should', 'look', 'like', 'thanks']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['do', 'you', 'think', 'implying', 'someone', \"can't\", 'get', 'laid', 'is', 'sexist', 'or', 'abusive']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'am', 'not', 'sexist', 'but', 'when', 'girls', 'beg', 'to', 'wear', 'yogas', \"don't\", 'get', 'uptight', 'if', 'you', 'see', 'a', 'guy', 'looking', 'then', 'you', 'are', 'just', 'help', 'i']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['lo', 'ool', 'what', 'goes', 'around', 'comes', 'around', 'kat', 'should', 'just', 'shut', 'up', 'mkr']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['rights', 'mean', 'nothing', 'if', 'they', 'can', 'be', 'violated', 'w', 'no', 'recourse']\n",
      "changed data 59 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'shows', 'called', 'feminazi', \"that's\", 'what', 'woman', 'r', 'called', '4', 'having', 'an', 'opinion', 'i', 'had', 'an', 'opinion', 'f']\n",
      "changed data 94 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['because', 'femininity', 'is', 'so', 'horrible', 'im', 'not', 'sexist', 'but', 'if', 'a', 'dude', 'cries', 'because', 'of', 'a', 'girl', 'in', 'a', 'wedding', 'dress', 'then', 'he', 'has', 'a', 'vagina']\n",
      "changed data 127 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['real', 'men', 'knew', 'how', 'to', 'respect', 'women', 'coz', 'their', 'mother', 'taught', 'em', 'not', 'like', 'u', 'modern', 'feminazi']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'know', 'you', 'know', 'my', 'intent', 'is', 'to', 'prove', 'they', \"don't\", 'know']\n",
      "changed data 56 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['german', 'professor', 'gives', 'meaning', 'to', 'term', 'femi', 'nazi', 'doesn', 't', 'hire', 'indian', 'men', 'bcz', 'of', 'india', 'ss', 'rape', 'problem']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['sounds', 'like', 'he', 'married', 'a', 'feminazi']\n",
      "changed data 34 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'need', 'to', 'stop', 'trying', 'to', 'be', 'country', 'singers', 'godawful', 'leave', 'it', 'to', 'the', 'men']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'awkward', 'moment', 'when', 'a', 'comment', 'is', 'more', 'informative', 'than', 'the', 'article', 'women', 'against', 'feminism', 'http']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['tests', 'are', 'like', 'women', 'hard', 'to', 'understand', 'easy', 'to', 'get', 'over', 'and', 'nonrefundable', 'true', 'story', 'only', 'all', 'women', 'not', 'sexist', 'just', 'saying']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['now', 'im', 'not', 'sexist', 'but', 'an', 'all', 'female', 'ghostbusters', '3', 'with', 'no', 'connection', 'to', 'the', 'original', 'movies', 'is', 'quite', 'possibly', 'the', 'wor']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['please', 'invade', 'my', 'pole', 'land', 'rt', 'this', 'makes', 'me', 'laugh', 'feminazi', 'feminism', 'yes', 'all', 'women', 'fem', '2', 'p2']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['girls', \"shouldn't\", 'do', 'drugs', 'not', 'sexist', 'just', 'the', 'truth']\n",
      "changed data 47 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'the', 'thought', 'of', 'an', 'all', 'female', 'ghostbusters', '3', 'scares', 'me', 'more', 'than', 'any', 'ghost', 'they', 'could', 'think', 'up', 'bad', 'idea']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'offence', 'to', 'anyone', 'but', 'women', 'are', 'poor', 'drivers', 'not', 'sexist', 'may', 'be', 'sexist']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yeah', 'no', 'male', 'led', 'empires', 'have', 'fallen', 'iirc', 'im', 'not', 'sexist', 'but', 'history', 'shows', 'us', 'that', 'empires', 'fall', 'when', 'women', 'take', 'control', 'of', 'them']\n",
      "changed data 127 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['being', 'a', 'fuck', 'face', 'to', 'a', 'girl', 'who', 'just', 'agreed', 'she', \"doesn't\", 'want', 'a', 'feminazi', 'hoe', 'in', 'her', 'business', 'so', 'fuck', 'off']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yeah', \"they're\", 'totally', 'just', 'rain', 'vaginas', 'a', 'man', 'with', 'an', 'umbrella', 'just', \"doesn't\", 'look', 'right', 'im', 'with', 'you', 'bro', 'not', 'sexist']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'a', 'massive', 'c', 'nt', 'the', 'biggest', 'ever', 'on', 'mkr', 'cunt', 'and', 'andre']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['zzz', 'call', 'me', 'sexist', 'but', 'im', 'yet', 'to', 'see', 'a', 'funny', 'female', 'comedian']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['rights', 'plural']\n",
      "changed data 19 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'people', 'call', 'me', 'sexist', 'i', 'say', 'truthful', 'i', 'mean', 'do', 'you', 'expect', 'a', 'man', 'to', 'do', 'cooking', 'cleaning', 'and', 'washing']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'shuld', 'familiarise', 'themselves', 'wit', 'the', 'pots', 'after', 'all', 'a', 'way', 'to', 'a', 'mans', 'heart', 'is', 'thru', 'the', 'stomach', 'just', 'saying']\n",
      "changed data 130 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'can', 'someone', 'punch', 'that', 'smug', 'smirk', 'off', 'kats', 'face', 'please']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'looks', 'like', 'medusa', 'tbh', 'that', 'ugly', 'personality', 'to', 'top', 'it', 'off', 'mkr']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['faa', 'ark', 'the', 'blonde', 'promo', 'girl', 'walks', 'with', 'all', 'the', 'grace', 'of', 'a', 'tra', 'die', 'in', 'drag', 'mkr']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sure', \"women's\", 'darts', 'should', 'be', 'on', 'tv', 'unless', 'these', 'too', 'are', 'just', 'really', 'bad', 'seen', 'better', 'in', 'the', 'pub', 'not', 'sexist', 'just', 'shocking']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['now', 'im', 'not', 'sexist', 'in', 'any', 'way', 'shape', 'or', 'form', 'but', 'i', 'think', 'women', 'are', 'better', 'at', 'gift', 'wrapping', \"it's\", 'the', 'xx', 'chromosome', 'thing']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['anyone', 'else', 'notice', 'that', 'all', '3', 'of', 'katie', 'and', 'nikki', 's', 'dishes', 'were', 'based', 'on', 'them', 'being', 'sluts', 'sausage', 'cock', 'and', 'tarts', 'mkr', 'mkr', '2015']\n",
      "changed data 126 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'have', 'the', 'look', 'that', 'every', 'man', 'wants', 'in', 'a', \"woman's\", 'eyes', 'surprised', 'with', 'a', 'bit', 'of', 'fear', 'holy', 'crap']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'i', \"don't\", 'give', 'a', 'fuck', 'about', \"women's\", 'football', 'ball', 'on', 'dor', '2014']\n",
      "changed data 75 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'believe', 'the', 'correct', 'term', 'is', 'bort', 'ches', 'im', 'not', 'sexist', 'but', 'some', 'girls', 'are', 'just', 'straight', 'birches', 'honestly']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'sexist', 'but', 'womem', 'and', 'sport', 'a', 'int', 'to', 'my', 'taste', 'tbh', 'especially', 'when', 'i', 'see', 'um', 'in', 'ufc', 'leave', 'it', 'out', 'go', 'be', 'a', 'nurse', 'or', 'hair', 'dre']\n",
      "changed data 126 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', 'i', \"wouldn't\", 'mind', 'a', 'stay', 'at', 'home', 'wife', \"don't\", 'need', 'my', 'kids', 'being', 'raised', 'by', 'someone', 'other', 'than', 'family']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'never', 'met', 'a', 'promo', 'girl', 'that', 'we', 'wars', 'a', 'full', 'length', 'dress']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'only', 'time', 'that', 'bitch', 'ever', 'sees', 'the', 'inside', 'of', 'a', 'church', 'is', 'when', 'she', 'brings', 'supplies', 'to', 'muslim']\n",
      "changed data 100 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['these', 'girls', 'say', \"they're\", 'butchers', 'i', 'once', 'worked', 'at', 'a', 'boat', 'hire', 'shop', 'so', 'i', 'must', 'be', 'a', 'ship', 'captain', 'mkr']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['just', 'saw', 'clips', 'of', 'how', 'the', 'oscars', 'mamm', 'ified', 'octavia', 'spencer', 'why', 'would', 'she', 'coon', 'herself', 'out', 'like', 'that', 'oh', 'so', 'she', 'can', 'play', 'another', 'maid']\n",
      "changed data 136 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'you', 'are', 'actually', 'complimenting', 'someone', 'when', 'you', 'refer', 'to', 'them', 'as', 'a', 'vagina', 'or', 'as', 'having', 'a', 'vagina']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminist', 'writer', 'think', 'valenti', 'of', 'au', 'from', 'what', 'i', 'know', 'started', 'the', 'questions', 'for', 'men', 'huff', 'po', 'others', 'wrote', 'up']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'really', 'scared', 'of', 'them', 'but', \"they're\", 'filthy', 'as', 'fuck', 'and', 'spread', 'diseases', 'like', 'a', 'sjw', 'feminazi', 'slut']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['gamer', 'gate', 'fem', 'freq', 'anti', 'sjw']\n",
      "changed data 25 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"we'd\", 'be', 'ranting', 'feminazi', 'bitches', 'during', 'the', 'wrong', 'time', 'of', 'the', 'month']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'sassy', 'foods', 'feeling', 'mkr', '2015', 'mkr', 'these', 'girls', 'are', 'vile', 'people', 'killer', 'blondes', 'basic', 'hot', 'dog', 'no', 'bun']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['prime', 'example', 'of', 'a', 'lying', 'feminazi', 'pussy', 'skank', \"can't\", 'take', 'the', 'blowback', 'of', 'her', 'exposure', 'locks', 'her', 'tweets', 'war', 'on', 'walker', 'lib', 'tard']\n",
      "changed data 134 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'litter', 'aly', 'im', 'sorry', 'sometimes', 'women', 'get', 'too', 'many', 'rights', 'yall', \"ain't\", 'innocent', 'at', 'all', 'like', 'men', 'can', 'ne']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['elegant', 'and', 'beautiful', 'cheap', 'and', 'trashy', 'nothing', 'more', 'unattractive', 'than', 'girls', 'banging', 'on', 'about', 'how', 'hot', 'hey', 'are', 'mkr', 'not', 'sassy']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['twitter', 'feminism', 'is', 'a', 'protection', 'racket', 'except', 'nobody', 'can', 'actually', 'guarantee', 'your', 'brand', 'any', 'safety', 'as', 'the', 'movement', 'is']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', 'girls', \"don't\", 'know', 'shit', 'about', 'sports', 'when', 'it', 'comes', 'to', 'football', 'they', 'repeat', 'anything', 'their', 'dad', 'says']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['to', 'get', 'rid', 'of', 'jeremy', 'is', 'part', 'and', 'parcel', 'of', 'the', 'feminazi', 'thought', 'police', 'tactics', 'that', 'are', 'strangling', 'this', 'country']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'every', 'pokemon', 'team', 'ive', 'ever', 'had', 'has', 'always', 'been', 'strictly', 'male', 'hmmm']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'drivers', 'are', 'shit']\n",
      "changed data 41 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'tart', 'lol']\n",
      "changed data 14 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ur', 'gonna', 'put', 'this', 'guys', 'fate', 'in', 'the', 'hands', 'of', '10', 'fat', 'dickless', 'hurt', 'in', 'un', 'american', 'ungrateful', 'women', 'i', 'mean', 'im', 'not', 'sexist', 'but']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['fem', 'freq', 'she', 'is', 'a', 'feminazi', 'also', 'i', 'am', 'proud', 'to', 'have', 'cartoon', 'plastic', 'surgery']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['german', 'professor', 'gives', 'meaning', 'to', 'term', 'femi', 'nazi', 'doesn', 't', 'hire', 'indian', 'men', 'bcz', 'of', 'india', 'ss', 'rape', 'problem']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'you', 'are', 'a', 'reprehensible', 'individual', 'mkr']\n",
      "changed data 43 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['as', 'soon', 'as', 'you', 'follow', 'enough', 'known', 'anti', 'feminists', 'their', 'block', 'list', 'activates', 'and', 'blocks', 'you', 'possible', 'she', 'is', 'unaware', 'of', 'me']\n",
      "changed data 126 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['dear', 'men', 'being', 'a', 'sexist', \"doesn't\", 'get', 'women', 'to', 'like', 'you', 'dear', 'women', 'nagging', 'does', 'not', 'work', 'no', 'sexist']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['worked', 'in', 'a', 'virtually', 'all', 'female', 'environment', 'not', 'sexist', 'but', 'comes', 'with', \"it's\", 'own', 'problems', 'sadly']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'feminists', 'will', 'trend', 'feminists', 'are', 'ugly', 'and', 'then', 'complain', 'how', 'the', 'fact', 'feminists', 'are', 'ugly', 'is', 'trending', 'is', 'proof', 'of', 'misogyny', 'in', 'society', 'k']\n",
      "changed data 137 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', '2015', 'mkr', 'i', 'think', 'kat', 'thinks', 'she', 'is', 'the', 'best', 'chef', 'ever', 'ah', 'hello', 'no']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'kat', 'got', 'a', 'new', 'decent', 'hair', 'cut', 'then', 'mkr']\n",
      "changed data 41 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['but', 'sorry', 'who', 'cares', 'that', 'the', 'national', 'organization', 'of', 'women', 'was', 'commie', 'you', 'were', 'talking', 'about', 'a', 'trucker', 'right', 'too', 'funny']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'guess', 'we', 'all', 'hate', 'kat', 'and', 'andrea', 'but', 'they', 'all', 'hated', 'the', 'fraulein', 's', 'more', 'you', 'bitches', 'cant', 'win', 'nor', 'beat', 'kat', 'the', 'dog', 'mkr', 'sorry', 'braun', 'hilder', 's']\n",
      "changed data 136 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['except', 'that', 'there', 'was', 'no', 'such', 'sexual', 'torture', 'and', 'she', 'is', 'a', 'lying', 'bitch']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['rios', 'e', 'valle', 'well', 'at', 'least', 'your', 'not', 'a', 'feminazi', 'ahaha', 'fe', 'hitler', 'ahha', 'a']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['get', 'rid', 'of', 'that', 'kat', 'the', 'worst', 'person', 'i', 'have', 'ever', 'seen', 'mkr']\n",
      "changed data 58 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['haha', 'mt', 'this', 'makes', 'me', 'laugh', 'every', 'single', 'time', 'feminazi', 'feminism', 'yes', 'all', 'women', 'misogyny', 'fem', '2']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['is', 'awesome', 'if', 'you', \"don't\", 'know', 'her', 'write', 'the', 'truth', 'about', 'feminism']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['sorry', 'you', 'need', 'to', 'add', '200', 'lbs', 'of', 'fat', 'for', 'the', 'western', 'feminazi']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'sorry', 'but', 'i', \"can't\", 'handle', 'women', 'commentators', 'or', 'women', 'talking', 'about', 'sports', 'on', 'espn', 'not', 'sexist']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'no', 'sexist', 'but', 'theres', 'something', 'about', 'women', 'playing', 'rugby', 'that', 'just', 'really', 'doesnt', 'look', 'right']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'fucking', 'hate', 'rich', 'ass', 'snobby', 'women', 'that', \"don't\", 'know', 'there', 'place', 'in', 'front', 'of', 'men', 'not', 'sexist', 'just', 'venting']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['beth', 'mo', 'wins', 'voice', 'annoys', 'the', 'living', 'hell', 'out', 'of', 'me', 'mute', 'espn', 'beef', 'o', 'bradys', 'bowl', 'beefs', 'bowl', 'not', 'sexist', 'just', 'saying']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mufasa', 'everything', 'the', 'light', 'touches', 'is', 'the', 'internet', 's', 'what', 'about', 'that', 'shadowy', 'place', 'm', \"that's\", 'feminazi', 'tumblr', 'you', 'must', 'never', 'go', 'there']\n",
      "changed data 139 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['arguing', 'with', 'a', 'feminazi', 'is', 'like', 'arguing', 'with', 'a', 'rock']\n",
      "changed data 52 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'women', 'just', 'cant', 'be', 'comedians', 'nor', 'can', 'they', 'be', 'rappers']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'really', 'sexist', 'though', 'is', 'it', 'taking', 'the', 'piss', 'out', 'of', 'feminists', 'not', 'women']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'now', 'thor', 'is', 'a', 'women', 'captain', 'america', 'is', 'now', 'black', 'and', 'iron', 'man', 'is', 'now', 'superior', 'silver', 'where', 'did', 'my', 'childhood', 'go']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"can't\", 'stand', 'female', 'announcers', 'doing', 'play', 'by', 'play', 'for', 'football', 'not', 'sexist', 'but', 'every', 'time', 'i', 'hear', 'holly', 'rowe', 'doing', 'a', 'game', 'i', 'ca']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hope', 'not', 'how', 'will', 'she', 'pay', 'her', 'bills']\n",
      "changed data 37 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['lt', 'of', 'course', 'im', 'not', 'sexist', 'but', 'the', 'way', 'they', 'spread', 'their', 'thoughts', 'are', 'annoying', 'and', 'women', 'have', 'been', 'equal', 'socially', 'for', 'q']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hahahaha', 'kat', 'that', 'was', 'worth', 'the', 'wait', 'mkr']\n",
      "changed data 41 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['according', 'to', 'who', 'the', 'feminazi', 'gestapo']\n",
      "changed data 39 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['katie', 'and', 'nikki', 'may', 'not', 'be', 'going', 'home', 'but', '57100', 'still', \"isn't\", 'that', 'impressive', 'i', \"wouldn't\", 'be', 'happy', 'about', 'that', 'score', 'you', 'smart', 'ass', 'bitch', 'mkr']\n",
      "changed data 139 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'but', 'there', 'are', '0', 'females', 'i', 'know', 'who', \"aren't\", 'stupid', 'in', 'some', 'way', 'or', 'another', 'they', 'just', \"don't\", 'listen']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', \"you're\", 'currently', 'living', 'in', 'scandinavia', 'my', 'condolences', 'for', 'living', 'at', 'feminazi', 'ground', 'zero']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['did', 'u', 'write', 'that', 'urself', 'omg', 'all', 'girls', 'should', 'come', 'with', 'a', 'sign', 'you', 'must', 'be', 'this', 'tall', 'to', 'ride', 'my', 'emotional', 'rollercoaster']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['raw', 'objectivism', 'google', 'christina', 'hoff', 'sommers', 'wage', 'gap', 'or', 'pay', 'gap', 'she', 'uses', 'real', 'facts', 'not', 'anecdotes', 'equal', 'pay']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'sexist', 'litmus', 'test', 'is', 'if', 'you', 'call', 'out', 'a', 'lvl', '2', 'sexist', 'comment', 'the', 'dood', 'comes', 'back', 'with', 'lvl', '10', 'guns', 'a', 'blazing', 'pussy', 'cunt', 'bitch']\n",
      "changed data 152 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['original', 'breathtaking', 'inspiring', 'course', 'i', 'do', 'now', 'get', 'back', 'to', 'the', 'kitchen', 'not', 'sexist']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'have', 'about', '12', 'like', 'that', 'would', 'be', 'hilarious', 'if', 'she', \"didn't\", 'need', 'help', 'so', 'bad']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'ban', 'the', 'mmm', 'im', 'not', 'sexist', 'but', 'female', 'comedians', 'are', 'all', 'shite', 'ban', 'them']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['idk', 'im', 'not', 'sexist', 'but', 'the', 'female', 'analyst', \"isn't\", 'doing', 'it', 'for', 'me']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'emotional', 'decisions', 'can', 'be', 'counter', 'productive', 'in', 'terms', 'of', 'team', 'spirit', 'but', 'are', 'very', 'e']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'the', 'hell', 'man', 'making', 'thor', 'a', 'woman', 'is', 'like', 'making', 'spiderman', 'a', 'woman', 'im', 'not', 'sexist', 'but', 'you', 'just', \"don't\", 'do', 'certain', 'things']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'more', 'women', 'know', 'what', 'diy', 'means', 'due', 'to', 'arts', 'and', 'crafts', 'lol']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['issue', 'with', 'female', 'hosts', 'on', 'sports', 'broadcast', 'is', 'that', 'most', 'have', 'no', 'variation', 'in', 'tone', 'of', 'voice', 'not', 'sexist']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['call', 'me', 'sexist', 'or', 'old', 'fashioned', 'but', 'i', 'prefer', 'females', 'who', 'dont', 'like', 'sports', 'i', 'want', 'a', 'girly', 'girlfriend', 'not', 'a', 'boyfriend']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['more', 'feminazi', 'garbage', 'if', 'more', 'directors', 'and', 'stuff', 'happen', 'to', 'be', 'males', 'who', 'cares', 'ok', 'we', 'get', 'it', 'men', 'are', 'bad', 'and', 'oppressing', 'women']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'a', 'female', 'referee', 'not', 'sexist', 'but', 'they', 'are', 'only', 'for', 'camps', 'right', 'this', 'will', 'not', 'be', 'happening', 'in', 'the', 'regu']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['they', 'say', 'they', 'care', 'about', \"men's\", 'rights', 'but', 'when', 'speaking', 'to', 'groups', 'who', 'represent', 'them', 'they', 'block', 'us', 'seem', 'they', 'enjoy', 'the']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"women's\", 'sports', 'are', 'dumb', 'i', \"won't\", 'pay', 'to', 'watch', \"women's\", 'sports', 'why', 'do', 'women', 'get', 'paid', 'if', 'no', 'one', 'is', 'watching', 'them']\n",
      "changed data 131 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['single', 'men', 'cannot', 'adopt']\n",
      "changed data 24 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['nothin', 'un', 'sexier', 'than', 'some', '1', 'that', 'think', \"they're\", 'all', 'that', 'male', 'or', 'female', 'the', 'eccles', 'awww', 'bless', 'them', 'they', 'still', 'think', 'they', 'were', 'hot', 'mkr']\n",
      "changed data 140 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['your', 'tweet', 'said', 'call', 'me', 'sexist', 'followed', 'by', 'sexist', 'statement', 'i', 'comply', 'u', 'take', 'issue', 'men', 'are', 'so', 'illogical', 'and', 'emotional']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'remember', 'that', 'israel', 'has', 'only', 'existed', 'for', 'a', 'fraction', 'of', 'the', 'nobel', 'period']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['3', 'male', 'indie', 'filmmakers', 'at', 'the', 'table', 'next', 'to', 'me', 'discussing', 'a', 'script', 'for', 'a', 'new', 'movie', 'it', 'sounds', 'sexist', 'this', 'is', 'y', 'we', 'need', 'more', 'women', 'in', 'media']\n",
      "changed data 140 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['these', '2', 'delusional', 'narcissistic', 'hostesses', 'on', 'mkr', 'make', 'me', 'want', 'to', 'vomit', 'my', 'own', 'dinner', 'up']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['by', 'people', 'you', 'mean', 'men', 'only', 'the', 'women', 'are', 'clearly', 'imprisoned', 'in', 'their', 'homes']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['israel', \"doesn't\", 'control', 'syria', 'that', 'is', 'simply', 'an', 'unrealistic', 'conspiracy', 'theory']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'translations', 'are', 'on', 'a', 'muslim', 'web', 'site', 'and', 'they', 'say', 'strike']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['wow', 'must', 'be', '5', 'or', '6', 'of', 'them']\n",
      "changed data 28 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['bugger', 'not', 'bye', 'bye', 'blondes', 'mkr']\n",
      "changed data 38 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['blame', 'one', 'not', 'all', 'is', 'an', 'epistemically', 'violent', 'hashtag', 'meant', 'to', 'minimize', 'rape', 'culture', 'and', 'structural', 'violence', \"it's\", 'lazy', \"it's\"]\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['this', 'shows', 'where', 'islam', 'would', 'be', 'if', 'they', \"didn't\", 'kill', 'apostates']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kim', 'jong', 'un', 'embraces', 'islam']\n",
      "changed data 27 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['because', 'im', 'a', 'feminazi', \"that's\", 'why']\n",
      "changed data 37 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['dude', 'we', 'were', 'the', 'best', 'team', 'ever', 'we', 'made', 'shit', 'happen', 'aggies', 'coon']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['did', 'you', 'not', 'watch', 'she', 'was', 'half', 'his', 'size', 'and', 'there', 'was', '80k', 'on', 'the', 'line', \"don't\", 'start', 'with', 'physical', 'equal', 'i']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but', 'this', 'is', 'gd', 'funny']\n",
      "changed data 34 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['how', 'islam', 'deprives', 'women', 'of', 'education']\n",
      "changed data 38 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yazid', 'i', 'girls', 'tell', 'of', 'escape', 'from', 'is']\n",
      "changed data 35 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', \"don't\", 'understand', 'mean', 'girls', 'and', 'certainly', 'not', 'mean', 'girls', 'on', 'aussie', 'television', 'mkr', 'mkr', '2015']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['a', \"he's\", 'a', 'racist', 'and', 'a', 'bigot', 'b', 'he', 'wants', 'to', 'sell', 'books', 'and', 'be', 'famous']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['who', 'is', 'you', 'what', 'is', 'an', 'illegal', 'settlement', 'all', 'muslim', 'settlements', 'were', 'made', 'by', 'violence']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'bet', 'the', 'campers', 'vote', 'strategically', 'at', 'least', 'that', 'is', 'what', 'kat', 'will', 'say', 'mkr']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ugh', 'these', 'fake', 'nerd', 'guys', 'that', \"don't\", 'even', 'know', 'who', 'wonder', 'woman', 'is']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['irans', 'plan', 'to', 'exterminate', 'israel', 'is', 'no', 'secret', 'except', 'maybe', 'to', 'obama', 's', 'administration', 'i', 'stand', 'with', 'israel', 'pj', 'net']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['woah', 'i', 'just', 'went', 'to', 'go', 'prod', 'sarah', 'jeong', 'to', 'see', 'if', 'i', 'could', 'ask', 'her', 'a', 'legal', 'question', 'and', 'saw', 'she', 'was', 'gone', 'hope', 'everything', 'is', 'ok']\n",
      "changed data 132 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['your', 'kindergarten', 'photoshopped', 'images', 'prove', 'conclusively', 'that', 'you', 'are', 'a', 'paid', 'putin', 'troll', 'fighting', 'for', 'fascism']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'israelis', 'should', 'demand', 'back', 'the', '40', 'of', 'the', 'arabian', 'peninsula', 'that', 'the', 'muslims', 'stole', 'from', 'them']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'wonder', 'if', 'they', 'got', 'hugged', 'often', 'enough', 'as', 'children', 'im', 'only', 'mostly', 'facetious', 'secure', 'people', \"don't\", 'use', 'words', 'like', 'feminazi']\n",
      "changed data 130 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['almost', 'all', 'jews', 'are', 'secular', 'and', 'could', 'care', 'less', 'about', 'following', 'the', 'example', 'of', 'moses']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'are', 'right', 'but', 'those', 'leaving', 'the', 'religion', 'are', 'not', 'telling', 'the', 'dae', 'sh', 'about', 'it']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['they', 'are', 'in', 'about', '20', 'verses', 'of', 'the', 'quran', 'and', 'they', 'are', 'in', 'the', 'hadiths']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'iran', 'won', 't', 'say', 'about', 'the', 'bomb']\n",
      "changed data 37 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['showing', 'that', 'you', 'are', 'a', 'liar', 'since', 'there', 'is', 'zero', 'evidence']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'per', 'capita', 'production', 'of', 'nobel', 'prize', 'winners', 'for', 'israel', 'is', 'more', 'than', '100', 'times', 'that', 'of', 'muslim', 'states']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'most', 'bullshit', 'terrible', 'stack', 'overflow', 'answer', 'ever', 'i', 'mean', \"it's\", 'technically', 'correct', 'but', 'shudder']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'all', 'women', 'because', 'once', 'at', 'a', 'louis', 'c', 'k', 'show', 'he', 'said', 'how', 'do', 'you', 'make', 'an', 'old', 'woman', 'you', 'take', 'a', 'young', 'woman', 'and', 'just', 'ruin', 'her']\n",
      "changed data 127 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'don', 't', 'need', 'fem', 'is', 'nsn', 'because', 'men', 'carry', 'heavy', 'things', 'that', 'i', 'cannot', 'like', 'shopping', 'boxes', 'and', 'a', 'huge', 'sense', 'of', 'superior', 'i']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['bottom', 'line', 'you', 'name', 'calling', 'insecure', 'feminazi', 'get', 'an', 'education', 'grow', 'a', 'spine', 'and', 'seek', 'professional', 'help']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'if', 'you', 'think', \"it's\", 'some', 'random', 'troll', 'asshats', 'you', 'would', 'be', 'incorrect', \"it's\", 'people', 'pissed', 'off', 'about', 'rape', 'jokes', 'telling', 'him', 'to', 'kill', 'himself']\n",
      "changed data 140 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['omg', 'all', 'those', 'girl', 'teams', 'and', '2', 'couples', 'hahaha', 'poor', 'kats', 'husband', 'and', 'lloyd', 'mkr']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ok', 'i', 'officially', 'dislike', 'kat', 'the', 'most', 'mkr']\n",
      "changed data 40 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'jews', 'are', 'only', 'at', 'war', 'with', 'the', 'muslims', 'who', 'want', 'to', 'wipe', 'them', 'out', 'micro', 'brain']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['when', '14', 'year', 'old', 'boys', 'use', 'the', 'term', 'feminazi', 'and', 'u', 'gotta', 'school', 'em', 'in', 'class']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['isis', 'in', 'mecca', 'but', 'i', 'thought', 'they', \"weren't\", 'real', 'muslims', 'and', 'had', 'nothing', 'to', 'do', 'with', 'islam']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'refined', 'and', \"it's\", 'pretty', 'like', 'us', 'seriously', 'someone', 'get', 'these', 'girls', 'english', 'lessons', 'they', 'have', 'no', 'idea', 'what', \"they're\", 'saying', 'mkr']\n",
      "changed data 135 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['here', 'look', 'at', 'the', 'statistics', 'for', 'what', 'muslims', 'believe']\n",
      "changed data 54 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['psa', 'feminazi', 'is', 'a', 'fucking', 'disgusting', 'term', 'that', 'compares', 'fighting', 'for', 'women', 's', 'rights', 'to', 'genocide', \"don't\", 'fucking', 'use', 'it']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['say', 'what', 'you', 'will', 'about', 'the', 'promo', 'girls', 'but', 'their', 'apple', 'pie', 'does', 'sound', 'kick', 'ass', 'mkr']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['your', 'childhood', 'was', 'a', 'piece', 'of', 'racist', 'and', 'sexist', 'shit', 'and', 'we', 'set', 'it', 'on', 'fire']\n",
      "changed data 74 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'hating', 'islam', 'is', 'rational', 'and', 'therefore', 'not', 'a', 'phobia']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['our', 'main', 'course', 'is', 'going', 'to', 'be', 'vacuous', 'and', 'narcissistic', 'just', 'like', 'us', 'mkr']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['female', 'devs', 'make', 'me', 'uncomfortable', 'so', \"let's\", 'imply', 'her', 'work', 'was', 'a', 'single', 'commit']\n",
      "changed data 79 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'one', 'ever', 'writes', 'a', 'book', 'called', 'men', 'stop', 'being', 'an', 'asshole', 'in', 'relationships', 'or', 'she', 'just', 'wants', 'to', 'be', 'treated', 'like', 'a', 'human', 'being', 'for', 'once']\n",
      "changed data 140 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'did', 'not', 'just', 'in', 'all', 'seriousness', 'tweet', 'i', \"can't\", 'be', 'sexist', 'my', 'mother', 'is', 'a', 'woman']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['toot', 'toot', 'the', 'karma', 'train', 'stopping', 'at', 'kats', 'station', 'mkr']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'you', 'compare', 'being', 'feminist', 'with', 'the', 'holocaust', 'im', 'proud', 'of', 'saying', 'that', 'im', 'a', 'fucking', 'feminazi']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'am', 'laughing', 'my', 'ass', 'off', 'at', 'your', 'faux', 'f', 'fense', 'the', 'original', 'tweet', 'said', 'women', \"shouldn't\", 'be', 'allowed', 'to', 'drive', 'for', 'chrissakes']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'question', 'is', 'why', 'are', 'the', 'dae', 'sh', 'murderers', 'of', 'innocent', 'civilians', 'complaining', 'about', 'the', 'civilians', 'they', 'hide', 'behind']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['again', 'showing', 'you', 'know', 'nothing', 'about', 'islam']\n",
      "changed data 44 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'brutality', 'of', 'the', 'old', 'testament', 'is', 'not', 'an', 'excuse', 'for', 'the', 'brutality', 'of', 'islam']\n",
      "changed data 79 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['british', 'teenager', 'raped', '90', 'times', 'in', 'weekend', 'report', 'into', 'uk', 'sex', 'slavery', 'finds', 'a', 'us', 'pol', 'htt']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yup', 'and', 'she', 'rarely', 'tells', 'them', 'to', 'stop']\n",
      "changed data 39 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'all', 'women', 'know', 'men', 'hate', 'us', 'and', 'try', 'to', 'carve', 'a', 'life', 'out', 'from', 'what', 'remains', 'when', 'you', 'subtract', 'our', 'daily', 'fear', 'rage', 'despair']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['because', 'what', 'you', 'think', 'is', 'islam', 'has', 'no', 'resemblance', 'to', 'the', 'real', 'islam']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hot', 'pot', 'come', 'on', 'girls', 'about', 'as', 'technical', 'as', 'a', 'ham', 'and', 'cheese', 'toast', 'ie', 'disappointing', 'mkr']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['good', 'idea', 'your', 'prophet', 'mohammed', 'said', 'camel', 'urine', 'is', 'great', 'medicine', 'try', 'some']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'have', 'no', 'problem', 'with', 'semites', 'only', 'with', 'islam']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'is', 'nothing', 'incorrect', 'about', 'the', 'generalization', 'look', 'at', 'minority', 'demographics', 'in']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'sexist', 'but']\n",
      "changed data 18 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['oh', 'you', 'have', 'no', 'idea', 'what', 'its', 'like', 'to', 'be', 'set', 'upon', 'by', 'the', 'bunny', 'army', 'you', 'really', 'want', 'to', 'see', 'that']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'is', 'showing', 'her', 'true', 'colours', 'mkr']\n",
      "changed data 35 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'is', 'a', 'disgrace', 'mkr']\n",
      "changed data 21 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'are', 'a', 'liar', 'islam', 'says', 'that', 'a', 'daughter', 'must', 'inherit', 'half', 'of', 'what', 'her', 'brother', 'inherits']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'wod', 'you', 'see', 'man', 'rik', 'and', 'his', 'wife', 'as', 'children', 'come', 'baaack']\n",
      "changed data 63 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['haha', 'mt', 'this', 'makes', 'me', 'laugh', 'every', 'single', 'time', 'feminazi', 'feminism', 'yes', 'all', 'women', 'misogyny', 'fem', '2']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', 'is', 'a', 'when', 'ch', 'boot', 'her', 'off', 'mkr']\n",
      "changed data 32 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['at', 'this', 'point', 'and', 'the', 'way', 'things', 'are', 'going', 'baghdadi', 'has', 'to', 'know', 'he', 'will', 'be', 'dead', 'before', 'the', 'end', 'of', '2015', 'islam']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['isis', 'jihadist', 'police', 'beat', 'up', 'woman', 'for', 'exposing', 'eyes', 'under', 'veiled', 'clothing', 'women', 'living', 'in', 'mosul', 'who', 'offen']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['sometimes', 'i', 'retweet', 'things', 'i', 'like', 'that', 'are', 'for', 'real', 'not', 'sexist', 'awesome', 'i', 'apologize', 'in', 'advance', 'for', 'confusion']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'are', 'a', 'liar', 'the', 'palestinians', 'never', 'had', 'a', 'country', 'and', 'the', 'jews', 'were', 'settled', 'not', 'bedouins']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'kat', 'is', 'defining', 'fair', 'hypocrite']\n",
      "changed data 39 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'what', 'pray', 'tell', 'is', 'their', 'place']\n",
      "changed data 38 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'guest', 'judges', 'should', 'deduct', 'points', 'for', 'the', 'catwalk', 'fucker', 'y', 'mkr']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['just', 'change', 'your', 'profile', 'pic', 'name', 'to', 'a', 'woman', 'pref', 'w', 'brightly', 'colored', 'hair', 'tweet', 'out', 'gamers', 'are', 'dead', 'or', 'games', 'are', 'sexist']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'find', 'that', \"it's\", 'the', 'people', 'who', 'claim', 'others', \"don't\", 'know', 'anything', 'about', 'islam', 'that', \"don't\", 'know', 'anything']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['segregation', 'of', 'the', 'sexes', 'is', 'just', 'one', 'more', 'of', 'the', 'backwards', 'steps', 'that', 'islam', 'hopes', 'to', 'achieve']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['regardless', 'of', 'who', 'thinks', 'what', 'there', 'is', 'only', 'one', 'definition', 'of', 'the', 'religion']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['same', 'applies', 'to', '14th', 'waffen', 'ss', 'they', 'fought', 'for', 'nazis', 'to', 'get', 'rid', 'of', 'soviets', 'they', 'never', 'had']\n",
      "changed data 96 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yeah', 'it', 'says', 'that', 'men', 'can', 'beat', 'women', 'so', 'what', 'is', 'your', 'point']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'the', 'muslims', 'claimed', 'that', 'mohammed', 'rose', 'to', 'heaven', 'at', 'now', 'site', 'of', 'al', 'aqsa', 'so', 'they', 'could', 'take', 'it']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"we've\", 'proved', \"we're\", 'not', 'just', 'the', 'dumb', 'blondes', 'with', 'pretty', 'faces', 'whatevs', 'mkr']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['these', 'girls', 'are', 'pretty', 'awful', 'go', 'home', 'mkr']\n",
      "changed data 42 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'three', 'best', 'muslim', 'states', 'you', 'could', 'come', 'up', 'with', 'prove', 'my', 'point']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['how', 'do', 'crimes', 'of', 'christianity', 'excuse', 'the', 'barbarity', 'of', 'islam', 'no', 'one', 'here', 'is', 'trying', 'to', 'sell', 'christianity']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['grammar', 'nazi', 'and', 'feminazi', 'are', 'offensive', 'terms', 'way', 'to', 'be', 'language', 'stalin', 's']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['is', 'the', 'personification', 'of', 'rape', 'culture', 'avoid', 'alex', 'valberg', 's', 'women']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ole', 'annie', 'on', 'that', 'tree', 'she', 'was', 'fired', 'up', 'barn', 'side', 'kennels', 'jk', 'ready', 'to', 'go', 'again', 'coon', 'hunting']\n",
      "changed data 94 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ob', 'surfer', 'lies', 'vile', 'islam', 'truth']\n",
      "changed data 30 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['some', 'ppl', 'promote', 'education', 'to', 'our', 'youth', 'promotes', 'the', 'kar', 'dash', 'ians', 'what', 'a', 'coon']\n",
      "changed data 82 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['stop', 'gamer', 'gate', 'anita', 'sar', 'kees', 'ian', 'is', 'a', 'feminazi', 'because', 'uh', 'communism']\n",
      "changed data 74 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['reddit', 'user', 'realises', 'her', 'boyfriend', 'is', 'gas', 'lighting', 'her', 'makes', 'him', 'watch', 'gaslight', 'sweet', 'justice']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['because', 'if', 'the', 'answer', 'is', 'nothing', 'then', 'you', 'have', 'blind', 'and', 'irrational', 'belief']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['exclusive', 'sex', 'offenders', 'too', 'violent', 'to', 'leave', 'uk', 'must', 'stay', 'because', 'risk', 'to', 'overseas', 'countries', 'http']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['moral', 'courage', 'ever', 'been', 'called', 'a', 'feminazi', 'watch', 'our', 'new', 'video', 'haha', 'many', 'times']\n",
      "changed data 84 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'really', 'hate', 'when', 'ppl', 'who', 'claim', 'to', 'want', 'gender', 'equality', 'are', 'afraid', 'to', 'call', 'themselves', 'feminists', 'because', 'they', 'think', 'feminist']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['gandhi', 'was', 'pretty', 'sexist', 'but', 'on', 'lady', 'ghostbusters', 'first', 'they', 'ignore', 'you', 'then', 'they', 'laugh', 'at', 'you', 'then', 'they', 'fight', 'you', 'then', 'you', 'win']\n",
      "changed data 134 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['before', 'mohammed', 'the', 'jews', 'lived', 'on', '40', 'of', 'the', 'arabian', 'peninsula', 'almost', 'all', 'that', 'land', 'has', 'been', 'stolen', 'by', 'mus', 'li']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['because', '40', 'of', 'arabian', 'penninsula', 'was', 'jewish', 'before', 'mohammed', 's', 'extermination', 's']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['ever', 'told', 'a', 'sexist', 'joke', 'and', 'a', 'woman', 'teammate', 'laughed', 'along', 'because', 'she', 's', 'cool', 'read', 'this', 'now']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'would', 'joan', 'jett', 'do', 'she', 'would', 'cut', 'her', 'own', 'damn', 'bangs']\n",
      "changed data 58 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'fact', 'i', 'think', 'ill', 'go', 'watch', 'some', 'gilmore', 'girls', 'on', 'netflix', 'goodnight']\n",
      "changed data 72 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', \"don't\", 'know', 'your', 'jewish', 'tribes', 'jews', 'of', 'kh', 'y', 'bar', 'had', 'nothing', 'to', 'do', 'with', 'a', 'treaty', 'against', 'the', 'meccans']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['cannot', 'stand', 'kat', 'on', 'mkr']\n",
      "changed data 27 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['car', 'is', '45', 'christian', 'and', 'the', 'muslims', 'took', 'the', 'government', 'by', 'force', 'finally', 'the', 'christians', 'fought', 'back']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'guys', \"didn't\", 'put', 'up', 'much', 'of', 'a', 'fight', 'in', 'tikrit', 'the', 'fall', 'of', 'the', 'dae', 'sh', 'is', 'on', 'an', 'accelerating', 'curve', 'islam']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['acting', 'like', 'men', 'who', 'respect', 'women', 'are', 'noteworthy', 'implies', 'that', 'men', 'are', 'inherently', 'deviant', \"that's\", 'insulting', 'to', 'all', 'genders']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['new', 'life', 'goal']\n",
      "changed data 13 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"don't\", 'like', 'these', 'girls', 'mkr', 'bi', 'shes', 'no', 'offence', 'girls']\n",
      "changed data 54 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['checked', 'the', 'gdc', '2015', 'tag', 'w', 'w', 'o', 'in', 'place', 'like', 'night', 'day', 'guess', 'which', 'led', 'to', 'more', 'productive', 'tweets']\n",
      "changed data 112 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['never', 'go', 'full', 'robot']\n",
      "changed data 20 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['gamer', 'gate', 'just', 'called', 'a', 'copy', 'boy', 'or', 'personal', 'assistant', 'on', '8chan']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'get', 'rid', 'of', 'kat', 'mkr']\n",
      "changed data 24 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'are', 'either', '100', 'ignorant', 'of', 'your', 'religion', 'or', 'you', 'are', 'a', '100', 'liar', 'you', 'tell', 'me', 'which']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'lets', 'see', 'who', 'the', 'producers', 'think', 'are', 'going', 'to', 'be', 'better', 'tv', 'kat', 'or', 'nikki', 'and', 'katie']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['because', 'when', 'young', 'women', 'look', 'at', 'colleges', 'to', 'attended', 'the', 'rape', 'statistics', 'are', 'more', 'important', 'than', \"it's\", 'qs', 'rankings', 'ye']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'got', 'them', 'directly', 'off', 'that', 'muslim', 'students', 'website']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['chortle', 'that', 'blind', 'date', 'last', 'night', 'is', 'why', 'i', \"don't\", 'do', 'blind', 'dates', 'all', 'she', 'did', 'was', 'call', 'me', 'sexist', 'because', 'i', 'like', 'being', 'chivalrous']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'does', 'that', 'have', 'to', 'do', 'with', 'the', 'barbarity', 'of', 'islam']\n",
      "changed data 54 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'like', 'these', 'girls', \"we're\", 'totes', 'pretty', 'so', \"let's\", 'get', 'the', 'men', 'thinking', 'with', 'their', 's', 'patch', 'cocks', 'but', 'how', 'oh', 'mkr']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', \"don't\", 'care', 'who', 'it', 'is', 'just', 'as', 'long', 'as', \"it's\", 'not', 'the', 'promo', 'girls', 'mkr']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['rape', 'is', 'not', 'a', 'rhetorical', 'device', 'rape', 'is', 'not', 'a', 'rhetorical', 'device', 'rape', 'is', 'not', 'a', 'rhetorical', 'device']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', '2015', 'stay', 'classy', 'steam', 'reviewers', 'you', 'could', 'at', 'least', 'use', 'ter', 'f', 'or', 'something', 'and', 'have', 'vague', 'plausible', 'deniability']\n",
      "changed data 132 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'why', 'do', 'i', 'think', 'jacs', 'really', 'still', 'has', 'no', 'idea', 'where', 'she', 'is', 'or', \"what's\", \"she's\", 'doing', 'there']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', \"you're\", 'still', 'believing', 'that', 'modern', 'feminism', \"doesn't\", 'demonize', 'men', 'read', 'a', 'couple', 'of', 'tweets', 'in', 'blame', 'one', 'not', 'all']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'katie', 'nikki', 'go', 'i', 'hope', 'one', 'of', 'them', 'give', 'kat', 'a', 'left', 'hook', 'on', 'the', 'way', 'out', 'what', 'a', 'bitch', 'mkr']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['giuliani', 'calls', 'out', 'obama', 'can', 't', 'say', 'radical', 'islam', 'because', 'he', 's', 'a', 'coward']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['certainly', 'the', 'paper', 'shows', 'that', 'the', 'idea', 'that', 'drinking', 'water', 'was', 'the', 'only', 'or', 'main', 'health', 'issue', 'in', 'iraq', 'is', 'a', 'lie']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['who', 'cares', \"it's\", 'about', 'the', 'example', 'that', 'the', 'religion', 'tries', 'to', 'follow', 'that', 'determines', 'if', 'it', 'can', 'be', 'reformed']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['god', 'i', 'hope', 'kats', 'gone', 'soon', 'what', 'are', 'stuck', 'up', 'mkr']\n",
      "changed data 56 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', \"you'll\", 'happily', 'use', 'the', 'word', 'feminazi', 'you', \"don't\", 'get', 'to', 'complain', 'about', 'other', 'people', 'causing', 'trouble']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['another', 'stupid', 'excuse', 'isis', 'is', 'islam', 'not', 'cia', 'or', 'mossad']\n",
      "changed data 56 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['actually', 'never', 'mind', 'no', 'clue', 'who', 'you', 'are', 'really', \"don't\", 'care', 'go', 'be', 'dramatic', 'in', 'someone', \"else's\", 'mentions']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'your', 'argument', 'is', 'that', 'islam', 'is', 'good', 'because', 'others', 'did', 'bad', 'things', 'in', 'the', 'past', 'try', 'again']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['it', 'hurts', 'my', 'brain', 'just', 'trying', 'to', 'come', 'up', 'with', 'sexist', 'facts', 'the', 'sun', 'is', '92', 'million', 'miles', 'from', 'earth']\n",
      "changed data 100 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'kat', 'thought', 'her', 'bake', 'was', 'great', 'so', 'delusional', 'mkr']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'fact', 'it', 'was', 'the', 'jews', 'that', 'settled', 'medina', 'before', 'the', 'arabs', 'then', 'the', 'muslims', 'exterminated', 'them']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['why', \"doesn't\", 'she', 'know', 'that', 'her', 'prophet', 'said', 'that', 'most', 'dwellers', 'of', 'hell', 'were', 'women']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'old', 'school', 'is', 'sexist', 'im', 'not', 'sexist', 'im', 'just', 'old', 'school']\n",
      "changed data 59 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'will', 'not', 'be', 'using', 'killer', 'blondes', 'as', 'a', 'hash', 'tag', 'mkr']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['men', 'denigrating', \"women's\", 'accomplishments', 'in', 'fields', 'they', 'themselves', 'know', 'nothing', 'about', 'since', '100000', '000', 'bc']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['blondes', 'scored', 'fairly', 'k8iemac', 'devastated', 'that', 'kat', 'is', 'still', 'there', \"wasn't\", 'the', 'biggest', 'fan', 'of', 'blondes', 'but', 'liked', 'them', 'more', 'than', 'kat', 'mkr']\n",
      "changed data 143 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'next', 'person', 'who', 'says', 'feminazi', 'will', 'get', 'a', 'slap', 'if', 'you', 'think', 'a', 'woman', 'is', 'mistreating', 'a', 'man', 'you', 'can', 'call', 'her', 'sexist', 'too', 'not', 'feminazi']\n",
      "changed data 138 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'can', 'call', 'a', 'female', 'a', 'feminist', 'or', 'a', 'feminazi', 'but', 'that', 'still', 'does', 'not', 'justify', 'you', 'being', 'a', 'complete', 'inconsiderate', 'jerk', 'defending']\n",
      "changed data 132 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['want', '2', 'get', 'men', '2', 'approach', 'other', 'men', 'on', 'buses', 'trains', 'and', 'sidewalks', 'pestering', 'them', '2', 'b', 'best', 'friends', '247', 'and', 'see', 'how', 'they', 'fucking', 'like', 'it']\n",
      "changed data 139 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['uc', 'irvine', 'which', 'sits', 'amidst', 'former', 'bastion', 'of', 'ca', 'conservatism', 'orange', 'county', 'is', 'a', 'leftist', 'pro', 'islam', 'anti', 'israel', 'radi']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'believe', 'in', 'equality', 'except', 'when', 'im', 'drunk', 'and', 'mad', 'at', 'women', 'why', 'i', \"don't\", 'trust', 'male', 'feminists']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"you're\", 'a', 'feminazi']\n",
      "changed data 20 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'is', 'no', 'sovereignty', 'of', 'dictators', 'only', 'real', 'elections', 'bestow', 'sovereignty']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['declaring', 'war', 'on', 'humanity', 'and', 'murdering', '280', 'million', 'people', 'to', 'allahu', 'akbar', 'is', 'evil']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['nikki', 's', 'inflection', 'doing', 'my', 'head', 'in', 'promo', 'girls', 'mkr', 'squeal', 'ugh']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'is', 'a', 'lie', 'the', 'prophet', 'mohammed', 'had', 'people', 'executed', 'for', 'criticizing', 'him']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"here's\", 'hoping', 'sudden', 'death', 'with', 'kat', 'annie', 'how', 'funny', 'would', 'that', 'be', 'mkr']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['only', 'two', 'verses', 'and', 'we', 'are', 'making', 'progress']\n",
      "changed data 43 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['tod', 'clarey', 'lemon', 'tart', 'looks', 'like', 'shit', 'mkr', 'please', \"don't\", 'hold', 'back', 'tell', 'it', 'how', 'you', 'see', 'it']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['they', \"don't\", 'relate', 'to', 'these', 'women', 'on', 'social', 'media', 'these', 'activists', 'these', 'feminists', 'they', \"don't\", 'relate', 'to', 'them', 'they', 'are', 'more', 'humble']\n",
      "changed data 134 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'run', 'your', 'mouth', 'like', 'a', 'idiot', 'with', 'unsupported', 'assertions', 'but', \"it's\", 'you', 'who', 'knows', 'zil', 'tch', 'bout', 'islam']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['coon', 'lenny', 'prince', '198', 'pounds', 'all', 'solid']\n",
      "changed data 38 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'you', 'talk', 'the', 'talk', 'you', 'gotta', 'walk', 'the', 'walk', 'do', 'the', 'girls', 'know', 'the', 'recipe', 'for', 'humble', 'pie', 'mkr']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['when', 'he', 'hears', 'blue', 'haired', 'bitch', 'gamer', 'gate', 'and', 'freebsd', 'girl', 'oh', 'dear']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'is', 'a', 'lie']\n",
      "changed data 14 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'know', \"you're\", 'doing', 'something', 'right', 'when', 'mr', 'as', 'are', 'throwing', 'a', 'tantrum', 'feminism', 'yes', 'all', 'women', 'feminists', 'feminazi']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'dumb', 'blondes', 'mmm', 'perhaps', 'just', 'delusions', 'of', 'adequacy', 'mkr']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'kat', 'you', 'snake', 'even', 'if', 'you', 'win', 'and', 'open', 'your', 'own', 'restaurant', \"who's\", 'going', 'to', 'go']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['amen', 'stand', 'your', 'ground', 'gun', 'ranges', 'ban', 'on', 'muslims', 'draws', 'fire', 't', 'cot', '2a']\n",
      "changed data 74 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['stop', 'saying', 'sass', 'or', 'i', 'will', 'put', 'my', 'foot', 'up', 'your', 'ass', 'mkr']\n",
      "changed data 54 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['feminazi', 'blog', 'reminds', 'liberals', 'today', 'is', 'national', 'day', 'of', 'appreciation', 'for', 'abortion', 'providers', 'via']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['wrong', 'again', 'asshole', 'just', 'look', 'at', 'the', 'christian', 'vs', 'muslim', 'population', 'demographic', 'for', 'the', 'last', '30', 'years']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'slave', 'girl', 'has', 'no', 'right', 'at', 'all', 'liar', 'the', 'owner', 'could', 'kill', 'here', 'and', 'there', 'would', 'be', 'no', 'punishment']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"let's\", 'see', 'why', 'would', 'there', 'be', 'a', 'movement', 'against', 'islam', 'but', 'not', 'against', 'any', 'other', 'religion']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'sassy', 'foods', 'feeling', 'mkr', '2015', 'mkr', 'these', 'girls', 'are', 'vile', 'people', 'killer', 'blondes', 'basic', 'hot', 'dog', 'no', 'bun']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['all', 'those', 'damn', 'uppity', 'women', 'demanding', 'a', 'better', 'quality', 'of', 'life', 'always', 'causing', 'problems']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['tell', 'me', 'how', 'old', 'you', 'are', 'i', \"don't\", 'want', 'to', 'be', 'here', 'attacking', 'a', 'child', 'about', 'her', 'religion']\n",
      "changed data 86 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['isis', 'on', 'islam', 'and', 'indoctrination', 'by', 'peter', 'townsend', 'today', 'free', 'copy', 'get', 'yours', 'now']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', 'purposely', 'pick', 'slightly', 'unattractive', 'girls', 'whore', 'besotted', 'with', 'themselves', 'makes', 'good', 'television', 'lol']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['when', 'did', 'kat', 'become', 'mr', 'burns', 'evil', 'mkr', 'shame', 'on', 'you']\n",
      "changed data 50 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mkr', \"you'd\", 'think', 'in', 'her', 'downtime', 'annie', 'would', 'have', 'paid', 'na', 'polon', 'per', 'dis', 'a', 'visit', 'and', 'learnt', 'not', 'to', 'use', 'the', 'same', 'coloured', 'blush', 'as', 'your', 'hair']\n",
      "changed data 139 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['same', 'with', 'feminists', 'though', 'just', 'saying', 'you', 'are', 'one', 'can', 'lead', 'to', 'instant', 'hostility', 'or', 'stuff', 'like', 'ur', 'not', 'a', 'feminazi', 'so', 'ur', 'ok']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'really', 'the', 'radicals', 'follow', 'the', 'religion', 'to', 'the', 'letter']\n",
      "changed data 59 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['here', 'is', 'a', 'comparison', 'of', 'slavery', 'in', 'islam', 'as', 'compared', 'to', 'the', 'west']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['tweet', 'stats', 'hash', 'tag', 'not', 'your', 'shield', 'mention', 'rogue', 'star', 'rt', 'quickly', 'create', 'a', 'shield', 'from', 'the', 'person', 'that', 'posted', \"women's\", 'addresses', 'on', 'twitter']\n",
      "changed data 138 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['halloween', 'is', 'a', 'busy', 'day', 'for', 'sexist', 'assholes']\n",
      "changed data 44 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['but', 'you', 'keep', 'returning', 'to', 'the', 'stupid', 'idea', 'that', 'christian', 'barbarity', 'justifies', 'muslim', 'barbarity']\n",
      "changed data 94 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'destruction', 'of', 'minority', 'populations', 'by', 'islam', 'continues']\n",
      "changed data 59 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'these', 'are', 'hadiths', 'that', 'i', 'copied', 'from', 'the', 'usc', 'muslim', 'students', 'hadith', 'database', 'look', 'them', 'up', 'for', 'yourself']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"can't\", 'get', 'any', 'more', 'freaky', 'than', 'having', 'sex', 'slaves', 'beating', 'women', 'making', 'women', 'slaves']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kat', \"didn't\", 'like', 'the', 'can', 'oodles', 'mkr', 'mkr', '2015']\n",
      "changed data 45 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['wife', 'beating', 'permitted', 'in', 'islam', 'read', 'the', 'debate', 'and', 'give', 'your', 'opinion']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['arab', 'nations', 'deeply', 'worried', 'by', 'iran', 'nuke', 'deal', 'israel', 'c', 'cot', 't', 'cot', 't', 'lot', 'gop', 'terror']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'fairly', 'obvious', 'that', 'you', 'are', 'willing', 'to', 'throw', 'women', 'under', 'the', 'bus', 'so', 'that', 'muslims', 'can', 'go', 'on', 'abusing', 'them']\n",
      "changed data 109 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kahlil', 'i', 'wrote', 'a', 'good', 'book', 'explaining', 'quantum', 'theory', 'that', 'i', 'read', 'but', 'his', 'opinions', 'on', 'muslim', 'scientists', 'are', 'hyperbolic']\n",
      "changed data 118 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'going', 'to', 'blow', 'your', 'mind', 'every', 'sentence', 'that', 'contains', 'an', 'adjective', 'signals', 'an', 'opinion', 'sexist', 'facts', 'maybe', 'not', 'colors', 'maybe']\n",
      "changed data 127 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['rape', 'is', 'not', 'a', 'punch', 'line']\n",
      "changed data 25 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['again', 'you', 'present', 'your', 'imams', 'propaganda', 'you', 'have', 'no', 'hadiths', 'or', 'verses']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['spirituality', 'is', 'a', 'connection', 'with', 'god', 'that', 'is', 'direct', 'and', 'includes', 'no', 'prophets', 'or', 'imams']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['people', 'are', 'taking', 'this', 'chris', 'paul', 'thing', 'way', 'to', 'far', 'joke', 'not', 'sexist', 'just', 'complaining']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['is', 'kat', 'strategic', 'or', 'just', 'a', 'sore', 'loser', 'mkr']\n",
      "changed data 42 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['men', 'acting', 'like', 'mythology', 'is', 'static', 'and', 'the', 'characters', \"don't\", 'change', 'drastically', 'over', 'generations']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['it', \"wouldn't\", 'be', 'fair', 'kat', 'knows', 'nothing', 'of', 'fair', 'wtf', 'hypocrite', 'mkr']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'sugar', 'and', 'butter', 'and', 'salt']\n",
      "changed data 30 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['by', 'the', 'way', 'i', 'have', 'to', 'ask', 'do', 'you', 'really', 'consider', 'islam', 'to', 'be', 'spiritual', 'really']\n",
      "changed data 82 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'jihadists', 'that', 'slaughtered', 'french', 'magazine', 'editor', 'and', 'staff', 'men', 'spoke', 'french', 'perfectly']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['these', 'white', 'racists', 'are', 'unqualified', 'and', 'were', 'only', 'hired', 'because', 'of', 'affirmative', 'action', 'for', 'whites', 'otherwise', 'known', 'as', 'life']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'have', 'disgraced', 'our', 'country', 'first', 'they', 'covered', 'for', 'seville', 'then', 'they', 'failed', 'with', 'muslims', 'rapes', 'now', 'this']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'meals', \"weren't\", 'great', 'but', 'they', 'didnt', 'deserve', '17', 'mkr']\n",
      "changed data 56 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['katie', 'and', 'nikki', 'shouldve', 'called', 'their', 'restaurant', 'conceited', 'not', 'sassy', 'mkr']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'no', 'one', 'said', 'only', 'women', 'should', 'stick', 'together', 'non', 'rapist', 'men', 'can', 'also', 'stick', 'together', 'with', 'their', 'women', 'friends']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'just', 'someone', 'in', 'a', 'youtube', 'comment', 'call', 'himself', 'a', 'feminist', 'and', 'then', 'proceed', 'to', 'describe', 'someone', 'as', 'a', 'feminazi', 'alright']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'they', 'had', 'no', 'rights', 'in', 'a', 'court', 'of', 'law', 'and', 'had', 'to', 'hide', 'the', 'practice', 'of', 'their', 'religion', 'from', 'muslims', 'and', 'had', 'to', 'pay', 'jiz', 'ya']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['nu', 'lands', 'nazis', 'i', \"didn't\", 'know', 'that', 'the', 'muslims', 'were', 'working', 'for', 'nuland']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'was', 'called', 'a', 'feminazi', 'well', \"that's\", 'new']\n",
      "changed data 47 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['oh', 'no', 'no', 'churn', 'er', 'good', 'luck', 'to', 'the', 'girls', 'they', 'are', 'gonna', 'need', 'it', 'mkr']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'and', 'the', 'thought', 'that', 'a', 'significant', 'other', 'would', 'be', 'targeted']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['new', 'life', 'mantra']\n",
      "changed data 15 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'islam', \"there's\", 'a', 'concept', 'called', 'taqiyya', 'which', 'dictates', 'that', 'lying', 'is', 'acceptable', 'if', 'it', 'advances', 'the', 'cause', 'of', 'allah', 'http']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['blame', 'one', 'not', 'all', 'you', \"don't\", 'assume', 'all', 'muslims', 'are', 'terrorists', 'so', 'why', 'assume', 'all', 'men', 'are', 'rapists']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['5', 'muslims', 'arrested', 'in', 'horrific', '4', 'week', 'long', 'repeated', 'gang', 'rape']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'that', 'is', 'what', 'you', 'and', 'islam', 'do']\n",
      "changed data 34 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['we', 'are', 'talking', 'about', 'what', 'mohammed', 'believed', 'and', 'is', 'therefore', 'a', 'part', 'of', 'islam']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['blondes', 'vs', 'blondes', 'mkr']\n",
      "changed data 22 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['shares', 'australia', 'pm', 'announces', 'new', 'counter', 'terror', 'measures', 'tony', 'abbott', 'says', 'st', 'by', 'arab']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'is', 'watching', 'mkr', 'i', 'have', 'to', 'say', 'that', 'kat', 'seems', 'like', 'an', 'absolute', 'crazy', 'bitch']\n",
      "changed data 79 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'and', 'muslims', 'shame', 'islam']\n",
      "changed data 30 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'what', 'is', 'the', 'conspiracy', 'islam', 'states', 'clearly', 'that', 'they', 'want', 'to', 'murder']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['palestinians', 'are', 'half', 'egyptian', 'and', 'half', 'saudi', 'palestinians', 'are', 'the', 'spearhead', 'for', 'the', 'arabs', 'against', 'the', 'jews']\n",
      "changed data 109 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['neo', 'nazis', 'and', 'holocaust', 'deniers', 'adore', 'because', 'he', 'thinks', 'just', 'like', 'them', 'more']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'was', 'no', 'muslim', 'golden', 'age', 'those', 'states', 'were', 'always', 'slave', 'states']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'scum', 'murdered', '3000', 'unarmed', 'ezi', 'di', 'civilians', 'in', 'cold', 'blood', 'and', 'you', 'expect', 'people', 'to', 'buy', 'your', 'victimhood']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['obviously', 'the', 'driving', 'meme', 'of', 'islam', 'is', 'world', 'domination', 'if', 'it', 'be', 'by', 'force', 'intimidation', 'or', 'persuasion']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"that's\", 'because', \"it's\", 'all', 'about', 'the', 'barbarity', 'of', 'islam', 'not', 'the', 'beards']\n",
      "changed data 69 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['270', 'million', 'people', 'were', 'killed', 'by', 'islamo', 'lunatics', 'shouting', 'allahu', 'akbar']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['look', 'at', 'the', 'data', 'dip', 'wad', '23', 'want', 'sharia', 'law', 'which', 'denies', 'all', 'rights', 'to', 'women', 'and', 'non', 'muslims']\n",
      "changed data 95 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'of', 'course', 'christians', 'all', 'over', 'the', 'world', 'are', 'oppressed', 'daily', 'by', 'muslim', 'bigotry']\n",
      "changed data 82 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'prophet', 'mohammed', 'was', 'illiterate', 'superstitions', 'and', 'knew', 'nothing', 'but', 'war', 'theft', 'and', 'slave', 'trading']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'is', 'amazing', 'is', 'how', 'often', 'allah', 's', 'revelations', 'met', 'mohammed', 'desires', 'but', 'no', 'one', 'became', 'suspicious', 'except', 'aisha']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'all', 'hot', 'air', 'mohammed', 'clearly', 'burned', 'people', 'as', 'shown', 'by', 'the', 'hadith', 'i', 'gave', 'you']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['did', 'your', 'prophet', 'rape', 'slaves', 'like', 'isis', 'yes', 'did', 'your', 'prophet', 'tell', 'women', 'to', 'cover', 'up', 'and', 'stay', 'home', 'like', 'isis', 'yes', 'islam']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['that', 'list', 'of', 'crimes', 'describes', 'what', 'muslims', 'have', 'been', 'doing', 'to', 'jews', 'in', 'the', 'me', 'for', '1400', 'years']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'know', 'that', 'they', 'are', 'forced', 'into', 'islam', 'by', 'being', 'brainwashed', 'and', 'never', 'being', 'given', 'a', 'choice']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['after', 'liberal', 'atheist', 'murders', 'three', 'muslim', 'students', 'who', 'were', 'members', 'of', 'antisemitic', 'muslim', 'bro', 'vi']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'me', 'led', 'the', 'world', 'in', 'development', 'before', 'there', 'was', 'islam', 'and', 'fell', 'behind', 'the', 'world', 'after', 'islam']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['your', 'prophet', 'was', 'a', 'low', 'life', 'murdering', 'raping', 'robbing', 'bigoted', 'sexist', 'pedophile']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'islamic', 'world', 'was', 'still', 'in', 'the', 'dark', 'ages', 'before', 'the', 'french', 'showed', 'up']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['try', 'sending', 'the', 'bible', 'to', 'some', 'muslim', 'lunatics', 'saudi', \"won't\", 'even', 'allow', 'it', 'in', 'the', 'country', 'more', 'religious', 'freedom', 'in', 'israel']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['isis', 'quran', 'islam', 'no', 'compulsion', 'in', 'religion', 'quran', '2256', 'think', 'again']\n",
      "changed data 73 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['any', 'fool', 'can', 'see', 'that', 'is', 'a', 'photoshop', 'job', 'goat', 'fucker']\n",
      "changed data 54 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'hamas', 'trains', 'child', 'soldiers', 'and', 'sends', 'them', 'to', 'get', 'killed', 'for', 'propaganda']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mohammed', 'was', 'illiterate', 'and', 'superstitious', 'and', 'islam', 's', 'backwardness', 'is', 'catching', 'up', 'to', \"it's\", 'jihadis']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'is', 'the', 'most', 'genocidal', 'religion', 'on', 'the', 'planet', 'mankind', 'must', 'exterminate', 'islam', 'or', 'be', 'exterminated', 'by', 'it']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kkk', 'is', 'almost', 'non', 'existent', 'but', 'most', 'muslims', 'are', 'terrorists']\n",
      "changed data 60 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yeah', 'they', 'could', 'use', 'camels', 'for', 'transport', 'abuse', 'and', 'rape', 'women', 'own', 'slaves', 'grovel', 'to', 'a', 'terrorist', 'god', 'pay', 'jiz', 'ya', 'what', 'fun']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['he', 'is', 'lying', \"it's\", 'islam', 'isis', 'does', 'nothing', 'that', 'the', 'prophet', 'mohammed', 'did', 'not', 'also', 'do']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'islamic', 'communities', 'have', 'proven', 'themselves', 'to', 'be', 'violent', 'lunatics']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['here', 'is', 'the', 'quran', 'stating', 'that', 'the', 'witness', 'of', 'a', 'woman', 'is', 'worth', 'half', 'that', 'of', 'a', 'man', 'islam']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mohammed', 'raped', 'a', 'i', 'shia', 'when', 'she', 'was', '9']\n",
      "changed data 37 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['all', 'religions', 'are', 'not', 'the', 'same', 'islam', 'is', 'more', 'violent', 'than', 'other', 'religions']\n",
      "changed data 75 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'difference', 'is', 'that', 'islam', 'has', 'a', 'requirement', 'for', 'jihad', 'has', 'over', '100', 'large', 'terrorist', 'orgs', 'and', 'declared', 'war', 'on', 'world']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['23', 'my', 'ass', 'at', 'least', 'half', 'of', 'them', 'are', 'murderous', 'lunatics']\n",
      "changed data 58 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['france', 'is', 'on', 'the', 'verge', 'of', 'major', 'cultural', 'backlash', 'against', 'even', 'moderate', 'islam', 'the', 'tension', 'has', 'built', 'for', 'decades', 'the', 'line', 'has']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'ap', 'hart', 'hide', 'is', 'a', 'muslim', 'thing', 'they', 'have', 'exterminated', 'jews', 'and', 'christians', 'in', 'most', 'of', 'me']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['humanity', 'struggled', 'for', 'hundreds', 'of', 'years', 'to', 'rid', 'itself', 'of', 'the', 'blight', 'of', 'slavery', 'leave', 'it', 'to', 'the', 'filth', 'of', 'islam', 'to', 'bring', 'it', 'right', 'back']\n",
      "changed data 135 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'phony', 'allah', 'worshiping', 'madrassa', 'rats', 'will', 'die', 'at', 'the', 'hands', 'of', 'controllers', 'a', 'thousand', 'miles', 'away']\n",
      "changed data 100 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['true', 'but', 'islam', 'is', 'corrupt', 'from', 'the', 'beginning']\n",
      "changed data 46 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'entire', 'history', 'of', 'islam', 'is', 'one', 'of', 'colonization', 'occupation', 'hatred', 'and', 'bigotry', 'get', 'an', 'education']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['they', 'have', 'been', 'beheading', 'christians', 'jews', 'buddhist', 'hindus', 'ez', 'id', 'is', 'etc', 'for', '1400', 'years', 'their', 'prophet', 'did', 'it', 'from', 'the', 'start']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['muslims', 'have', 'these', 'delusions', 'about', 'producing', 'algebra', 'but', 'many', 'others', 'beat', 'them', 'to', 'it']\n",
      "changed data 86 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['karen', 'armstrong', 'is', 'a', 'liar', 'that', 'tries', 'to', 'whitewash', 'the', 'inherent', 'violence', 'hatred', 'and', 'bigotry', 'of', 'islam', 'and', 'its', 'prophet']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['how', 'can', 'you', 'protest', 'over', 'a', 'cartoon', 'silence', 'over', 'child', 'rape', 'paedophiles']\n",
      "changed data 72 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['muslims', 'are', 'raping', 'christian', 'girls', 'with', 'impunity', 'around', 'the', 'world']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'the', 'quran', 'tells', 'muslims', 'to', 'murder', 'people', 'of', 'other', 'religions']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['but', 'the', 'truth', 'is', 'that', 'mohammed', 'followers', 'were', 'worthless', 'starving', 'thugs', 'that', 'produced', 'nothing', 'and', 'so', 'wanted', 'the', 'riches', 'of', 'kh', 'y', 'bar']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['this', 'coming', 'from', 'a', 'muslim', 'hypocrite', 'the', 'religion', 'that', 'declared', 'war', 'on', 'humanity', 'and', 'are', 'population', 'imperialists']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'is', 'no', 'such', 'thing', 'as', 'islamophobia', 'that', 'is', 'like', 'saying', 'there', 'is', 'nazi', 'phobia']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['gay', 'man', 'thrown', 'off', 'the', 'rooftop', 'by', 'isis', 'for', 'being', 'gay', 'despite', 'what', 'the', 'radical', 'left', 'claims', 'this', 'is', 'the', 'real', 'islam']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['saudi', 'arabia', 'sewer', 'quran', 's', 'when', 'you', \"can't\", 'express', 'your', 'hatred', 'for', 'islam', 'in', 'words', 'throw', 'them', 'in', 'the', 'sewer']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['please', 'draw', 'mohammed', 'just', 'a', 'smiley', 'face', 'will', 'do', 'but', 'draw', 'it', 'and', 'post', 'it', 'i', 'am', 'charlie', 'hebdo']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['an', 'islamist', 'human', 'rights', 'group', 'lol', 'now', 'there', 'is', 'a', 'contradiction', 'in', 'terms']\n",
      "changed data 75 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'are', 'the', 'one', 'who', 'believes', 'in', 'a', 'god', 'that', 'asks', 'one', 'part', 'of', 'his', 'creation', 'to', 'murder', 'another', 'part', 'impossible', 'to', 'get', 'any', 'stupider']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['this', 'is', 'kiddie', 'stuff', 'compared', 'to', 'what', 'muslims', 'do', 'every', 'day']\n",
      "changed data 59 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'muslim', 'land', 'has', 'ever', 'lived', 'under', 'anything', 'but', 'tyrants', 'dummy']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'logic', 'of', 'a', 'retarded', 'madrassa', 'rat']\n",
      "changed data 37 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'we', 'dont', 'ban', 'islam', \"they'll\", 'ban', 'us', 'rt', 'new', 'islamic', 'state', 'video', 'features', 'french', 'cont']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['again', 'you', 'are', 'an', 'idiot', 'the', 'muslims', 'were', 'thugs', 'they', 'quickly', 'figured', 'out', 'that', 'if', 'they', 'forced', 'everyone', 'to', 'convert', 'there']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'followers', 'of', 'the', 'religion', 'give', 'a', 'shit', 'about', 'the', 'prophet', 'of', 'the', 'religion', 'he', 'is', 'their', 'example', 'and', 'always', 'will', 'be']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['fuck', 'islam', 'mohammed', 'was', 'a', 'pedophile', 'murderer', 'bigot', 'sexist', 'rapist', 'slave', 'trader', 'caravan', 'robber', 'and', 'liar']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['most', 'people', 'who', 'become', 'muslims', 'are', 'either', 'forced', 'or', 'pressured', 'into', 'it', 'and', 'the', 'children', 'have', 'no', 'choice']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['rofl', 'no', 'one', 'with', 'a', 'brain', 'will', 'want', 'a', 'madrassa', 'education', 'will', 'they', 'teach', 'mohammed', 's', 'remedies']\n",
      "changed data 94 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['keep', 'repeating', 'and', 'believing', 'your', 'brain', 'dead', 'fairy', 'tales', 'and', 'follow', 'your', 'prophets', 'medical', 'advice']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'has', 'exterminated', 'all', 'kinds', 'of', 'minorities', 'in', 'the', 'me', 'saudi', 'used', 'to', 'be', 'full', 'of', 'minorities']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'are', 'a', 'muslim', 'you', 'are', 'brain', 'dead', 'you', 'repeat', 'what', 'others', 'have', 'said', 'a', 'million', 'times']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'tells', 'us', 'that', 'we', 'must', 'accept', 'sexism', 'so', 'i', 'guess', 'muslim', 'sexism', 'must', 'be', 'accepted', 'or', 'you', 'are', 'racist']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'christianity', 'is', 'banned', 'in', 'muslim', 'countries', 'why', 'do', 'we', 'allow', 'islam', 'here', 'just', 'asking']\n",
      "changed data 86 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['their', 'best', 'age', 'by', 'any', 'objective', 'measure', 'you', 'want', 'to', 'apply', 'is', 'now', 'in', 'secular', 'usa']\n",
      "changed data 82 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['why', 'do', 'blacks', 'coon', 'on', 'television', 'or', 'the', 'movies', 'if', 'they', \"don't\", 'they', \"won't\", 'work', 'they', \"won't\", 'make', 'money']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['not', 'to', 'mention', 'crushing', 'poverty', 'endless', 'violence', 'and', 'legal', 'forced', 'marriage', 'of', '8', 'year', 'old', 'girls']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['hard', 'to', 'get', 'stupider', 'than', 'you', 'still', 'no', 'change', 'in', 'ratios', 'justified', 'by', 'anything', 'israel', 'did']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['whatever', 'happens', 'in', 'yemen', 'when', 'it', 'is', 'over', 'the', 'country', 'will', 'be', 'in', 'the', 'hands', 'of', 'one', 'islamofascist', 'group', 'or', 'another', 'islam']\n",
      "changed data 120 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'hate', 'atmosphere', 'is', 'built', 'into', 'islam', 'so', 'extinguish', 'it']\n",
      "changed data 58 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'is', 'about', 'forcing', 'people', 'to', 'be', 'muslims', 'even', 'your', 'perverted', 'imams', 'know', 'it']\n",
      "changed data 79 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mohammed', 'the', 'cancer', 'of', 'murder', 'hatred', 'slavery', 'bigotry', 'and', 'sexism', 'for', 'mankind']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'rape', 'murder', 'pedophelia', 'bigotry', 'war', 'sexism', 'and', 'genocide', 'are', 'just', 'and', 'peaceful', 'the', 'islam', 'is', 'just', 'and', 'peaceful']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'micro', 'brain', 'im', 'okay', 'with', 'israel', 'doing', 'what', 'they', 'have', 'to', 'do', 'to', 'stop', 'the', 'same', 'extermination', 'the', 'muslims', 'have', 'done', 'for', '1400', 'yrs']\n",
      "changed data 130 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'has', 'never', 'been', 'a', 'resistance', 'to', 'oppression', 'it', 'has', 'always', 'been', 'source', 'of', 'oppression', 'to', 'both', 'believers', 'and', 'non', 'believer']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'one', 'will', 'cut', 'my', 'head', 'off', 'or', 'sell', 'me', 'into', 'slavery', 'the', 'only', 'thing', 'that', 'will', 'happen', 'is', 'the', 'irs', 'will', 'take', 'some', 'of', 'my', 'property']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['more', 'left', 'wing', 'scum', 'caving', 'in', 'to', 'the', 'violence', 'of', 'islam']\n",
      "changed data 55 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islamophobia', 'is', 'like', 'the', 'idea', 'of', 'nazi', 'phobia', 'islam', 'is', 'a', 'religion', 'of', 'hate', 'and', 'it', 'must', 'be', 'outlawed']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['most', 'of', 'mohammed', 's', '17', 'military', 'expeditions', 'were', 'offensive']\n",
      "changed data 58 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['good', 'tweet', 'but', 'they', 'actually', 'start', 'selling', 'their', 'daughters', 'at', '9']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['peace', 'you', 'idiot', 'islam', 'declared', 'war', 'on', 'all', 'humanity', '1400', 'years', 'ago', 'how', 'can', 'you', 'blab', 'on', 'about', 'peace', 'time', 'to', 'exterminate', 'islam']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'nuns', 'are', 'wearing', 'the', 'chosen', 'robs', 'of', 'the', 'clergy', 'not', 'what', 'every', 'woman', 'is', 'required', 'to', 'wear', 'by', 'the', 'sexist', 'religion']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['nothing', 'desperate', 'about', 'it', 'humanity', 'realizes', 'now', 'that', 'islam', 'declared', 'war', 'on', 'it', '1400', 'years', 'ago', 'and', 'islam', 'will', 'be', 'destroyed']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mohammed', 'led', '17', 'major', 'military', 'expeditions', 'that', 'killed', 'tens', 'of', 'thousands', 'jesus', 'led', 'none']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['too', 'stupid', 'hamas', 'is', 'a', 'murdering', 'terrorist', 'group', 'that', 'forces', 'islamist', 'oppression', 'on', 'everyone', 'the', 'can']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['sir', 'winston', 'churchhill', 'islam', 'is', 'a', 'dangerous', 'in', 'a', 'man', 'as', 'rabies', 'in', 'a', 'dog']\n",
      "changed data 74 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'we', 'would', 'all', 'be', 'worshiping', 'the', 'god', 'of', 'terrorism']\n",
      "changed data 52 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['true', 'and', 'the', 'outcome', 'has', 'to', 'be', 'bad', 'when', 'either', 'group', 'of', 'islamo', 'lunatics', 'wins']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'prophet', 'mohammed', 'used', 'catapults', 'on', 'cities', 'killing', 'women', 'and', 'children', 'so', 'you', 'just', 'called', 'him', 'a', 'coward']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islamophobia', 'is', 'like', 'nazi', 'phobia', \"it's\", 'an', 'abuse', 'of', 'language', 'for', 'political', 'gain', 'it', 'looks', 'like', 'this']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['do', 'you', 'think', 'the', 'yokels', 'at', 'salon', 'know', 'that', 'mohammed', 'was', 'a', 'slave', 'trader', 'who', 'owned', '28', 'slaves', 'not', 'a', 'chance', 'islam']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['war', 'chopping', 'off', 'heads', 'raping', 'slave', 'girls', 'were', 'mohammed', 's', 'favorite', 'activities']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'first', 'slaughter', 'ers', 'were', 'the', 'muslims', 'they', 'invaded', 'the', 'christian', 'world', '400', 'years', 'before', 'the', 'crusades']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['events', 'event', 'interior', 'ministry', 'launches', 'major', 'antiterror', 'exercise', 'arab', 'arabs', 'islam']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['saudi', 'sewer', 'quran', 's', 'what', 'a', 'perfect', 'idea', 'and', 'what', 'a', 'perfect', 'place', 'for', 'a', 'quran', 'saudi', 'islam', 'isis', 'pakistan']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'is', 'the', 'perpetuity', 'of', 'oppression', 'and', 'i', 'thank', 'isis', 'for', 'helping', 'the', 'world', 'to', 'kill', 'islam']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['really', 'muslims', 'understand', 'this', 'they', 'just', 'want', 'to', 'be', 'able', 'to', 'use', 'the', 'name', 'racism', 'to', 'shut', 'us', 'up']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['im', 'not', 'interested', 'in', 'your', 'sectarian', 'divisions', 'shia', 'and', 'sunni', 'are', 'all', 'following', 'the', 'vile', 'pedophile', 'prophet']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'we', 'have', 'heard', 'it', 'a', 'million', 'times', 'islam', 'is', 'not', 'about', 'islam']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['blame', 'one', 'not', 'all', 'because', 'the', 'problem', 'is', 'clearly', 'not', 'systemic', 'at', 'all', \"that's\", 'why', 'all', 'problems', 'ended', '1000', 'years', 'ago', 'when', 'the', 'on', 'l']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['another', 'cheap', 'muslim', 'lie', 'give', 'me', 'proof', 'of', 'any', 'muslim', 'that', 'was', 'punished', 'for', 'not', 'being', 'kind', 'to', 'the', 'slave', 'girls', 'they', 'raped']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'unbelievable', 'the', 'way', 'that', 'journalists', 'fall', 'all', 'over', 'themselves', 'to', 'make', 'excuses', 'for', 'the', 'barbarity', 'of', 'islam', 'not', 'one', 'has', 'read', 'the', 'quran']\n",
      "changed data 139 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'is', 'simply', 'an', 'excuse', 'to', 'rape', 'murder', 'loot', 'practice', 'bigotry', 'sexism', 'and', 'hatred', 'the', 'prophet', 'mohamed', 'designed', 'it', 'to', 'get', 'thugs', 'to', 'follow']\n",
      "changed data 140 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['taqiyya', 'is', 'used', 'by', 'all', 'muslims', 'mohammed', 'was', 'a', 'liar', 'who', 'often', 'lied', 'for', 'islam']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'hatred', 'against', 'jews', 'and', 'christians', 'and', 'others', 'was', 'begun', 'by', 'mohammed']\n",
      "changed data 72 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'know', 'more', 'about', 'the', 'religion', 'than', 'you', 'and', 'it', 'is', 'pure', 'evil']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['isis', 'is', 'il', 'terror', 'terrorism', 'terrorist', 'terrorists', 'israel', 'islam', 'islamic', 'zion', 'zionist', 'arab']\n",
      "changed data 86 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['thank', 'u', 'god', 'for', 'blessing', 'real', 'hustlers', 'for', 'real', 'niggas', 'meek', 'mill', 'season', 's', 'o', 'coon', 'vine', 'by', 'the', 'theory', 'is']\n",
      "changed data 110 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'prophet', 'mohammed', 'declared', 'war', 'on', 'all', 'non', 'muslims', '1400', 'years', 'ago', 'exactly', 'to', 'impose', 'such']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['already', 'did', 'found', 'out', 'that', 'islam', 'is', 'a', 'vile', 'bigoted', 'hateful', 'murdering', 'sexist', 'slavery', 'supporting', 'cult']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'like', 'that', 'picture', 'of', 'israel', 'in', 'the', 'context', 'of', 'the', 'muslim', 'world', 'a', 'tiny', 'spec', 'they', 'blame', 'for', 'all', 'their', 'own', 'evil']\n",
      "changed data 112 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['idiots', 'like', 'you', 'making', 'such', 'declarations', 'have', 'no', 'contact', 'with', 'reality', 'islam', 'is', 'inhuman', 'and', 'must', 'be', 'outlawed']\n",
      "changed data 109 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['let', 'people', 'read', 'the', 'quran', 'and', 'see', 'that', 'it', 'comes', 'from', 'hate']\n",
      "changed data 58 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'treatment', 'of', 'christians', 'was', 'nearly', 'as', 'bad', 'and', 'people', 'not', 'of', 'the', 'book', 'were', 'murdered', 'outright']\n",
      "changed data 97 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['retweet', 'my', 'famous', 'post', 'exposing', 'zionist', 'jews', 'evil', 'gay', 'feminazi', 'behind', 'genocide']\n",
      "changed data 86 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yeap', 'there', 'is', 'only', 'so', 'much', 'bandwidths', 'for', 'self', 'genocidal', 'jews', 'and', \"it's\", 'blumenthal', 's', 'turn', 'to', 'be', 'the', 'center', 'of', 'attention']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['a', 'shit', 'sucking', 'muslim', 'bigot', 'like', 'you', \"wouldn't\", 'recognize', 'history', 'if', 'it', 'crawled', 'up', 'your', 'cunt', 'you', 'think', 'photoshop', 'is', 'a', 'truth', 'machin']\n",
      "changed data 128 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['outlaw', 'islam']\n",
      "changed data 13 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'poor', 'women', 'do', 'it', 'because', 'they', 'marry', 'a', 'muslim', 'man', 'and', 'are', 'pressured', 'into', 'it', 'men', 'do', 'it', 'in', 'prison', \"it's\", 'a', 'criminals', 'relig']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'there', 'is', 'the', 'next', 'thing', 'that', 'is', 'wrong', 'with', 'muslims', 'most', 'of', 'you', 'are', 'liars', '80', 'were', 'killed', 'by', 'taliban']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['we', 'are', 'in', 'an', 'era', 'where', 'everyone', 'has', 'come', 'to', 'understand', 'what', 'the', 'real', 'islam', 'is', 'all', 'about']\n",
      "changed data 88 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['this', 'is', 'a', 'list', 'of', 'lies', 'and', 'baloney', 'that', 'you', 'are', 'presenting', 'it', 'has', 'zero', 'to', 'do', 'with', 'the', 'quran', 'and', 'hadiths']\n",
      "changed data 105 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'problem', 'is', 'that', 'i', 'have', 'far', 'more', 'knowledge', 'of', 'islam', 'than', 'you', 'and', 'it', 'is', 'your', 'mind', 'that', 'has', 'been', 'shrunken', 'by', 'islam']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'what', 'are', 'the', 'muslim', 'scholars', 'going', 'to', 'do', 'say', 'you', 'caught', 'us', 'our', 'religion', 'is', 'violent', 'and', 'barbaric']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['are', 'those', 'the', 'ezi', 'di', 'children', 'murdered', 'by', 'muslims']\n",
      "changed data 49 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['most', 'of', 'the', 'christians', 'that', 'were', 'a', 'part', 'of', 'the', 'palestinians', 'have', 'been', 'killed', 'or', 'driven', 'out', 'by', 'palestinian', 'muslims']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['objecting', 'to', 'this', 'is', 'what', 'the', 'pc', 'morons', 'of', 'salon', 'call', 'racism']\n",
      "changed data 61 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['con', 'watermelon', 'and', 'grape', 'soda', 'coon', 'lol', 'rt', 'i', 'need', 'about', '50lbs', 'of', 'that', '280', 'lb', 'catfish', 'that', 'was', 'caught']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['exactly', 'is', 'this', 'why', 'europe', 'has', 'sharia', 'no', 'go', 'zones', 'so', 'they', 'can', 'hide', 'their', 'child', 'rapists']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'declared', 'war', 'on', 'all', 'mankind', '1400', 'years', 'ago', 'now', 'muslims', 'cry', 'like', 'babies', 'when', 'others', 'respond', 'in', 'kind']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['wrong', 'isis', 'follows', 'the', 'example', 'of', 'mohammed', 'and', 'the', 'quran', 'exactly']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['regarding', 'slavery', 'the', 'slaves', 'in', 'america', 'were', 'initially', 'sold', 'by', 'muslim', 'slavers', 'christians', 'got', 'rid', 'of', 'slavery', 'on', 'their', 'own']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'you', 'can', 'lie', 'about', 'islam', 'all', 'you', 'want', 'and', 'you', 'can', 'deny', 'all', 'the', 'evidence', 'all', 'you', 'want', 'no', 'one', 'cares', 'about', 'opinion']\n",
      "changed data 117 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'your', 'prophet', 'said', 'was', 'that', 'women', 'are', 'stupid', 'and', 'corrupt', 'islam']\n",
      "changed data 67 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['graphic', 'of', 'growth', 'of', 'radical', 'islam', 'following', '6', 'years', 'of', 'care', 'nurture', 'coddling', 'by', 'us', 'democrats', 'islamists']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['13', 'of', 'all', 'muslims', 'believe', 'that', 'people', 'who', 'want', 'to', 'leave', 'the', 'religion', 'should', 'be', 'murdered']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'people', 'who', 'disagreed', 'with', 'islam', 'at', 'the', 'time', 'of', 'the', 'pedophile', 'prophet', 'were', 'murdered', 'by', 'him', \"it's\", 'in', 'the', 'hadiths']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'how', 'much', 'freedom', 'of', 'speech', 'is', 'there', 'in', 'gaza', 'and', 'do', 'you', 'jew', 'hating', 'bigots', 'write', 'about', 'that']\n",
      "changed data 94 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['why', 'do', 'you', 'complain', 'islam', 'has', 'been', 'ethnic', 'cleansing', 'everybody', 'for', '1400', 'years', \"isn't\", 'it', 'time', 'to', 'return', 'the', 'favor']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['who', 'is', 'justifying', 'christianity', 'micro', 'brain', 'i', \"don't\", 'have', 'a', 'religion', 'why', 'are', 'you', 'hiding', 'islamic', 'barbarity']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'is', 'no', 'love', 'and', 'respect', 'in', 'the', 'quran', 'it', 'is', 'full', 'of', 'hatred', 'bigotry', 'sexism', 'and', 'incitement', 'to', 'murder']\n",
      "changed data 106 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['muslim', 'declared', 'war', 'on', 'humanity', 'and', 'their', 'have', 'been', 'murdering', 'men', 'women', 'and', 'children', 'for', '1400', 'years']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'say', 'islam', 'is', 'a', 'religion', 'i', 'say', 'it', 'is', 'a', 'cult', 'islam', 's', 'activities', 'match', 'the', 'description', 'of', 'crime', 'mafia', 'you', 'must', 'bust', 'it']\n",
      "changed data 125 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['inciting', 'hatred', 'is', 'central', 'to', 'islam', 'in', 'order', 'to', 'promote', 'imperialism']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'agree', 'he', 'is', 'just', 'like', 'the', 'prophet', 'mohammed']\n",
      "changed data 46 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'muslims', 'including', 'their', 'prophet', 'revel', 'in', 'their', 'illiteracy']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'is', 'and', 'insult', 'to', 'all', 'mankind', 'time', 'to', 'insult', 'the', 'disgusting', 'religion', 'back']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['isis', 'in', 'mosul', 'executed', '13', 'people', 'for', 'watching', 'soccer', 'game', 'muslims', 'will', 'go', 'crazy', 'about', 'a', 'mohammed', 'cartoon', 'but', \"won't\", 'protest', 'murder', 'islam']\n",
      "changed data 138 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['as', 'a', 'result', 'of', 'muslim', 'hatred', 'there', 'are', 'no', 'jews', 'or', 'christians', 'left', 'in', 'saudi']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['honest', 'and', 'fair', 'is', 'that', 'islam', 'is', 'a', 'religion', 'of', 'hate', 'murder', 'bigotry', 'and', 'sexism']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'reason', 'that', 'is', 'newsworthy', 'is', 'because', 'the', 'muslim', 'contribution', 'is', 'so', 'much', 'more', 'often', 'destructive']\n",
      "changed data 99 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'is', 'no', 'comparing', 'the', 'vileness', 'of', 'mohammed', 'to', 'jesus', 'or', 'buddha', 'or', 'lao', 'tse', 'he', 'was', 'simply', 'a', 'criminal']\n",
      "changed data 102 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['stop', 'threatening', 'people', 'to', 'get', 'them', 'to', 'do', 'what', 'you', 'want', \"that's\", 'why', 'people', 'hate', 'religion']\n",
      "changed data 89 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'the', 'problem', 'of', 'the', 'palestinians', 'and', 'all', 'muslims', 'is', 'that', 'they', 'cannot', 'stand', 'for', 'a', 'jewish', 'state', 'to', 'exist', 'on', 'the', 'peninsula']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['gang', 'rape', 'd', 'saudi', 'woman', 'gets', '200', 'lashes', 'jail', 'time', 'saudis', 'on', 'un', 'human', 'rights', 'council']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['a', 'muslims', 'religious', 'freedom', 'involves', 'denying', 'others', 'their', 'freedom', 'so', 'islam', 'must', 'be', 'outlawed']\n",
      "changed data 96 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'we', 'bombed', 'hitler', 'and', 'various', 'dictators', 'because', 'they', 'refused', 'to', 'be', 'slaves', 'muslims', 'need', 'to', 'get', 'a', 'grip', 'on', 'reality']\n",
      "changed data 115 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['if', 'you', \"don't\", 'like', 'hate', 'then', 'you', 'will', 'have', 'to', 'rewrite', 'half', 'of', 'the', 'quran']\n",
      "changed data 71 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'islam', 'women', 'must', 'be', 'locked', 'in', 'their', 'houses', 'and', 'muslims', 'claim', 'this', 'is', 'treating', 'them', 'well']\n",
      "changed data 92 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['so', 'asshole', 'muslims', 'reserve', 'the', 'right', 'to', 'act', 'like', 'animals', 'and', 'scream', 'like', 'women', 'when', 'other', 'do', 'it']\n",
      "changed data 96 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'sunnah', 'of', 'the', 'prophet', 'is', 'the', 'basis', 'for', 'sharia', 'law', 'along', 'with', 'the', 'quran', 'and', 'the', 'quran', 'is', 'woman', 'hating']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['all', 'you', 'are', 'doing', 'with', 'your', 'verses', 'is', 'proving', 'sexism', 'in', 'islam']\n",
      "changed data 62 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['who', 'cares', 'if', 'it', 'has', 'been', 'revised', 'a', 'million', 'times', 'what', 'does', 'that', 'have', 'to', 'do', 'with', 'the', 'barbarity', 'of', 'islam']\n",
      "changed data 104 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'that', 'was', 'just', 'your', 'pedophile', 'prophet', 'making', 'excuses', 'to', 'slaughter', 'them', 'and', 'steal', 'their', 'land', 'and', 'property']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['same', 'time', 'admiring', 'the', 'ottoman', 'state', 'which', 'was', 'a', 'total', 'authoritarian', 'slave', 'state']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['may', 'allah', 'piss', 'on', 'him', 'before', 'giving', 'him', 'his', '72', 'virgin', 'pigs']\n",
      "changed data 59 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'is', 'a', 'good', 'place', 'for', 'criminals', 'to', 'find', 'a', 'home']\n",
      "changed data 51 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'by', 'the', 'way', 'the', 'entire', 'world', 'hates', 'islam']\n",
      "changed data 45 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'pray', 'that', 'the', 'lying', 'criminal', 'bitch', 'goes', 'to', 'prison']\n",
      "changed data 52 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['look', 'the', 'heroes', 'of', 'the', 'torah', 'exterminated', 'every', 'man', 'woman', 'child', 'and', 'animal', 'in', 'some', 'cities']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['who', 'cares', 'where', 'they', 'were', 'born', 'camel', 'breath', 'they', 'call', 'themselves', 'israelis', 'and', 'jews']\n",
      "changed data 85 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['it', \"can't\", 'be', 'separate', 'while', 'those', 'wishing', 'to', 'destroy', 'israel', 'use', 'it', 'as', 'a', 'basis', 'of', 'their', 'genocidal', 'fanaticism']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['majority', 'of', 'muslims', 'are', 'sexist', 'and', 'bigots']\n",
      "changed data 42 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['isis', 'practices', 'genocide', 'because', 'the', 'prophet', 'mohammed', 'practiced', 'it']\n",
      "changed data 66 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['why', 'should', 'the', 'world', 'care', 'when', 'islam', 'declared', 'war', 'on', 'all', 'humanity', '1400', 'years', 'ago']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['furthermore', 'many', 'muslim', 'countries', \"don't\", 'even', 'have', 'a', 'record', 'system', 'to', 'account', 'for', 'all', 'murders']\n",
      "changed data 93 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mohammed', 'loved', 'robbing', 'caravans', 'and', 'looting', 'for', 'a', 'living', 'isis', 'robs', 'and', 'lots', 'for', 'a', 'living']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['read', 'it', 'liberals', 'get', 'a', 'reality', 'check', 'on', 'life', 'within', 'islam', 'your', 'views', 'beliefs', 'any', 'democracy', 'system', 'mean', 'nothing', 'http']\n",
      "changed data 127 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'bibi', 'is', 'telling', 'europe', 'n', 'jews', 'to', 'move', 'to', 'israel', 'so', 'that', 'they', \"won't\", 'have', 'to', 'face', 'the', 'same', 'muslim', 'persecution', 'as', 'in', 'europe']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'not', 'israelis', 'that', 'are', 'constantly', 'murdering', 'christians', 'hindus', 'for', 'blasphemy', 'in', 'pakistan', \"it's\", 'genocidal', 'muslim', 'mobs']\n",
      "changed data 126 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"it's\", 'basically', 'an', 'empty', 'excuse', 'that', 'leftists', 'and', 'muslims', 'use', 'for', 'their', 'anti', 'us', 'bigotry']\n",
      "changed data 87 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'question', 'is', 'is', 'there', 'hope', 'for', 'a', 'peac', 'ful', 'coexisting', 'islam', 'no', 'the', 'answer', 'i']\n",
      "changed data 80 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['political', 'satire', 'always', 'pisses', 'off', 'someone', 'why', 'should', 'the', 'religion', 'of', 'hatred', 'and', 'murder', 'be', 'excluded']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['german', 'professor', 'gives', 'meaning', 'to', 'term', 'femi', 'nazi', 'doesn', 't', 'hire', 'indian', 'men', 'bcz', 'of', 'india', 'ss', 'rape', 'problem']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['a', 'typical', 'muslim', 'response', 'is', 'to', 'excuse', 'muslim', 'barbarity', 'by', 'pointing', 'elsewhere']\n",
      "changed data 78 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'furthermore', 'they', 'murder', 'anyone', 'who', 'tries', 'to', 'convert', 'a', 'muslim', 'but', 'demand', 'the', 'freedom', 'to', 'convert', 'non', 'muslims']\n",
      "changed data 113 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'muslims', 'sold', 'the', 'slaves', 'mohammed', 'owned', '28', 'slaves', 'quran', 'supports', 'slavery']\n",
      "changed data 79 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['fuck', 'you', 'every', 'palestinian', 'school', 'teaches', 'children', 'to', 'hate', 'jews', 'and', 'kuff', 'ir']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['virtually', 'all', 'the', 'women', 'being', 'beaten', 'and', 'tortured', 'in', 'the', 'world', 'are', 'getting', 'it', 'from', 'muslims']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'treats', 'women', 'like', 'slaves', 'and', 'anyone', 'who', 'says', 'otherwise', 'is', 'stabbing', 'women', 'in', 'the', 'back']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['her', 'very', 'actions', 'are', 'proof', 'that', 'she', 'is', 'a', 'liar', 'and', 'a', 'propagandist', 'because', 'if', 'she', 'did', 'to', 'hamas', 'what']\n",
      "changed data 101 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['people', 'were', 'making', 'scientific', 'discoveries', 'including', 'algebra', 'before', 'islam']\n",
      "changed data 75 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'the', 'israelis', 'are', 'fighting', 'no', 'one', 'except', 'the', 'muslim', 'terrorist']\n",
      "changed data 65 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['to', 'say', \"don't\", 'hate', 'islam', 'is', 'as', 'dumb', 'as', 'saying', \"don't\", 'hate', 'nazism']\n",
      "changed data 68 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mohammed', 'told', 'muslims', 'to', 'take', 'over', 'all', 'of', 'the', 'arabian', 'penninsula', 'from', 'jews', 'and', 'christians']\n",
      "changed data 90 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['mohammed', 'raped', 'women', 'jesus', 'did', 'not', 'mohammed', 'had', 'people', 'murdered', 'jesus', 'did', 'not']\n",
      "changed data 81 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['bloodthirsty', 'muslims', 'flock', 'to', 'isis', 'theatre', 'that', 'loop', 'jihadi', 'atrocities', 'they', 'enjoy', 'watching', 'http']\n",
      "changed data 100 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['even', 'with', 'what', 'hitler', 'did', 'to', 'jews', 'still', 'more', 'want', 'to', 'live', 'in', 'the', 'west', 'than', 'under', 'hate', 'filled', 'islam']\n",
      "changed data 98 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['erol', 'exposes', 'the', 'feminazi', 'groomed', 'by', 'zionist', 'jews', 'to', 'join', 'them', 'in', 'hell', 'for', 'eternity', 'as', 'misery', 'craves', 'company']\n",
      "changed data 119 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['these', 'are', 'all', 'excuses', 'to', 'hide', 'the', 'fact', 'that', 'you', 'are', 'following', 'an', 'evil', 'and', 'inhuman', 'religion']\n",
      "changed data 91 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'is', 'a', 'worldwide', 'mega', 'disaster', 'islam', 'wants', 'to', 'drag', 'us', 'down', 'to', 'the', '7th', 'century', 'darkness', 'death', 'and', 'destruction']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'was', 'mohammed', 'defending', 'when', 'he', 'attacked', 'the', 'jews', 'of', 'kh', 'y', 'bar']\n",
      "changed data 64 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'about', 'the', 'environmental', 'racism', 'experience', 'by', 'christians', 'jews', 'ez', 'id', 'is', 'hindus', 'etc', 'in', 'muslim', 'countries']\n",
      "changed data 109 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['chomsky', 'is', 'a', 'racist', 'islam', 'created', 'isis']\n",
      "changed data 40 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islam', 'declared', 'war', 'on', 'all', 'humanity', '1400', 'years', 'ago', 'while', 'individual', 'muslims', 'may', 'or', 'may', 'not', 'be', 'terrorist', 'islam', 'is', 'terrorist']\n",
      "changed data 123 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['they', 'are', 'not', 'protected', 'under', 'islam', 'islam', 'declared', 'war', 'on', 'all', 'humanity', '1400', 'years', 'ago']\n",
      "changed data 86 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['good', 'morning', 'here', 'is', 'the', 'history', 'of', 'islamic', 'slavery']\n",
      "changed data 53 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'the', 'hadiths', 'are', 'even', 'more', 'specific', 'about', 'approving', 'sexual', 'slavery']\n",
      "changed data 70 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['one', 'must', 'imagine', 'god', 'being', 'insulted', 'by', 'having', 'such', 'a', 'hateful', 'war', 'mongering', 'decisive', 'bigoted', 'sexist', 'religion', 'calling', 'him']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'then', 'the', 'pedophile', 'sold', 'all', 'the', 'women', 'and', 'children', 'into', 'slavery', 'and', 'he', 'is', 'the', 'model', 'for', 'hamas', 'and', 'isis']\n",
      "changed data 108 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['since', '13', 'of', 'all', 'islam', 'believes', 'that', 'people', 'who', 'leave', 'the', 'religion', 'should', 'be', 'murdered', 'where', 'are', 'the', 'moderate', 'muslims']\n",
      "changed data 116 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['saying', 'that', 'islam', 'is', 'a', 'way', 'of', 'life', 'means', 'that', 'it', 'tells', 'you', 'that', 'you', 'cannot', 'have', 'democracy', 'and', 'must', 'instead', 'follow', 'a', 'pedophile']\n",
      "changed data 126 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'terrorists', 'follow', 'islam', 'exactly', 'as', 'it', 'is', 'defined', 'in', 'the', 'quran', 'and', 'the', 'hadith', 'there', 'are', 'over', '100', 'islam', 'terrorist', 'groups']\n",
      "changed data 124 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['poor', 'dae', 'sh', 'now', 'they', \"can't\", 'rape', 'her', 'torture', 'her', 'or', 'make', 'an', 'execution', 'video', 'of', 'her']\n",
      "changed data 84 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['no', 'more', 'so', 'than', 'the', 'muslims', 'who', 'consider', 'their', 'birthright', 'to', 'be', 'global', 'domination']\n",
      "changed data 83 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['in', 'the', 'quran', 'islam', 'declared', 'war', 'on', 'all', 'mankind', '1400', 'years', 'ago', 'it', 'is', 'time', 'for', 'humanity', 'to', 'return', 'the', 'favor']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['every', 'day', 'we', 'get', 'the', 'results', 'of', 'the', 'hatred', 'bred', 'by', 'islam']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['muslim', 'bigot', 'wanted', 'by', 'interpol', 'and', 'protected', 'by', 'u', 's', 'ally', 'wants', 'un', 'to', 'criminalize', 'criticism', 'of', 'islam', 'internationally', 'htt']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['nothing', 'has', 'any', 'value', 'for', 'arabs', 'except', 'the', 'destruction', 'of', 'the', 'jews', 'and', 'the', 'arabs', 'must', 'pay', 'palestine', 'to', 'destroy', 'em']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'that', 'is', 'why', 'mohammed', 'called', 'blacks', 'ras', 'in', 'heads']\n",
      "changed data 54 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "[\"let's\", 'stop', 'pretending', 'and', 'get', 'the', 'war', 'between', 'the', 'islamo', 'lunatics', 'and', 'the', 'rest', 'of', 'the', 'world', 'started', 'now']\n",
      "changed data 103 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['kind', 'of', 'the', 'same', 'thing', 'the', 'prophet', 'mohammed', 'did']\n",
      "changed data 48 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['yes', 'muslim', 'bigots', 'are', 'murdering', 'christians', 'all', 'over', 'africa', 'and', 'have', 'been', 'for', 'decades']\n",
      "changed data 86 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'few', 'things', 'islam', 'came', 'up', 'with', 'were', 'from', 'conquered', 'people', 'and', 'came', 'in', 'spite', 'of', 'islam', 'not', 'because', 'of', 'it']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['women', 'should', 'have', 'the', 'same', 'sexual', 'rights', 'as', 'me', 'but', 'islam', 'says', 'they', 'clearly', \"don't\"]\n",
      "changed data 82 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['because', 'islam', 'makes', 'people', 'stupid']\n",
      "changed data 34 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['by', 'showing', 'your', 'love', 'of', 'a', 'genocidal', 'animal', 'like', 'baghdadi', 'you', 'have', 'given', 'the', 'reason', 'why', 'islam', 'must', 'be', 'exterminated']\n",
      "changed data 114 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['here', 'is', 'the', 'quran', 'telling', 'women', 'not', 'to', 'leave', 'their', 'house']\n",
      "changed data 57 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['and', 'if', 'muslims', 'are', 'angry', 'that', 'they', \"can't\", 'force', 'islam', 'on', 'people', 'who', 'cares']\n",
      "changed data 74 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['muslims', 'rape', 'and', 'enslave', '3000', 'ezi', 'di', 'women', 'who', 'did', 'nothing', 'and', 'you', 'give', 'me', 'this']\n",
      "changed data 79 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'can', 'never', 'get', 'rid', 'of', 'the', 'radical', 'muslims', 'because', 'the', 'quran', 'and', 'hadiths', 'will', 'always', 'produce', 'more']\n",
      "changed data 100 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['what', 'is', 'there', 'to', 'teach', 'we', 'both', 'know', 'that', 'the', 'quran', 'is', 'a', 'cult', 'manual', 'of', 'hatred', 'imperialism', 'murder', 'sexism', 'and', 'bigotry']\n",
      "changed data 121 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['islamic', 'state', 'members', 'say', \"they're\", 'following', 'islam', 'while', 'non', 'muslims', 'who', \"don't\", 'even', 'read', 'up', 'on', 'religion', 'insist', \"they're\", 'not']\n",
      "changed data 122 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'was', 'no', 'golden', 'age', 'jews', 'were', 'regularly', 'slaughter', 'by', 'muslims', 'in', 'pogroms']\n",
      "changed data 77 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['the', 'prophet', 'mohammed', 'did', 'the', 'same', 'thing', 'baghdadi', 'can', 'quote', 'quran', 'and', 'hadiths', 'for', 'every', 'action', 'he', 'takes']\n",
      "changed data 107 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['i', 'know', 'that', 'your', 'husband', 'can', 'beat', 'you', 'there', 'and', 'there', 'is', 'nothing', 'you', 'can', 'do']\n",
      "changed data 76 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['he', 'is', 'following', 'the', 'example', 'of', 'his', 'prophet', 'who', 'married', 'one', 'of', 'his', 'wives', 'at', '6', 'they', 'do', 'everything', 'their', 'prophet', 'does', 'exactly', 'is', 'am']\n",
      "changed data 129 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['you', 'claim', 'to', 'be', 'anti', 'religion', 'yet', 'you', 'spend', 'all', 'your', 'time', 'lying', 'for', 'the', 'benefit', 'of', 'islam', 'the', 'religion', 'of', 'hate']\n",
      "changed data 112 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['every', 'muslim', 'convert', 'becomes', 'muslim', 'propaganda']\n",
      "changed data 47 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['lol', 'it', 'is', 'common', 'knowledge', 'and', 'there', 'are', 'countless', 'reports', 'about', 'muslim', 'no', 'go', 'areas', 'in', 'europe', 'just', 'google', 'it']\n",
      "changed data 111 into (1, 31)\n",
      "Tokenizing tweets with tokenize_for_dictionary\n",
      "\n",
      "['there', 'is', 'nothing', 'that', 'isis', 'does', 'that', 'the', 'prophet', 'mohammed', \"didn't\", 'also', 'do']\n",
      "changed data 73 into (1, 31)\n"
     ]
    }
   ],
   "source": [
    "from data.word import fit_input_into_vocab\n",
    "_word = np.array(list(map(lambda x: fit_input_into_vocab(x, vocab=_vocab), pred_positives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_feed_dicts = []\n",
    "_feed_dicts.append({\"input/labels:0\": np.zeros(len(_word)).reshape(len(_word), 1), \"input/X:0\":_word.reshape((len(pred_positives), _word_text_len)) , \"dropout_keep_prob:0\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_names = [\"output/prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/homes/jhpark/hate-speech/logs/combined/word/ckpt/model-360000.ckpt\n",
      "(878,)\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i, ckpt in enumerate(checkpoint_files):\n",
    "    print(ckpt.all_model_checkpoint_paths[0])\n",
    "    tf.reset_default_graph()\n",
    "    saver = tf.train.import_meta_graph(\"{}.meta\".format(ckpt.all_model_checkpoint_paths[0]))\n",
    "\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        graph = tf.get_default_graph()\n",
    "        pred = sess.run(output_names[i] + \":0\", _feed_dicts[i])\n",
    "        print(pred.shape)\n",
    "        preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final predictions\n",
    "for i in range(len(pred_positives)):\n",
    "    if preds[0][i] == 0:\n",
    "        final_pred[pred_positives_idx[i]] = 1 # sexism\n",
    "    else:\n",
    "        final_pred[pred_positives_idx[i]] = 2 # racism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get real labels\n",
    "from data.preprocess import load_from_file\n",
    "\n",
    "data_sexism = load_from_file(\"sexism_binary\")\n",
    "data_racism = load_from_file(\"racism_binary\")\n",
    "\n",
    "x_abusive_valid = []\n",
    "y_abusive_valid = []\n",
    "real_y_abusive_valid = []\n",
    "\n",
    "def put_into_abusive(x_sex, y_sex, x_race, y_race, x, y, _y=None):\n",
    "    assert len(x_sex) == len(y_sex)\n",
    "    assert len(x_race) == len(y_race)\n",
    "    if _y == None:\n",
    "        _y = []\n",
    "    for i in range(len(x_sex)):\n",
    "        if y_sex[i] == 1:\n",
    "            x.append(x_sex[i])\n",
    "            y.append(y_sex[i])\n",
    "            _y.append(1)\n",
    "    for i in range(len(x_race)):\n",
    "        x.append(x_race[i])\n",
    "        y.append(y_race[i])\n",
    "        if y_race[i] == 1:\n",
    "            _y.append(2)\n",
    "        else:\n",
    "            _y.append(0)\n",
    "put_into_abusive(data_sexism[\"x_valid\"], data_sexism[\"y_valid\"], data_racism[\"x_valid\"], data_racism[\"y_valid\"], x_abusive_valid, y_abusive_valid, _y=real_y_abusive_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(y_abusive_valid == data[\"y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(x_abusive_valid == data[\"x_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1876"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x ==0, final_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1865"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x ==0, real_y_abusive_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 1, final_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 1, real_y_abusive_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 2, final_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 2, real_y_abusive_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _evaluate(true, pred, _class):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i] and true[i] == _class:\n",
    "            tp += 1\n",
    "        elif pred[i] != _class and true[i] == _class:\n",
    "            fn += 1\n",
    "        elif pred[i] == _class and true[i] != _class:\n",
    "            fp += 1\n",
    "    return tp / (tp + fp), tp / (tp + fn), tp, fp\n",
    "\n",
    "def _accuracy(true, pred):\n",
    "    count = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            count += 1\n",
    "    return count / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5287569573283859, 0.49137931034482757, 285, 254)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_evaluate(real_y_abusive_valid, final_pred, 1) #sexism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4306784660766962, 0.47249190938511326, 146)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_evaluate(real_y_abusive_valid, final_pred, 2) # racism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8699360341151386, 0.8750670241286863, 1632)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_evaluate(real_y_abusive_valid, final_pred, 0) # none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7490922294843864"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_accuracy(real_y_abusive_valid, final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
