{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver_path = \"./logs/rf3/hybrid/ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                             model-380000.ckpt.index\r\n",
      "model-360000.ckpt.data-00000-of-00001  model-380000.ckpt.meta\r\n",
      "model-360000.ckpt.index                model-390000.ckpt.data-00000-of-00001\r\n",
      "model-360000.ckpt.meta                 model-390000.ckpt.index\r\n",
      "model-370000.ckpt.data-00000-of-00001  model-390000.ckpt.meta\r\n",
      "model-370000.ckpt.index                model-final.ckpt.data-00000-of-00001\r\n",
      "model-370000.ckpt.meta                 model-final.ckpt.index\r\n",
      "model-380000.ckpt.data-00000-of-00001  model-final.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "%ls logs/rf3/hybrid/ckpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/homes/jhpark/hate-speech/logs/rf3/hybrid/ckpt/model-360000.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = tf.train.get_checkpoint_state(saver_path)\n",
    "print(checkpoint_file.all_model_checkpoint_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file.all_model_checkpoint_paths[0]))\n",
    "\n",
    "\n",
    "# create session for evaluation\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "saver.restore(sess, checkpoint_file.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input/X_word',\n",
       " 'input/X_char',\n",
       " 'input/labels',\n",
       " 'input/one_hot/on_value',\n",
       " 'input/one_hot/off_value',\n",
       " 'input/one_hot/depth',\n",
       " 'input/one_hot',\n",
       " 'input/Reshape/shape',\n",
       " 'input/Reshape',\n",
       " 'dropout_keep_prob',\n",
       " 'Const',\n",
       " 'embedding/random_uniform/shape',\n",
       " 'embedding/random_uniform/min',\n",
       " 'embedding/random_uniform/max',\n",
       " 'embedding/random_uniform/RandomUniform',\n",
       " 'embedding/random_uniform/sub',\n",
       " 'embedding/random_uniform/mul',\n",
       " 'embedding/random_uniform',\n",
       " 'embedding/W',\n",
       " 'embedding/W/Assign',\n",
       " 'embedding/W/read',\n",
       " 'embedding/embedding_lookup',\n",
       " 'embedding/ExpandDims/dim',\n",
       " 'embedding/ExpandDims',\n",
       " 'ExpandDims/dim',\n",
       " 'ExpandDims',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/shape',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/mean',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/stddev',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/TruncatedNormal',\n",
       " 'channel0-conv-maxpool-1/truncated_normal/mul',\n",
       " 'channel0-conv-maxpool-1/truncated_normal',\n",
       " 'channel0-conv-maxpool-1/W',\n",
       " 'channel0-conv-maxpool-1/W/Assign',\n",
       " 'channel0-conv-maxpool-1/W/read',\n",
       " 'channel0-conv-maxpool-1/Const',\n",
       " 'channel0-conv-maxpool-1/b',\n",
       " 'channel0-conv-maxpool-1/b/Assign',\n",
       " 'channel0-conv-maxpool-1/b/read',\n",
       " 'channel0-conv-maxpool-1/conv',\n",
       " 'channel0-conv-maxpool-1/BiasAdd',\n",
       " 'channel0-conv-maxpool-1/relu',\n",
       " 'channel0-conv-maxpool-1/pool',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/shape',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/mean',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/stddev',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/TruncatedNormal',\n",
       " 'channel1-conv-maxpool-3/truncated_normal/mul',\n",
       " 'channel1-conv-maxpool-3/truncated_normal',\n",
       " 'channel1-conv-maxpool-3/W',\n",
       " 'channel1-conv-maxpool-3/W/Assign',\n",
       " 'channel1-conv-maxpool-3/W/read',\n",
       " 'channel1-conv-maxpool-3/Const',\n",
       " 'channel1-conv-maxpool-3/b',\n",
       " 'channel1-conv-maxpool-3/b/Assign',\n",
       " 'channel1-conv-maxpool-3/b/read',\n",
       " 'channel1-conv-maxpool-3/conv',\n",
       " 'channel1-conv-maxpool-3/BiasAdd',\n",
       " 'channel1-conv-maxpool-3/relu',\n",
       " 'channel1-conv-maxpool-3/pool',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/shape',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/mean',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/stddev',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/TruncatedNormal',\n",
       " 'channel0-conv-maxpool-2/truncated_normal/mul',\n",
       " 'channel0-conv-maxpool-2/truncated_normal',\n",
       " 'channel0-conv-maxpool-2/W',\n",
       " 'channel0-conv-maxpool-2/W/Assign',\n",
       " 'channel0-conv-maxpool-2/W/read',\n",
       " 'channel0-conv-maxpool-2/Const',\n",
       " 'channel0-conv-maxpool-2/b',\n",
       " 'channel0-conv-maxpool-2/b/Assign',\n",
       " 'channel0-conv-maxpool-2/b/read',\n",
       " 'channel0-conv-maxpool-2/conv',\n",
       " 'channel0-conv-maxpool-2/BiasAdd',\n",
       " 'channel0-conv-maxpool-2/relu',\n",
       " 'channel0-conv-maxpool-2/pool',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/shape',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/mean',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/stddev',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/TruncatedNormal',\n",
       " 'channel1-conv-maxpool-4/truncated_normal/mul',\n",
       " 'channel1-conv-maxpool-4/truncated_normal',\n",
       " 'channel1-conv-maxpool-4/W',\n",
       " 'channel1-conv-maxpool-4/W/Assign',\n",
       " 'channel1-conv-maxpool-4/W/read',\n",
       " 'channel1-conv-maxpool-4/Const',\n",
       " 'channel1-conv-maxpool-4/b',\n",
       " 'channel1-conv-maxpool-4/b/Assign',\n",
       " 'channel1-conv-maxpool-4/b/read',\n",
       " 'channel1-conv-maxpool-4/conv',\n",
       " 'channel1-conv-maxpool-4/BiasAdd',\n",
       " 'channel1-conv-maxpool-4/relu',\n",
       " 'channel1-conv-maxpool-4/pool',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/shape',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/mean',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/stddev',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/TruncatedNormal',\n",
       " 'channel0-conv-maxpool-3/truncated_normal/mul',\n",
       " 'channel0-conv-maxpool-3/truncated_normal',\n",
       " 'channel0-conv-maxpool-3/W',\n",
       " 'channel0-conv-maxpool-3/W/Assign',\n",
       " 'channel0-conv-maxpool-3/W/read',\n",
       " 'channel0-conv-maxpool-3/Const',\n",
       " 'channel0-conv-maxpool-3/b',\n",
       " 'channel0-conv-maxpool-3/b/Assign',\n",
       " 'channel0-conv-maxpool-3/b/read',\n",
       " 'channel0-conv-maxpool-3/conv',\n",
       " 'channel0-conv-maxpool-3/BiasAdd',\n",
       " 'channel0-conv-maxpool-3/relu',\n",
       " 'channel0-conv-maxpool-3/pool',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/shape',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/mean',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/stddev',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/TruncatedNormal',\n",
       " 'channel1-conv-maxpool-5/truncated_normal/mul',\n",
       " 'channel1-conv-maxpool-5/truncated_normal',\n",
       " 'channel1-conv-maxpool-5/W',\n",
       " 'channel1-conv-maxpool-5/W/Assign',\n",
       " 'channel1-conv-maxpool-5/W/read',\n",
       " 'channel1-conv-maxpool-5/Const',\n",
       " 'channel1-conv-maxpool-5/b',\n",
       " 'channel1-conv-maxpool-5/b/Assign',\n",
       " 'channel1-conv-maxpool-5/b/read',\n",
       " 'channel1-conv-maxpool-5/conv',\n",
       " 'channel1-conv-maxpool-5/BiasAdd',\n",
       " 'channel1-conv-maxpool-5/relu',\n",
       " 'channel1-conv-maxpool-5/pool',\n",
       " 'concat/axis',\n",
       " 'concat',\n",
       " 'Reshape/shape',\n",
       " 'Reshape',\n",
       " 'dropout/dropout/Shape',\n",
       " 'dropout/dropout/random_uniform/min',\n",
       " 'dropout/dropout/random_uniform/max',\n",
       " 'dropout/dropout/random_uniform/RandomUniform',\n",
       " 'dropout/dropout/random_uniform/sub',\n",
       " 'dropout/dropout/random_uniform/mul',\n",
       " 'dropout/dropout/random_uniform',\n",
       " 'dropout/dropout/add',\n",
       " 'dropout/dropout/Floor',\n",
       " 'dropout/dropout/div',\n",
       " 'dropout/dropout/mul',\n",
       " 'W/Initializer/random_uniform/shape',\n",
       " 'W/Initializer/random_uniform/min',\n",
       " 'W/Initializer/random_uniform/max',\n",
       " 'W/Initializer/random_uniform/RandomUniform',\n",
       " 'W/Initializer/random_uniform/sub',\n",
       " 'W/Initializer/random_uniform/mul',\n",
       " 'W/Initializer/random_uniform',\n",
       " 'W',\n",
       " 'W/Assign',\n",
       " 'W/read',\n",
       " 'output/Const',\n",
       " 'output/b',\n",
       " 'output/b/Assign',\n",
       " 'output/b/read',\n",
       " 'output/L2Loss',\n",
       " 'output/add',\n",
       " 'output/L2Loss_1',\n",
       " 'output/add_1',\n",
       " 'output/logits/MatMul',\n",
       " 'output/logits',\n",
       " 'output/prediction/dimension',\n",
       " 'output/prediction',\n",
       " 'training/Rank',\n",
       " 'training/Shape',\n",
       " 'training/Rank_1',\n",
       " 'training/Shape_1',\n",
       " 'training/Sub/y',\n",
       " 'training/Sub',\n",
       " 'training/Slice/begin',\n",
       " 'training/Slice/size',\n",
       " 'training/Slice',\n",
       " 'training/concat/values_0',\n",
       " 'training/concat/axis',\n",
       " 'training/concat',\n",
       " 'training/Reshape',\n",
       " 'training/Rank_2',\n",
       " 'training/Shape_2',\n",
       " 'training/Sub_1/y',\n",
       " 'training/Sub_1',\n",
       " 'training/Slice_1/begin',\n",
       " 'training/Slice_1/size',\n",
       " 'training/Slice_1',\n",
       " 'training/concat_1/values_0',\n",
       " 'training/concat_1/axis',\n",
       " 'training/concat_1',\n",
       " 'training/Reshape_1',\n",
       " 'training/SoftmaxCrossEntropyWithLogits',\n",
       " 'training/Sub_2/y',\n",
       " 'training/Sub_2',\n",
       " 'training/Slice_2/begin',\n",
       " 'training/Slice_2/size',\n",
       " 'training/Slice_2',\n",
       " 'training/Reshape_2',\n",
       " 'training/Const',\n",
       " 'training/Mean',\n",
       " 'training/mul/x',\n",
       " 'training/mul',\n",
       " 'training/add',\n",
       " 'training/cost/tags',\n",
       " 'training/cost',\n",
       " 'training/global_step/initial_value',\n",
       " 'training/global_step',\n",
       " 'training/global_step/Assign',\n",
       " 'training/global_step/read',\n",
       " 'training/gradients/Shape',\n",
       " 'training/gradients/Const',\n",
       " 'training/gradients/Fill',\n",
       " 'training/gradients/training/add_grad/Shape',\n",
       " 'training/gradients/training/add_grad/Shape_1',\n",
       " 'training/gradients/training/add_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/training/add_grad/Sum',\n",
       " 'training/gradients/training/add_grad/Reshape',\n",
       " 'training/gradients/training/add_grad/Sum_1',\n",
       " 'training/gradients/training/add_grad/Reshape_1',\n",
       " 'training/gradients/training/add_grad/tuple/group_deps',\n",
       " 'training/gradients/training/add_grad/tuple/control_dependency',\n",
       " 'training/gradients/training/add_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/training/Mean_grad/Reshape/shape',\n",
       " 'training/gradients/training/Mean_grad/Reshape',\n",
       " 'training/gradients/training/Mean_grad/Shape',\n",
       " 'training/gradients/training/Mean_grad/Tile',\n",
       " 'training/gradients/training/Mean_grad/Shape_1',\n",
       " 'training/gradients/training/Mean_grad/Shape_2',\n",
       " 'training/gradients/training/Mean_grad/Const',\n",
       " 'training/gradients/training/Mean_grad/Prod',\n",
       " 'training/gradients/training/Mean_grad/Const_1',\n",
       " 'training/gradients/training/Mean_grad/Prod_1',\n",
       " 'training/gradients/training/Mean_grad/Maximum/y',\n",
       " 'training/gradients/training/Mean_grad/Maximum',\n",
       " 'training/gradients/training/Mean_grad/floordiv',\n",
       " 'training/gradients/training/Mean_grad/Cast',\n",
       " 'training/gradients/training/Mean_grad/truediv',\n",
       " 'training/gradients/training/mul_grad/Shape',\n",
       " 'training/gradients/training/mul_grad/Shape_1',\n",
       " 'training/gradients/training/mul_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/training/mul_grad/mul',\n",
       " 'training/gradients/training/mul_grad/Sum',\n",
       " 'training/gradients/training/mul_grad/Reshape',\n",
       " 'training/gradients/training/mul_grad/mul_1',\n",
       " 'training/gradients/training/mul_grad/Sum_1',\n",
       " 'training/gradients/training/mul_grad/Reshape_1',\n",
       " 'training/gradients/training/mul_grad/tuple/group_deps',\n",
       " 'training/gradients/training/mul_grad/tuple/control_dependency',\n",
       " 'training/gradients/training/mul_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/training/Reshape_2_grad/Shape',\n",
       " 'training/gradients/training/Reshape_2_grad/Reshape',\n",
       " 'training/gradients/output/add_1_grad/Shape',\n",
       " 'training/gradients/output/add_1_grad/Shape_1',\n",
       " 'training/gradients/output/add_1_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/output/add_1_grad/Sum',\n",
       " 'training/gradients/output/add_1_grad/Reshape',\n",
       " 'training/gradients/output/add_1_grad/Sum_1',\n",
       " 'training/gradients/output/add_1_grad/Reshape_1',\n",
       " 'training/gradients/output/add_1_grad/tuple/group_deps',\n",
       " 'training/gradients/output/add_1_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/add_1_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/zeros_like',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/PreventGradient',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/ExpandDims',\n",
       " 'training/gradients/training/SoftmaxCrossEntropyWithLogits_grad/mul',\n",
       " 'training/gradients/output/add_grad/Shape',\n",
       " 'training/gradients/output/add_grad/Shape_1',\n",
       " 'training/gradients/output/add_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/output/add_grad/Sum',\n",
       " 'training/gradients/output/add_grad/Reshape',\n",
       " 'training/gradients/output/add_grad/Sum_1',\n",
       " 'training/gradients/output/add_grad/Reshape_1',\n",
       " 'training/gradients/output/add_grad/tuple/group_deps',\n",
       " 'training/gradients/output/add_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/add_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/output/L2Loss_1_grad/mul',\n",
       " 'training/gradients/training/Reshape_grad/Shape',\n",
       " 'training/gradients/training/Reshape_grad/Reshape',\n",
       " 'training/gradients/output/L2Loss_grad/mul',\n",
       " 'training/gradients/output/logits_grad/BiasAddGrad',\n",
       " 'training/gradients/output/logits_grad/tuple/group_deps',\n",
       " 'training/gradients/output/logits_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/logits_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/output/logits/MatMul_grad/MatMul',\n",
       " 'training/gradients/output/logits/MatMul_grad/MatMul_1',\n",
       " 'training/gradients/output/logits/MatMul_grad/tuple/group_deps',\n",
       " 'training/gradients/output/logits/MatMul_grad/tuple/control_dependency',\n",
       " 'training/gradients/output/logits/MatMul_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/AddN',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Shape',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Shape_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/dropout/dropout/mul_grad/mul',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Sum',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Reshape',\n",
       " 'training/gradients/dropout/dropout/mul_grad/mul_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Sum_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/Reshape_1',\n",
       " 'training/gradients/dropout/dropout/mul_grad/tuple/group_deps',\n",
       " 'training/gradients/dropout/dropout/mul_grad/tuple/control_dependency',\n",
       " 'training/gradients/dropout/dropout/mul_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/AddN_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/Shape',\n",
       " 'training/gradients/dropout/dropout/div_grad/Shape_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/BroadcastGradientArgs',\n",
       " 'training/gradients/dropout/dropout/div_grad/RealDiv',\n",
       " 'training/gradients/dropout/dropout/div_grad/Sum',\n",
       " 'training/gradients/dropout/dropout/div_grad/Reshape',\n",
       " 'training/gradients/dropout/dropout/div_grad/Neg',\n",
       " 'training/gradients/dropout/dropout/div_grad/RealDiv_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/RealDiv_2',\n",
       " 'training/gradients/dropout/dropout/div_grad/mul',\n",
       " 'training/gradients/dropout/dropout/div_grad/Sum_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/Reshape_1',\n",
       " 'training/gradients/dropout/dropout/div_grad/tuple/group_deps',\n",
       " 'training/gradients/dropout/dropout/div_grad/tuple/control_dependency',\n",
       " 'training/gradients/dropout/dropout/div_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/Reshape_grad/Shape',\n",
       " 'training/gradients/Reshape_grad/Reshape',\n",
       " 'training/gradients/concat_grad/Rank',\n",
       " 'training/gradients/concat_grad/mod',\n",
       " 'training/gradients/concat_grad/Shape',\n",
       " 'training/gradients/concat_grad/ShapeN',\n",
       " 'training/gradients/concat_grad/ConcatOffset',\n",
       " 'training/gradients/concat_grad/Slice',\n",
       " 'training/gradients/concat_grad/Slice_1',\n",
       " 'training/gradients/concat_grad/Slice_2',\n",
       " 'training/gradients/concat_grad/Slice_3',\n",
       " 'training/gradients/concat_grad/Slice_4',\n",
       " 'training/gradients/concat_grad/Slice_5',\n",
       " 'training/gradients/concat_grad/tuple/group_deps',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_2',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_3',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_4',\n",
       " 'training/gradients/concat_grad/tuple/control_dependency_5',\n",
       " 'training/gradients/channel0-conv-maxpool-1/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-3/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-2/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-4/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-3/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-5/pool_grad/MaxPoolGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-1/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-3/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-2/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-4/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-3/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-5/relu_grad/ReluGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-1/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-2/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-4/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-3/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/BiasAddGrad',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-5/BiasAdd_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Shape',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Shape_1',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-1/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Shape',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Shape_1',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-3/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Shape',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Shape_1',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-2/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Shape',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Shape_1',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-4/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Shape',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Shape_1',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel0-conv-maxpool-3/conv_grad/tuple/control_dependency_1',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Shape',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Conv2DBackpropInput',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Shape_1',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/Conv2DBackpropFilter',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/tuple/group_deps',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/tuple/control_dependency',\n",
       " 'training/gradients/channel1-conv-maxpool-5/conv_grad/tuple/control_dependency_1',\n",
       " 'training/beta1_power/initial_value',\n",
       " 'training/beta1_power',\n",
       " 'training/beta1_power/Assign',\n",
       " 'training/beta1_power/read',\n",
       " 'training/beta2_power/initial_value',\n",
       " 'training/beta2_power',\n",
       " 'training/beta2_power/Assign',\n",
       " 'training/beta2_power/read',\n",
       " 'training/zeros',\n",
       " 'channel0-conv-maxpool-1/W/Adam',\n",
       " 'channel0-conv-maxpool-1/W/Adam/Assign',\n",
       " 'channel0-conv-maxpool-1/W/Adam/read',\n",
       " 'training/zeros_1',\n",
       " 'channel0-conv-maxpool-1/W/Adam_1',\n",
       " 'channel0-conv-maxpool-1/W/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-1/W/Adam_1/read',\n",
       " 'training/zeros_2',\n",
       " 'channel0-conv-maxpool-1/b/Adam',\n",
       " 'channel0-conv-maxpool-1/b/Adam/Assign',\n",
       " 'channel0-conv-maxpool-1/b/Adam/read',\n",
       " 'training/zeros_3',\n",
       " 'channel0-conv-maxpool-1/b/Adam_1',\n",
       " 'channel0-conv-maxpool-1/b/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-1/b/Adam_1/read',\n",
       " 'training/zeros_4',\n",
       " 'channel1-conv-maxpool-3/W/Adam',\n",
       " 'channel1-conv-maxpool-3/W/Adam/Assign',\n",
       " 'channel1-conv-maxpool-3/W/Adam/read',\n",
       " 'training/zeros_5',\n",
       " 'channel1-conv-maxpool-3/W/Adam_1',\n",
       " 'channel1-conv-maxpool-3/W/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-3/W/Adam_1/read',\n",
       " 'training/zeros_6',\n",
       " 'channel1-conv-maxpool-3/b/Adam',\n",
       " 'channel1-conv-maxpool-3/b/Adam/Assign',\n",
       " 'channel1-conv-maxpool-3/b/Adam/read',\n",
       " 'training/zeros_7',\n",
       " 'channel1-conv-maxpool-3/b/Adam_1',\n",
       " 'channel1-conv-maxpool-3/b/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-3/b/Adam_1/read',\n",
       " 'training/zeros_8',\n",
       " 'channel0-conv-maxpool-2/W/Adam',\n",
       " 'channel0-conv-maxpool-2/W/Adam/Assign',\n",
       " 'channel0-conv-maxpool-2/W/Adam/read',\n",
       " 'training/zeros_9',\n",
       " 'channel0-conv-maxpool-2/W/Adam_1',\n",
       " 'channel0-conv-maxpool-2/W/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-2/W/Adam_1/read',\n",
       " 'training/zeros_10',\n",
       " 'channel0-conv-maxpool-2/b/Adam',\n",
       " 'channel0-conv-maxpool-2/b/Adam/Assign',\n",
       " 'channel0-conv-maxpool-2/b/Adam/read',\n",
       " 'training/zeros_11',\n",
       " 'channel0-conv-maxpool-2/b/Adam_1',\n",
       " 'channel0-conv-maxpool-2/b/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-2/b/Adam_1/read',\n",
       " 'training/zeros_12',\n",
       " 'channel1-conv-maxpool-4/W/Adam',\n",
       " 'channel1-conv-maxpool-4/W/Adam/Assign',\n",
       " 'channel1-conv-maxpool-4/W/Adam/read',\n",
       " 'training/zeros_13',\n",
       " 'channel1-conv-maxpool-4/W/Adam_1',\n",
       " 'channel1-conv-maxpool-4/W/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-4/W/Adam_1/read',\n",
       " 'training/zeros_14',\n",
       " 'channel1-conv-maxpool-4/b/Adam',\n",
       " 'channel1-conv-maxpool-4/b/Adam/Assign',\n",
       " 'channel1-conv-maxpool-4/b/Adam/read',\n",
       " 'training/zeros_15',\n",
       " 'channel1-conv-maxpool-4/b/Adam_1',\n",
       " 'channel1-conv-maxpool-4/b/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-4/b/Adam_1/read',\n",
       " 'training/zeros_16',\n",
       " 'channel0-conv-maxpool-3/W/Adam',\n",
       " 'channel0-conv-maxpool-3/W/Adam/Assign',\n",
       " 'channel0-conv-maxpool-3/W/Adam/read',\n",
       " 'training/zeros_17',\n",
       " 'channel0-conv-maxpool-3/W/Adam_1',\n",
       " 'channel0-conv-maxpool-3/W/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-3/W/Adam_1/read',\n",
       " 'training/zeros_18',\n",
       " 'channel0-conv-maxpool-3/b/Adam',\n",
       " 'channel0-conv-maxpool-3/b/Adam/Assign',\n",
       " 'channel0-conv-maxpool-3/b/Adam/read',\n",
       " 'training/zeros_19',\n",
       " 'channel0-conv-maxpool-3/b/Adam_1',\n",
       " 'channel0-conv-maxpool-3/b/Adam_1/Assign',\n",
       " 'channel0-conv-maxpool-3/b/Adam_1/read',\n",
       " 'training/zeros_20',\n",
       " 'channel1-conv-maxpool-5/W/Adam',\n",
       " 'channel1-conv-maxpool-5/W/Adam/Assign',\n",
       " 'channel1-conv-maxpool-5/W/Adam/read',\n",
       " 'training/zeros_21',\n",
       " 'channel1-conv-maxpool-5/W/Adam_1',\n",
       " 'channel1-conv-maxpool-5/W/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-5/W/Adam_1/read',\n",
       " 'training/zeros_22',\n",
       " 'channel1-conv-maxpool-5/b/Adam',\n",
       " 'channel1-conv-maxpool-5/b/Adam/Assign',\n",
       " 'channel1-conv-maxpool-5/b/Adam/read',\n",
       " 'training/zeros_23',\n",
       " 'channel1-conv-maxpool-5/b/Adam_1',\n",
       " 'channel1-conv-maxpool-5/b/Adam_1/Assign',\n",
       " 'channel1-conv-maxpool-5/b/Adam_1/read',\n",
       " 'training/zeros_24',\n",
       " 'W/Adam',\n",
       " 'W/Adam/Assign',\n",
       " 'W/Adam/read',\n",
       " 'training/zeros_25',\n",
       " 'W/Adam_1',\n",
       " 'W/Adam_1/Assign',\n",
       " 'W/Adam_1/read',\n",
       " 'training/zeros_26',\n",
       " 'output/b/Adam',\n",
       " 'output/b/Adam/Assign',\n",
       " 'output/b/Adam/read',\n",
       " 'training/zeros_27',\n",
       " 'output/b/Adam_1',\n",
       " 'output/b/Adam_1/Assign',\n",
       " 'output/b/Adam_1/read',\n",
       " 'training/Adam/learning_rate',\n",
       " 'training/Adam/beta1',\n",
       " 'training/Adam/beta2',\n",
       " 'training/Adam/epsilon',\n",
       " 'training/Adam/update_channel0-conv-maxpool-1/W/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-1/b/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-3/W/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-3/b/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-2/W/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-2/b/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-4/W/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-4/b/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-3/W/ApplyAdam',\n",
       " 'training/Adam/update_channel0-conv-maxpool-3/b/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-5/W/ApplyAdam',\n",
       " 'training/Adam/update_channel1-conv-maxpool-5/b/ApplyAdam',\n",
       " 'training/Adam/update_W/ApplyAdam',\n",
       " 'training/Adam/update_output/b/ApplyAdam',\n",
       " 'training/Adam/mul',\n",
       " 'training/Adam/Assign',\n",
       " 'training/Adam/mul_1',\n",
       " 'training/Adam/Assign_1',\n",
       " 'training/Adam/update',\n",
       " 'training/Adam/value',\n",
       " 'training/Adam',\n",
       " 'Merge/MergeSummary',\n",
       " 'init/NoOp',\n",
       " 'init/NoOp_1',\n",
       " 'init',\n",
       " 'save/Const',\n",
       " 'save/SaveV2/tensor_names',\n",
       " 'save/SaveV2/shape_and_slices',\n",
       " 'save/SaveV2',\n",
       " 'save/control_dependency',\n",
       " 'save/RestoreV2/tensor_names',\n",
       " 'save/RestoreV2/shape_and_slices',\n",
       " 'save/RestoreV2',\n",
       " 'save/Assign',\n",
       " 'save/RestoreV2_1/tensor_names',\n",
       " 'save/RestoreV2_1/shape_and_slices',\n",
       " 'save/RestoreV2_1',\n",
       " 'save/Assign_1',\n",
       " 'save/RestoreV2_2/tensor_names',\n",
       " 'save/RestoreV2_2/shape_and_slices',\n",
       " 'save/RestoreV2_2',\n",
       " 'save/Assign_2',\n",
       " 'save/RestoreV2_3/tensor_names',\n",
       " 'save/RestoreV2_3/shape_and_slices',\n",
       " 'save/RestoreV2_3',\n",
       " 'save/Assign_3',\n",
       " 'save/RestoreV2_4/tensor_names',\n",
       " 'save/RestoreV2_4/shape_and_slices',\n",
       " 'save/RestoreV2_4',\n",
       " 'save/Assign_4',\n",
       " 'save/RestoreV2_5/tensor_names',\n",
       " 'save/RestoreV2_5/shape_and_slices',\n",
       " 'save/RestoreV2_5',\n",
       " 'save/Assign_5',\n",
       " 'save/RestoreV2_6/tensor_names',\n",
       " 'save/RestoreV2_6/shape_and_slices',\n",
       " 'save/RestoreV2_6',\n",
       " 'save/Assign_6',\n",
       " 'save/RestoreV2_7/tensor_names',\n",
       " 'save/RestoreV2_7/shape_and_slices',\n",
       " 'save/RestoreV2_7',\n",
       " 'save/Assign_7',\n",
       " 'save/RestoreV2_8/tensor_names',\n",
       " 'save/RestoreV2_8/shape_and_slices',\n",
       " 'save/RestoreV2_8',\n",
       " 'save/Assign_8',\n",
       " 'save/RestoreV2_9/tensor_names',\n",
       " 'save/RestoreV2_9/shape_and_slices',\n",
       " 'save/RestoreV2_9',\n",
       " 'save/Assign_9',\n",
       " 'save/RestoreV2_10/tensor_names',\n",
       " 'save/RestoreV2_10/shape_and_slices',\n",
       " 'save/RestoreV2_10',\n",
       " 'save/Assign_10',\n",
       " 'save/RestoreV2_11/tensor_names',\n",
       " 'save/RestoreV2_11/shape_and_slices',\n",
       " 'save/RestoreV2_11',\n",
       " 'save/Assign_11',\n",
       " 'save/RestoreV2_12/tensor_names',\n",
       " 'save/RestoreV2_12/shape_and_slices',\n",
       " 'save/RestoreV2_12',\n",
       " 'save/Assign_12',\n",
       " 'save/RestoreV2_13/tensor_names',\n",
       " 'save/RestoreV2_13/shape_and_slices',\n",
       " 'save/RestoreV2_13',\n",
       " 'save/Assign_13',\n",
       " 'save/RestoreV2_14/tensor_names',\n",
       " 'save/RestoreV2_14/shape_and_slices',\n",
       " 'save/RestoreV2_14',\n",
       " 'save/Assign_14',\n",
       " 'save/RestoreV2_15/tensor_names',\n",
       " 'save/RestoreV2_15/shape_and_slices',\n",
       " 'save/RestoreV2_15',\n",
       " 'save/Assign_15',\n",
       " 'save/RestoreV2_16/tensor_names',\n",
       " 'save/RestoreV2_16/shape_and_slices',\n",
       " 'save/RestoreV2_16',\n",
       " 'save/Assign_16',\n",
       " 'save/RestoreV2_17/tensor_names',\n",
       " 'save/RestoreV2_17/shape_and_slices',\n",
       " 'save/RestoreV2_17',\n",
       " 'save/Assign_17',\n",
       " 'save/RestoreV2_18/tensor_names',\n",
       " 'save/RestoreV2_18/shape_and_slices',\n",
       " 'save/RestoreV2_18',\n",
       " 'save/Assign_18',\n",
       " 'save/RestoreV2_19/tensor_names',\n",
       " 'save/RestoreV2_19/shape_and_slices',\n",
       " 'save/RestoreV2_19',\n",
       " 'save/Assign_19',\n",
       " 'save/RestoreV2_20/tensor_names',\n",
       " 'save/RestoreV2_20/shape_and_slices',\n",
       " 'save/RestoreV2_20',\n",
       " 'save/Assign_20',\n",
       " 'save/RestoreV2_21/tensor_names',\n",
       " 'save/RestoreV2_21/shape_and_slices',\n",
       " 'save/RestoreV2_21',\n",
       " 'save/Assign_21',\n",
       " 'save/RestoreV2_22/tensor_names',\n",
       " 'save/RestoreV2_22/shape_and_slices',\n",
       " 'save/RestoreV2_22',\n",
       " 'save/Assign_22',\n",
       " 'save/RestoreV2_23/tensor_names',\n",
       " 'save/RestoreV2_23/shape_and_slices',\n",
       " 'save/RestoreV2_23',\n",
       " 'save/Assign_23',\n",
       " 'save/RestoreV2_24/tensor_names',\n",
       " 'save/RestoreV2_24/shape_and_slices',\n",
       " 'save/RestoreV2_24',\n",
       " 'save/Assign_24',\n",
       " 'save/RestoreV2_25/tensor_names',\n",
       " 'save/RestoreV2_25/shape_and_slices',\n",
       " 'save/RestoreV2_25',\n",
       " 'save/Assign_25',\n",
       " 'save/RestoreV2_26/tensor_names',\n",
       " 'save/RestoreV2_26/shape_and_slices',\n",
       " 'save/RestoreV2_26',\n",
       " 'save/Assign_26',\n",
       " 'save/RestoreV2_27/tensor_names',\n",
       " 'save/RestoreV2_27/shape_and_slices',\n",
       " 'save/RestoreV2_27',\n",
       " 'save/Assign_27',\n",
       " 'save/RestoreV2_28/tensor_names',\n",
       " 'save/RestoreV2_28/shape_and_slices',\n",
       " 'save/RestoreV2_28',\n",
       " 'save/Assign_28',\n",
       " 'save/RestoreV2_29/tensor_names',\n",
       " 'save/RestoreV2_29/shape_and_slices',\n",
       " 'save/RestoreV2_29',\n",
       " 'save/Assign_29',\n",
       " 'save/RestoreV2_30/tensor_names',\n",
       " 'save/RestoreV2_30/shape_and_slices',\n",
       " 'save/RestoreV2_30',\n",
       " 'save/Assign_30',\n",
       " 'save/RestoreV2_31/tensor_names',\n",
       " 'save/RestoreV2_31/shape_and_slices',\n",
       " 'save/RestoreV2_31',\n",
       " 'save/Assign_31',\n",
       " 'save/RestoreV2_32/tensor_names',\n",
       " 'save/RestoreV2_32/shape_and_slices',\n",
       " 'save/RestoreV2_32',\n",
       " 'save/Assign_32',\n",
       " 'save/RestoreV2_33/tensor_names',\n",
       " 'save/RestoreV2_33/shape_and_slices',\n",
       " 'save/RestoreV2_33',\n",
       " 'save/Assign_33',\n",
       " 'save/RestoreV2_34/tensor_names',\n",
       " 'save/RestoreV2_34/shape_and_slices',\n",
       " 'save/RestoreV2_34',\n",
       " 'save/Assign_34',\n",
       " 'save/RestoreV2_35/tensor_names',\n",
       " 'save/RestoreV2_35/shape_and_slices',\n",
       " 'save/RestoreV2_35',\n",
       " 'save/Assign_35',\n",
       " 'save/RestoreV2_36/tensor_names',\n",
       " 'save/RestoreV2_36/shape_and_slices',\n",
       " 'save/RestoreV2_36',\n",
       " 'save/Assign_36',\n",
       " 'save/RestoreV2_37/tensor_names',\n",
       " 'save/RestoreV2_37/shape_and_slices',\n",
       " 'save/RestoreV2_37',\n",
       " 'save/Assign_37',\n",
       " 'save/RestoreV2_38/tensor_names',\n",
       " 'save/RestoreV2_38/shape_and_slices',\n",
       " 'save/RestoreV2_38',\n",
       " 'save/Assign_38',\n",
       " 'save/RestoreV2_39/tensor_names',\n",
       " 'save/RestoreV2_39/shape_and_slices',\n",
       " 'save/RestoreV2_39',\n",
       " 'save/Assign_39',\n",
       " 'save/RestoreV2_40/tensor_names',\n",
       " 'save/RestoreV2_40/shape_and_slices',\n",
       " 'save/RestoreV2_40',\n",
       " 'save/Assign_40',\n",
       " 'save/RestoreV2_41/tensor_names',\n",
       " 'save/RestoreV2_41/shape_and_slices',\n",
       " 'save/RestoreV2_41',\n",
       " 'save/Assign_41',\n",
       " 'save/RestoreV2_42/tensor_names',\n",
       " 'save/RestoreV2_42/shape_and_slices',\n",
       " 'save/RestoreV2_42',\n",
       " 'save/Assign_42',\n",
       " 'save/RestoreV2_43/tensor_names',\n",
       " 'save/RestoreV2_43/shape_and_slices',\n",
       " 'save/RestoreV2_43',\n",
       " 'save/Assign_43',\n",
       " 'save/RestoreV2_44/tensor_names',\n",
       " 'save/RestoreV2_44/shape_and_slices',\n",
       " 'save/RestoreV2_44',\n",
       " 'save/Assign_44',\n",
       " 'save/RestoreV2_45/tensor_names',\n",
       " 'save/RestoreV2_45/shape_and_slices',\n",
       " 'save/RestoreV2_45',\n",
       " 'save/Assign_45',\n",
       " 'save/restore_all/NoOp',\n",
       " 'save/restore_all/NoOp_1',\n",
       " 'save/restore_all']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.get_default_graph()\n",
    "[n.name for n in graph.as_graph_def().node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata & test set\n",
    "\n",
    "check whether the loaded graph computes correctly with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Summary:\n",
      "Train: Total Positive Labels=1750 (0.1421)\n",
      "Test: Total Positive Labels=309 (0.1421)\n",
      "\n",
      "dataset passed the assertion test\n"
     ]
    }
   ],
   "source": [
    "from data.hybrid import load_data_from_file\n",
    "\n",
    "(x_train, y_train, x_test, y_test, initW, vocab) = load_data_from_file(\"racism_final2_binary\")\n",
    "word_text_len = x_train[0][\"word\"].shape[0]\n",
    "word_vocab_size = len(vocab.vocabulary_)\n",
    "char_text_len = x_train[0][\"char\"].shape[0]\n",
    "char_vocab_size = x_train[0][\"char\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.hybrid import extract_from_batch\n",
    "\n",
    "batchW, batchC = extract_from_batch(x_test)\n",
    "feed_dict = {\"input/labels:0\": y_test, \"input/X_word:0\": batchW, \"input/X_char:0\": batchC, \"dropout_keep_prob:0\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = sess.run(\"output/prediction:0\", feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=0.7006 recall=0.8026 f1=0.7481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model.helper import calculate_metrics\n",
    "precision, recall, f1 = calculate_metrics(y_test, pred)\n",
    "print(\"precision=%.4f recall=%.4f f1=%.4f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the metrics are same as the final output, we can validate that the pre-trained model has been loaded successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the graph to compute softmax prob & entropy\n",
    "use entropy loss to measure the uncertainty to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = graph.get_tensor_by_name(\"output/logits:0\")\n",
    "softmax_prob = tf.nn.softmax(logits, name=\"softmax\")\n",
    "entropy = tf.reduce_sum(tf.scalar_mul(-1, tf.multiply(softmax_prob, tf.log(softmax_prob))) ,axis=1, name=\"entropy\")\n",
    "\n",
    "n_candidates = tf.placeholder(tf.int32, name=\"n_candidates\")\n",
    "get_candidates = tf.nn.top_k(entropy, n_candidates, name=\"candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict.update({n_candidates: 20})\n",
    "prob, candidates = sess.run([softmax_prob, get_candidates], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = candidates.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50059927,  0.49940071],\n",
       "       [ 0.49695459,  0.50304538],\n",
       "       [ 0.49438867,  0.50561136],\n",
       "       [ 0.49303412,  0.50696588],\n",
       "       [ 0.49082008,  0.50917995],\n",
       "       [ 0.48928291,  0.51071709],\n",
       "       [ 0.51123738,  0.48876265],\n",
       "       [ 0.48819947,  0.51180053],\n",
       "       [ 0.48716697,  0.51283306],\n",
       "       [ 0.51528043,  0.4847196 ],\n",
       "       [ 0.48431545,  0.5156846 ],\n",
       "       [ 0.48359284,  0.51640719],\n",
       "       [ 0.48174015,  0.51825988],\n",
       "       [ 0.48026633,  0.51973373],\n",
       "       [ 0.48009303,  0.51990694],\n",
       "       [ 0.47959855,  0.52040136],\n",
       "       [ 0.47808233,  0.5219177 ],\n",
       "       [ 0.52640969,  0.47359025],\n",
       "       [ 0.47341239,  0.52658761],\n",
       "       [ 0.52788436,  0.47211567]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the most uncertain (probabilities near 0.5) samples are chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load new unlabelled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unlabelled = pd.read_csv('./data/crawled/unlabelled/racism_tweets.tsv',\n",
    "                     sep=\"\\t\",\n",
    "                     header=None,\n",
    "                     skiprows=[0],\n",
    "                     names=[\"Tweet_ID\", \"Text\", \"Previous\"],\n",
    "                     error_bad_lines=False)\n",
    "unlabelled = unlabelled.drop_duplicates(subset=[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1340</td>\n",
       "      <td>1339</td>\n",
       "      <td>1.003000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1304</td>\n",
       "      <td>1339</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>#IslamIsTheProblem</td>\n",
       "      <td>@BreitbartLondon Sadiq Khan is a fifth columni...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.742637e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.794854e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.297241e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.280868e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.843121e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.747524e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.454298e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tweet_ID                                               Text  \\\n",
       "count                 1340                                               1339   \n",
       "unique                1304                                               1339   \n",
       "top     #IslamIsTheProblem  @BreitbartLondon Sadiq Khan is a fifth columni...   \n",
       "freq                    19                                                  1   \n",
       "mean                   NaN                                                NaN   \n",
       "std                    NaN                                                NaN   \n",
       "min                    NaN                                                NaN   \n",
       "25%                    NaN                                                NaN   \n",
       "50%                    NaN                                                NaN   \n",
       "75%                    NaN                                                NaN   \n",
       "max                    NaN                                                NaN   \n",
       "\n",
       "            Previous  \n",
       "count   1.003000e+03  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean    2.742637e+17  \n",
       "std     3.794854e+17  \n",
       "min     3.297241e+06  \n",
       "25%     4.280868e+08  \n",
       "50%     2.843121e+09  \n",
       "75%     7.747524e+17  \n",
       "max     8.454298e+17  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#blackLivesMatter is just a way to HATE white people. Shouldnt #WhiteLivesMatter cuz ALL lives do? /', '#blacklivesmatter ???? Uh.... to who??? Not ME...... #WHITElivesmatter ^', '#blacklivesmatter ???? Uh.... to who??? Not ME...... #WHITElivesmatter ;;', 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots/', '.@scott_usmc @SouthFlaVet @CNNPolitics Is his life a \"#WhiteLivesMatter a  #BluelivesMatter or a: #BlackLivesMatter or an: #AllLivesMatter?:', '@JackPosobiec @HURRICANEPAUL #WhiteGenocide Wake Up, Fight Back!', 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots|', '#blacklivesmatter ???? Uh.... to who??? Not ME...... #WHITElivesmatter *', '@TacoSalad85 @lporiginalg @YouTube Smash the Matriarchy 😂', '845396808943702016']\n"
     ]
    }
   ],
   "source": [
    "texts = list(unlabelled[\"Text\"])\n",
    "print(texts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove too frequent tags from previous dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_filtered = list(filter(lambda x: not str(x).isdigit(), texts))\n",
    "final_filtered = list(filter(lambda x: len(str(x).split(\" \")) > 3, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1140"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./data/crawled/racism_to_be_labelled.tsv\", \"w\") as f:\n",
    "    for tweet in final_filtered[:800]:\n",
    "        f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "random.shuffle(final_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "half_index = int(len(final_filtered)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604\n",
      "604\n"
     ]
    }
   ],
   "source": [
    "pool_random_sampling = final_filtered[:half_index]\n",
    "pool_uncertainty_sampling = final_filtered[half_index:]\n",
    "print(len(pool_random_sampling))\n",
    "print(len(pool_uncertainty_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"./data/crawled/unlabelled/racism_random.tsv\"):\n",
    "    with open(\"./data/crawled/unlabelled/racism_random.tsv\", \"w\") as f:\n",
    "        for line in pool_random_sampling:\n",
    "            f.write(str(line) + \"\\n\")\n",
    "    print(\"Saved file\")\n",
    "else:\n",
    "    print(\"load from file\")\n",
    "    pool_random_sampling = []\n",
    "    with open(\"./data/crawled/unlabelled/racism_random.tsv\", \"r\") as f:\n",
    "        for line in f:\n",
    "            pool_random_sampling.append(line.rstrip())\n",
    "    print(len(pool_random_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"./data/crawled/unlabelled/racism_uncertain.tsv\"):\n",
    "    with open(\"./data/crawled/unlabelled/racism_uncertain.tsv\", \"w\") as f:\n",
    "        for line in pool_uncertainty_sampling:\n",
    "            f.write(str(line) + \"\\n\")\n",
    "    print(\"Saved file\")\n",
    "else:\n",
    "    print(\"load from file\")\n",
    "    pool_uncertainty_sampling = []\n",
    "    with open(\"./data/crawled/unlabelled/sexism_uncertain.tsv\", \"r\") as f:\n",
    "        for line in f:\n",
    "            pool_uncertainty_sampling.append(line.rstrip())\n",
    "    print(len(pool_uncertainty_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "democrat_tweets.tsv   republican_tweets.tsv  unlabelled_data_analysis.ipynb\r\n",
      "racism_random.tsv     sexism_random.tsv      youtube1.csv\r\n",
      "racism_tweets.tsv     sexism_tweets.tsv      youtube2.csv\r\n",
      "racism_uncertain.tsv  sexism_uncertain.tsv   youtube3.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/crawled/unlabelled/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare tsv for labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selected N samples from pool_random_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_samples = random.sample(pool_random_sampling, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to feed pool_uncertainty_sampling into the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.preprocess import preprocess_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n",
      "569\n",
      "['to ignore sweden is to invite the same here. bansharia &amp; endislamnow', 'terror islam needs a reformation. islamistheproblem terrorism notlonewolf', 'i understand your situation. you wanted to defend your cult but your peaceful brothers always fail you', 'listenin to strip it down makes me wanna throw more country that sounds like the white version of r&amp;b on my phone whitelivesmatter', \"frog avi, whitelivesmatter in the bio. dude, it's not worth it. save your blood pressure, mute &amp; let them scream into the ether\", 'so you are defending a jew hating +murdering sand nazi, wtg jezebel, you whorish retards stopislam', 'try islam, his religion, + you win the prize, lol stopislam', 'saying that 3 times makes you thrice the idiot 😝', \"silly point for india is a deadset cheat, moving on to the pitch as the delivery is in the air... and we're the cheats! racecard indvaus\", 'what a absolute surprise! london islamistheproblem rockvillerape isis isisisgay']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = list(map(lambda x:preprocess_tweet(str(x)), pool_uncertainty_sampling))\n",
    "valid_tweets = []\n",
    "valid_tweets_preprocessed = []\n",
    "for i, tweet in enumerate(preprocessed):\n",
    "    if tweet:\n",
    "        valid_tweets.append(pool_uncertainty_sampling[i])\n",
    "        valid_tweets_preprocessed.append(tweet)\n",
    "print(len(valid_tweets))\n",
    "print(len(valid_tweets_preprocessed))\n",
    "print(valid_tweets_preprocessed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.char import text_to_1hot_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 140, 70)\n"
     ]
    }
   ],
   "source": [
    "pool_char = np.array(list(map(lambda x: text_to_1hot_matrix(str(x)), valid_tweets_preprocessed)))\n",
    "print(pool_char.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.word import load_data_from_file as load_vocabulary\n",
    "from data.tokenizer import tokenize_with_dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, _, x_test, _, _, vocab = load_vocabulary(\"racism_final2_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = list(map(lambda x: tokenize_with_dictionary(x ,vocab.vocabulary_._mapping.keys()), valid_tweets_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['to', 'ignore', 'sweden', 'is', 'to', 'invite', 'the', 'same', 'here', 'ban', 'sharia', 'end', 'islam', 'now'], ['terror', 'islam', 'needs', 'a', 'reformation', 'islam', 'is', 'the', 'problem', 'terrorism', 'not', 'lonewolf'], ['i', 'understand', 'your', 'situation', 'you', 'wanted', 'to', 'defend', 'your', 'cult', 'but', 'your', 'peaceful', 'brothers', 'always', 'fail', 'you'], ['listen', 'in', 'to', 'strip', 'it', 'down', 'makes', 'me', 'wanna', 'throw', 'more', 'country', 'that', 'sounds', 'like', 'the', 'white', 'version', 'of', 'r', 'b', 'on', 'my', 'phone', 'white', 'lives', 'matter'], ['frog', 'avi', 'white', 'lives', 'matter', 'in', 'the', 'bio', 'dude', \"it's\", 'not', 'worth', 'it', 'save', 'your', 'blood', 'pressure', 'mute', 'let', 'them', 'scream', 'into', 'the', 'ether'], ['so', 'you', 'are', 'defending', 'a', 'jew', 'hating', 'murdering', 'sand', 'nazi', 'wtg', 'jezebel', 'you', 'whorish', 'retards', 'stop', 'islam'], ['try', 'islam', 'his', 'religion', 'you', 'win', 'the', 'prize', 'lol', 'stop', 'islam'], ['saying', 'that', '3', 'times', 'makes', 'you', 'thrice', 'the', 'idiot'], ['silly', 'point', 'for', 'india', 'is', 'a', 'dead', 'set', 'cheat', 'moving', 'on', 'to', 'the', 'pitch', 'as', 'the', 'delivery', 'is', 'in', 'the', 'air', 'and', \"we're\", 'the', 'cheats', 'race', 'card', 'in', 'dva', 'us'], ['what', 'a', 'absolute', 'surprise', 'london', 'islam', 'is', 'the', 'problem', 'rockville', 'rape', 'isis', 'is', 'isis', 'gay']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert x_test.shape[1] >= max([len(x) for x in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 35)\n"
     ]
    }
   ],
   "source": [
    "joined = list(map(lambda x: \" \".join(x), tokenized))\n",
    "pool_word = np.array(list(vocab.fit_transform(joined)))\n",
    "print(pool_word.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(valid_tweets)\n",
    "unlabelled_feed_dict = {\"input/labels:0\": np.zeros(n).reshape(n, 1), \"input/X_word:0\": pool_word, \"input/X_char:0\": pool_char, \"dropout_keep_prob:0\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabelled_feed_dict.update({n_candidates: 100})\n",
    "_prob, _candidates = sess.run([softmax_prob, get_candidates], unlabelled_feed_dict)\n",
    "idx = _candidates.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_tweets = np.array(valid_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@IraqiSecurity @Franjota I hope they get the bastard #stopIslam 0.728043\n",
      "Close your borders England, France, Germany and Italy! Stop being politically correct! It's killing you. #LondonTerrorAttack #NoRefugees 0.732504\n",
      "@Tmchakra1 What another SATANIC LIE. #NOMUSLIMUSA #NOSHARIA #BANALLMUSLIMS #ALLAHSUX 0.733567\n",
      "Don't #PrayForLondon fight back against the worst religion in the world, #StopIslam 0.883715\n",
      "Don't be in the herd of fools believing the \"Radical Islam\" lie. Stop and THINK... It's not that hard to do.  0.882549\n",
      "@AsadAli2047 Your country executes blasphemers, Islamofascist savage #StopIslam #NotAllMuslims 0.863361\n",
      "@DeplorableKatie What an OXYMORON. NO MUSLIMS ARE AMERICANS. #NOMUSLIMUSA #NOSHARIA #BANALLMUSLIMS #ALLAHSUX 0.762895\n",
      "@fitzscrounger @tfowkes02 @Channel4News deflect,deflect,deflect, thats all you have huh, he who denies #IslamIsTheProblem is the loon #Facts 0.833966\n",
      "@BateZen @TRobinsonNewEra Sort of blows your 'one singer' analogy right out of the water, doesn't it? #StopIslam #KhalidMasood 0.897361\n",
      "50 Million Muslims are prepared to use violence, We must Deislamize in order to survive. LESS ISLAM = MORE PEACE AND FREEDOM #STOPIslam 0.846596\n",
      "@WomaninHavana Man was non practicising Muslim, but dont let #Facts get int he way of your narrative eh? #IslamIsTheProblem,stop being cucks 0.926067\n",
      "#PrayForLondon to find their testicles and fast. #ukpoli #uspoli #cdnpoli #stopislam #fuckislam #nukemecca 0.955205\n",
      "@LeaaKeren So you support them having to cover, being raped, abused, stoned, etc. Feminism stands up for #WomensRightsNotWrongs #NoSharia 0.800629\n",
      "@lbc #lbc @KTHopkins I feel much MORE concern that he WAS British-born!! Means RADICALISATION ingrained in Britain's society! #STOPISLAM 0.897024\n",
      "@ABCNews24 @JohnBlaxland1 what they want is to take over the entire world with their sharia law &amp; death to all those that say no. #NOSHARIA 0.73933\n",
      "This is a spiritual battle. There is no political freedom without spiritual freedom. #StopIslam #BanIslam 0.865617\n",
      "DONALD TRUMP &amp; QUINN are both the biggest bitches to walk this earth 😀 fuck both of you racist bitches 😘😘 #NiggaLove oh wait he said #Nigger 0.783435\n",
      "@JustinTrudeau @RuthRogul look at France &amp; the refugee mess! #norefugees 0.769854\n",
      "@ealDonaldTrumpp I guess he should be licking Muzzy ass like Obama would've done #StopIslam not #PrayForLondonِ 0.977831\n",
      "@_ThatGuyT agreed, prejudice is not, righteous indignation most certainly IS, #IslamIsTheProblem, no excuses, no excetions... 0.781268\n",
      "@StefanMolyneux I have some words... #StopIslam #deportation 0.832301\n",
      "@FambrasBrasil Islamic state approve the terrorist attack in London. What did you talk about this? #NoSharia #StopSharia #NoMulticulturalism 0.94171\n",
      "@realDonaldTrump Before he enter on the Execution of his Office, he shall take the following #NoRefugees 0.965202\n",
      "@realDonaldTrump amendment to the Constitution by the legislatures of the several States, as #NoRefugees 0.758239\n",
      "@DonnieWahlberg @kirstiealley yeah that helps. #StopIslam #StopIslamInUS 0.778855\n",
      "What does #realchange mean to you @JustinTrudeau  \"open honest government\" listen to cnd's that oppose #m103 and Islam ideology #nosharia 0.815328\n",
      "Just had a black dude say \" suck my dick cracka ass\" #whitelivesmatter #stopthehate 0.863779\n",
      "@Political_Trump the white,cis privileged man is looking at the dying person,the muslim is looking at her phone #StopIslam 0.757043\n",
      "@Refinery29 @ChelseaClinton what about protecting Muslim women from being murdered by their men or they have no rights? Morons! #StopIslam 0.965539\n",
      "@DonaldJTrumpJr Khan was quoted incorrectly. What he meant: \"major muslim city\". #StopIslam @Independent 0.943691\n",
      "@CM2ALAW He should have wasted every stinking one of them. #Nomuslimusa #nosharia #banallmuslims #ALLAHSUX 0.927956\n",
      "According to dhimmis @DougSaunders the London Muslim terrorists turned into an infidel immediately after 1st kill #notallmuslims #StopIslam 0.754626\n",
      "Justin Trudeau is George Soros's puppet. He hates Canada and Canadians! #IslamIsTheProblem. 0.796102\n",
      "@Avron_p @JackCodini now you can say Canada is racist. Kill all the gays, Jews, infidels &amp; apostates! Allah Akbar! Inshallah! #nosharia 0.809208\n",
      "@DaveChappelle are you aware Mohammad had black slaves? #stopislam 0.987212\n",
      "The ignorance of people tweeting #stopislam ... How about #stopisis??? Terrorism has NO religion !! 0.949208\n",
      "@TheRebelTV @FaithGoldy you should list the names of the MPs who voted this bill and let them pay when better times will come .#STOPISLAM 0.76544\n",
      ".@marianorajoy @fhollande @theresa_may @AngelaMerkelCDU @PaoloGentiloni  @Pontifex Protect #Europe freedom &amp; europeans' lives #stopislam 0.884408\n",
      "@tyrone345345 @tariqnasheed Fuck me you talk some shit. Doesn't matter what colour  a terrorist is, a terrorist is a terrorist  0.895762\n",
      "@foxnation The British government needs to end Sharia,No-go Zones in London and anywhere else in the UK #NoSharia  0.966167\n",
      "Critical Islamic facts to help you debate. @PoliticalIslam must be defeated. #STOPIslam #BillWarnerPhD #MakeDCListen #BanSharia 0.95831\n",
      "Without war #Islam is not going anywhere. #Mohammad had been successful with terror always remember that. #BanIslam #StopIslam 0.818134\n",
      "@realDonaldTrump The Congress shall have Power to dispose of and make all needful Rules and #NoRefugees 0.75734\n",
      "#Khalidmasood vile #britishborn #muslim, Shows us,#Muslims will NEVER integrate in UK. How many more must die before we act? #NoMoreMosques 0.877812\n",
      "@DougSaunders Traitor coward, weakling and appeaser, a grade b hack at best #StopIslam 0.853394\n",
      "@realDonaldTrump transmit within four days to the President pro tempore of the Senate and the #NoRefugees 0.895538\n",
      "@frenchiefrance And what drives the disturbed individual? A hateful ideology that allows no other. And #appeasement of it 0.843119\n",
      "@ThePapaganda too bad the perp was a Muslim just like the other 30000 attacks. #StopIslam 0.777022\n",
      "#BanIslam  #NoSharia 0.853122\n",
      "@TEN_GOP Islam is a cancer on the world. #BanIslam #BanShariaLaw #NoMoreMosques #MoreMosquesMoreProblems 0.759954\n",
      "@IanPTrait #Islam is Against #Hindu #Atheist #Buddhist #Minorities #Democracy #Christians SO wtf else ppl need to Get it?!! #STOPIslam 0.7506\n",
      "@foxnation @Priebus I agree like #NoRefugees #NoSharia &amp; a real reduction of America's Population Rate #AmericaFirst #MakeAmericaGreatAgain 0.848741\n",
      "@Daily_Express Islam is the one and only Problem here - any questions? #stopislam #banislam 0.777326\n",
      "Yo #racist @SpikeLee ur boy #colinkaepernick had choice to sit, teams have choice not to hire the ahole #racecard #GodBlessAmerica 0.763027\n",
      "@CTVNews these 3 are disgusting.  If they immigrated here with their nonsense Sharia mindset.. they should be deported. #NOSHARIA 0.752005\n",
      "So the Canadian government is protecting one religion by taking away Canadian rights!! #freespeech #ourrights #stopislam 0.928026\n",
      "Sovereign Nation = Borders, Language &amp; Culture. Lose any one of these you lose the nation.  #BuildThatWall #NoSharia 0.737314\n",
      "I do not permit a woman to teach or to have authority over a man, she must be silent. #stopislam 0.903022\n",
      "@AngeloJohnGage @INTJutsu He'd be the HEAD of #ISIS! Time to #StopIslam #PrayForLondon #LondonAttack #Westminster 0.786306\n",
      "@michaelkeyes @AatTimothy F-in Savages they are the scourge of the earth. #stopIslam #protectIsrael  #proIsrael 0.971365\n",
      "@KletoKate I understand what you mean but freedom, liberties, human rights &amp; free speech are worth fighting for. #NoSharia @Franktmcveety 0.804956\n",
      "@MacleansMag The West will continue to pay an every increasing price for pretending #Islam is just like any other religion. #StopIslam 0.850577\n",
      "@foxandfriends @StacyBrewer18 How foolish! #NoSharia #FreeLeonardPeltier 0.834309\n",
      "Stop saying #stopislam or #banislam will you #banchristianity and #banjews ? you might as well #BanReligions 0.933316\n",
      "@KlamoSandy  I tried, Twitter had other plans! Lets try it again!  #NoRefugees   @Melissa_AFV 0.847422\n",
      "#London #Westminster Islam is not a religion nor peace. It is hate, death, conquest, submission. Islam has no place in the West. #StopIslam 0.910794\n",
      "@Refinery29 No it isn't liar. Those women can be beaten, raped or murdered in many Islamic places if they don't wear them. #StopIslam 0.929382\n",
      "Islam=Hate  2 Buddhists, Jewish, Hindu, Christian, Atheists, Minorities. and Says Muslims are Superior.  #StopIslam #WorlduniteagainstIslam 0.843742\n",
      "@S_T_O_P_TERROR @MAGA_SAF2 Such hatred Muslims have for us. #NoRefugees 0.763199\n",
      ".@jihadwatchRS  was he part of #MuslimBrotherhood #Shariah police? #IslamIstheProblem #ShariahIstheProblem #QuranIstheProblem #Islam #Jihad 0.784574\n",
      "@TEN_GOP @TheNotoriousXAV these retards need a lobotomy! #stopislam sad things is I use to believe in every word he is saying... 0.871131\n",
      "@DeadofKnight68 SENSOR MUSLIMS WORLDWIDE. #NOMUSLIMUSA #NOSHARIA #BANALLMUSLIMS #ALLAHSUX 0.74341\n",
      "@benthepoet The religion \"of love and peace\" Europe 4 europeans. #stopislam 0.930868\n",
      "@iNina_17 The real crime is the thousands upon thousands that die at the hands of radical islam. #stopislam 0.741312\n",
      "@Priya_d92 I don't fear being killed by Muslim terrorists. I fear Sharia and forced religious compliance. #NoSharia 0.869269\n",
      "@EdSherwood7 the liberals are beyond belief in their stupidity. 🤒😧#Muslim #BanIslam #Corrupt #NoSharia 0.897062\n",
      "@lynnemorvirat @CarolHusband @horseneverlies @themadsloth I feel bad for the people of Canada. #NoSharia #NoJihadInAmerica 0.844285\n",
      "@DaydreamerTMZJ more useless celebrity twaddle ignore adele  #PrayForLondon no #StopIslam 0.975677\n",
      "@VP @RealityCallsCo CLOSE THE FUCKING BORDERS completely. NOW #london #bansharia #nosharia #NoRefugees End this now. 0.781718\n",
      "Should all muslim in UK be deport and ban islam? #Islam #NoSharia #antiislam #fuckislam #londonattack #UKIP #UK #noburka #Wilders #MAGA #EU 0.911909\n",
      "Stop the provocation of the Muslims: they curse 17 times a day! @chad_rustle #StopIslamisation #StopIslam #IslamHorsDEurope 0.754971\n",
      "@asamjulian @BrittPettibone they have to go back #sendthemback #noRefugees 0.868691\n",
      "@ShaunKing @realDonaldTrump Nice to see U are defending the criminals as usual Sean. Ur R the enemy within #StopIslam #Dindunuffin 0.863473\n",
      "There is no place for barbarity in a free and democratic society,there is no place for islam. #stopislam #banislam 0.94057\n",
      "Is he an #Imam?            @reallyo1 #AntiAmerican #AntiIslam #NoSharia #Daesh #ISIS #RadicalIslam #IslamicState #Infidel #StopMuslimBigots 0.984344\n",
      "Stop the fucking import of islam to europe for Fuck sake !! #PrayforUK #PrayForLondonِ #stopislam 0.927998\n",
      "@SamanthaClarkH  NEVER EVER WOULD I ACCEPT THAT CRAP FOR MY CHILD. SOMEONE WOULD BE HURTING IF THEY DID.  #NOMUSLIMUSA #NOSHARIA 0.736447\n",
      "@SweetCharityS HOLY SHIT! These SICK FUCKS!!! They take our kindness FOR cowardice. I've SMARTENED UP, now our GOVT MUST! #STOPIslam #MAGA 0.935825\n",
      "@AgendaOfEvil wake up Australia #stopislam #Banislam #islamistheracist #islamisthefascist #LeavetheUN #Pray4MuslimBan 0.869498\n",
      "Time 4 Americans to stand up for our Judeo Christian culture's &amp; be Patriotic again. In Montana they have to pass Senate Bill 97 #NOSHARIA 0.817976\n",
      "@MacleansMag There are no questions. The answer is already very clear to anyone who cares: #StopIslam  0.946172\n",
      "@BrurellSue @realclearisrael @iamIqraKhalid  Tragically, your concern is well warranted.   #NoSharia  #USA  #Canada   #Israel  #LeavetheUN 0.748535\n",
      "@SteveHewittUK Try Islam, his religion, + you win the prize, lol #StopIslam 0.798176\n",
      "Sharia Law and  Jihadi Extremists are the problem. #NoSharia #extremism 0.868736\n",
      "@TheMeemStreams muslims can't spell women's rights #Stopislam (by the way, @Twitter has pulled \"stopislam\" from it's hashtags 0.893933\n",
      "@Anni_Cyrus Hello, Thanks for the real news. Media Known to lie! #StopIslam 0.851292\n",
      "@Jezebel So you are defending a Jew hating +murdering sand Nazi, wtg Jezebel, you whorish retards #sTopISlam 0.889348\n",
      "@bobfox321 Heeelll NO! No sharia, no Muzzies #STOPIslam #PoliticalIslam #CSPI 0.976868\n",
      "@PoliticalKathy Absolutely agree Kathy.  Awesome synopsis of the reality of Islam.  #BanIslam #NoSharia 0.845592\n",
      "@VisitAbuDhabi only if we can come to you and can't everything to suit us. #nosharia 0.861159\n"
     ]
    }
   ],
   "source": [
    "positive_prob = _prob[:, 1]\n",
    "top_pos_idx = np.argpartition(positive_prob, -100)[-100:]\n",
    "for i in top_pos_idx:\n",
    "    print(\"%s %s\" % (valid_tweets[i], _prob[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like them model is biased on certain hashtags like #notallmen #gamergate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the most uncertain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"Oh but aren't these the poor women and children they want us to welcome with open arms?? \\U0001f644 #NoRefugees\"\n",
      " '#Muslim controlled area of #Luton celebrates #londonattack #NoGoZone #Bury #IslamIsTheProblem #BanIslam #Deport #Pedophiles Promises more...'\n",
      " \"Is saying that Arabs used white women as sex slaves for 100's of years xenophobic? Cuz it is true #slavery #slaves #racism #STOPIslam\"\n",
      " '@RealGauharAli  the main reason behind terrorism is the existence of people like you #stopislam @Trufflesmum'\n",
      " 'Khalid Masood aka Adrian Elms a dindu, converted to Islam after being jailed for stabbing a man in the face. #StopIslam #BlackLiesMatter'\n",
      " '@FloEshalomi @SadiqKhan @cllrbray Go back to Middle East and spread your Islam spawn there! #STOPIslam'\n",
      " '@ryyma12 Sorry you liar. ISIS is 100% Islam #PrayForLondonِ #StopIslam'\n",
      " \"@UNRefugeeAgency @TheCut The refugee crisis is nothing more than another Islamic invasion of the West. That's the truth! #NoRefugees\"\n",
      " '@zakSkybrush2 your stats are as wrong as your point, Dhimmis #NothingToDoWithIslam #StopIslam'\n",
      " '@MilitarisCath too much is too much trop ç est trop #stopislam'\n",
      " '@washingtonpost you can achieve that only if you #BanTheBurka #BanTheMullah Ban #FGM. Destroy #radical #islamists. #Trump'\n",
      " 'Hey Black people stop killing us!!! #WhiteLivesMatter'\n",
      " '@theonejrf @PaulineHansonOz Islamaphobia- a term created by facists, and used by cowards, to manipulate morons, #IslamIsTheProblem'\n",
      " \"@AshleyJudd You're kidding, right? Women's rights should NOT include submit to laws where women are ruled by men. #NoSharia\"\n",
      " 'Tolerance is good, but it can go to far.  Like tolerating terrorism, polygamy, mutilation and slavery. #NoSharia  #IslamPromotesPeace'\n",
      " '@cocosbentmind @RodStryker More proof, if any is needed, that Islam rejects even the possibility of any laws except sharia. #NoSharia'\n",
      " 'Another peaceful and compassionate Muslim terrorist doing hateful things in #London. Hate Preacher Abu Izzadeen. #WakeUp #IslamIsTheProblem'\n",
      " '@RetireTheLeft that is y I am so glad that hood rat is gone #STOPIslam'\n",
      " '@michelekirkBPR That is so ludicrous #yestoban #nosharia #keepamericagreat 🇺🇸'\n",
      " '@ANOMALY1 @TRobinsonNewEra thanks for the retweet twitter no longer availabed TRobisons other fact charged video #IslamIsTheProblem'\n",
      " '@cbatson941 Invasion force ready to drain the welfare systems and rape European women! Should go back and fight! #NoRefugees'\n",
      " '@ABSCBNNews bombing, killings brought to you by the religion of piss, islam, the curse of humanity #islamistheproblem'\n",
      " 'Quran 2:191: \"Kill them wherever you find them\" There is nothing worst than being taught \"peace\" when U R just a child... \" #StopIslam pic.t'\n",
      " \"3 million Muslims in the UK and #IslamIsTheProblem REALLY? I don't see the Logic. \"\n",
      " '@IAmBirmingham Aggressive political ideology with a misplaced superiority complex. Contributes nothing to GB. #stopIslam #Westminster'\n",
      " '@AshleyWarrior my daughters life is more important to me than the feelings of #libtards islam = death cult #banislam #nosharia #neversubmit'\n",
      " 'IN THE STU BOUT TO COOK WITH @TonyJGuapo 🔩 FUCK SLEEP #nigger'\n",
      " \"@TEN_GOP @mikewright1997 No,@POTUS  isn't. We need 2 STOP THIS CULT ISLAM B4 this happens to USA!  #banIslam #BanSHARIA #banMuslims #MAGA\"\n",
      " '#BanSharia #StopIslam from gaining footholds in non-muslim countries'\n",
      " \"@SonofLiberty357 The religion of peace, preaching it's peaceful message again. #BanTheBurka\"\n",
      " \"@AmyRae666 THIS. Things that ain't RACES but COUNTRIES/RELIGIONS get called RACES now(like Morocco in trial vs @geertwilderspvv). #StopIslam\"\n",
      " '@EylonALevy So to prove his point he mows ppl down &amp; stabs an officer. Yeah, we get it. #IslamisEVIL #BANISLAM #ISLAMISTHEPROBLEM'\n",
      " \"@TRobinsonNewEra Keep telling them Tommy. Hopefully they'll start to listen. You're a warrior! #StopIslam #PrayForLondon #WakeUpAmerica\"\n",
      " 'How of will you write #PrayForLondon or other places? Enough mourning prayers! #Pray for #victory and #FightBack #WakeUpAmerica #StopIslam'\n",
      " '@canarymission @JoHnnY_Boiiiiii Why are these muslim pigs in my country? #Deport #BuildThatWall #BanIslam #IslamIsTheProblem #NoRefugees'\n",
      " '@theonejrf ...exactly #IslamIsTheProblem,the induvidual MUSLIM who choses or has imposed upon them Islam must choose secular values OR Quran'\n",
      " \"Yeah, muslims KILL DOGS … DON'T THEY… There is NO #MuslimWomensDay! What a sham! #IslamIsTheProblem  😡 @_Makada_ @DorAnnCecil\"\n",
      " 'From the commonsense people of America to the commonsense conservatives in #Britain, we stand with you. #GodBlessLondon #StopIslam'\n",
      " \"@EmpireFOX Wtf Anika! You killed Rhonda, doesn't anyone remember this!? #whitelivesmatter\"\n",
      " '@BBCBreaking @BBCNews Islamic Terrorist suporting Merkel should be executed for the destruction of Europe.  #NoIslam #IslamIsTheProblem'\n",
      " '@9NewsAUS #nosharia ban them'\n",
      " '@RanaAyyub Something has happened in London.Stand against #islamic_terrorism London mayor is an agent of #ISIS #IslamIsTheProblem'\n",
      " 'U know u r a bag of crap if ur main concern after the latest Islamic terror attack is to fret about Islamophobia #StoPIslam #PrayforLondon'\n",
      " '@TIME #IslamIsTheProblem, stop lying you cowards, your cowardice is causing harm, dont you see that?'\n",
      " '@TRobinsonNewEra @SamvanRooy1 OMFG. This is so MEAN. #stopislam'\n",
      " '@stephenasmith @FirstTake Again another #Milking of the #RaceCard by @espn I hope @Disney will stop this #Evil tirade from this #Racist'\n",
      " '@AmyMek No crime, just an \"Honor Killing\" gone awry! #BanSharia wipe it off the face of the earth!'\n",
      " '@realDonaldTrump committed; but when not committed within any State, the Trial shall be at such #NoRefugees'\n",
      " 'Islam is a cult and a terrorist org not a religion. Worship criminal. Advocates pedophilia, torture, murder. #pedogate #obamagate #nosharia'\n",
      " '@BasedVet @jabenjoh Happy #muslimwomensday Let us behead some infidels #islam =death #allahakbar =SeigHeil #BanSHARIA #BanHijab #NoRefugees'\n",
      " '@BBCBreaking Fucking Muslim scum, and the left wingers continue to bury their heads in the sand. #IslamIsTheProblem'\n",
      " 'Very Religious #Muslim code 4 Suicide #Terrorist, \"Moderate #Muslim\" code 4 #Muslim who straps them into the belt. #IslamIsTheProblem'\n",
      " '@Sporty1546 Eradication of islam would go a long way towards world peace. All who have their eyes wide open -see.  #StopIslam #LondonAttack'\n",
      " \"@Independent Funny that it's only women standing in the picture haha, the ones who don't have a say in their own ''religion' #banislam\"\n",
      " '🔥 MT \"@realmcstanson Throw that #Islamist pack out if you want to save your last bit of honor, #UK. #LondonAttacks #StopIslam\"'\n",
      " \"#BanSharia almost all are unaware of what islam is they just copy paste From1 to1.6 bn doesn't come bykilling someone there's something init\"\n",
      " '@KTHopkins Quran is the manual of the devil #Stopislam'\n",
      " '.@sean_spicier @MNHockeymama There are No lone wolf #Islamic killers. 2b Muslim is to believe t/tenets of t/book of hate. #Islamistheproblem'\n",
      " '@Politic2020 @JeanEBraveaux The @POTUS knew you know. I bet the wall has gone up another 10 feet. #DrainTheSwamp #BuildTheWall #MAGA'\n",
      " '@mikandynothem And if a muslim woman is ever nominated, the question will be, \"What size justice burka do you wear?\" #StopIslam'\n",
      " '@RitaPanahi @BryanSeymour1 @7NewsSydney There are many more that fear what they fled will once again become their future. #IslamIsTheProblem'\n",
      " '@BorisJohnson Islam is a political system designed to conquer, not a religion Politicians R whores 4 Islamic insanity #NoSharia #BanIslam'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots^'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots.'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots!!'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots..'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots;;'\n",
      " '@Rokra_R #IslamIsTheProblem, Fuck your pedo prophet,'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots*'\n",
      " 'BLM, Crooked Hillary, Obama are the REAL RACISTS!!!! #WhiteLivesMatter #BlueLivesMatter #FuckIdiots?!'\n",
      " \"@AP @jongambrellAP  He has trademarks in Egypt, Saudi Arabia, Turkey &amp; UAE. It's not a concidence that none of these are in the #MuslimBan\"\n",
      " '@FranceNews24 @au_peuple Brought to you by #Islam the religion of tolerance and #peace as said by @BarackObama #IslamIsTheProblem'\n",
      " 'How many more innocent people have to die before the violent backward ideology is rejected in the modern western world? #StopIslam'\n",
      " '@JackPosobiec @nooneishere51 Those two 💩💩 words can really piss every kind of nationalist off. #NoRefugees #MuslimBan #NoIslam'\n",
      " '.@TRobinsonNewEra - Keep fighting for your country and against the plague of islamist terror. We support you!!! #STOPIslam'\n",
      " '@greencane654 / Its one or the other #BanSharia #StopIslam or live with more senseless murder &amp; torture .'\n",
      " '#HangAyazNizami But Islamophobia is bad \\U0001f914 STOP. DEFENDING. TERRORISTS. #IslamIsTheProblem'\n",
      " '@StockMonsterUSA @deplorablezilla Islam is not a religion nor peace. Islam is hate, death, conquest, submission. #BanIslam #StopIslam'\n",
      " \"@SamiahSaid He would only want freedom for you and all people. You don't have freedom even though you think you do. #bansharia\"\n",
      " \"I have no problem w/ a muslim woman choosing to wear a hijab, but she shouldn't have vagina seared shut, or be stoned, for not! #NoSharia\"\n",
      " '@USAAssociation @ReadTheHornNews @Starbucks should have stayed out of politics..hope your stock crashes and burns...#NoRefugees #VetsFirst'\n",
      " '@Inthemainewoods @mary122514 Disgusting #NoRefugees'\n",
      " \"@themadsloth @ISupport_Israel  It's evident that Canada's survival is at stake.  With the U.S. in great danger as well.   #NoSharia\"\n",
      " '@FoxNews @CristobalJAlex @TheJusticeDept Immigration Reform is to Deport ALL Illegals, Secure the Border, &amp; Stop #WhiteGenocide.'\n",
      " 'London #LondonAttack Another minister of the religion peace inflicts #jihad on infidels #BanSHARIA #TravelBan #NoRefugees #AllahAkbar =DEATH'\n",
      " \"LET'S GET THIS SHIT TRENDING! #WhiteLivesMatter @lildismiss\"\n",
      " '@MayorofLondon Islam is not a religion of peace. Islam is hate, death, conquest, submission. Islam is not Civilization. #BanIslam #StopIslam'\n",
      " '@ARnews1936 @OnlineMagazin Geez #NoRefugees'\n",
      " \"@TEN_GOP May not be the same #Terrorist but remember it's the same Demonic Islamic Ideology! 👿#PrayForLondon #NoSharia #IslamicState #RNRTN\"\n",
      " '#Islam is not a religion nor peaceful. Islam is hate, death, conquest, submission. Islam does not want to coexist, but dominate. #StopIslam'\n",
      " \"@91Gulfvet @zoombouse @Lrihendry They are spreading it like Rosie O'Donnell does. It is time to #NOMUSLIMUSA #nosharia #banallmuslims\"\n",
      " 'Radical Islamists? Islam IS radical. #BanShariaLaw #BanIslam #NoMoreMosques #MoreMosquesMoreProblems'\n",
      " '@AmyMek Absolutely sickening to see western leaders bowing down to this sick ideology!  #StopIslam'\n",
      " '@AmyMek WAKE UP AND START STANDING UP TO THE NEIGHBORS ALLOWED TO SHIT ON YOUR LAWN. '\n",
      " '@PaulCarmichaelV @GrrrGraphics Why does London look like Pakistan instead of the Capital of England? #brexit #deportthemall #NoRefugees'\n",
      " '@MissLizzyNJ Why does London look like Pakistan instead of the Capital of England? #brexit #deportthemall #NoRefugees'\n",
      " 'Jeff #Sessions cited the 2-yr-old case of a woman shot dead by an illegal #immigrant #deported 5 times as an argument for #deporting. #Duh!'\n",
      " \"Instead of using the hashtag #NoRefugees, maybe we should be more truthful and start shouting #NoBarbarians! That's what they are.\"\n",
      " \"@ABC @newsbusters He's absolutely right. The man is destroying London as a city in which to live or to visit. #StopIslam\"]\n"
     ]
    }
   ],
   "source": [
    "candidate_tweets = valid_tweets[idx]\n",
    "print(candidate_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine both and make into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_idx = np.concatenate((idx, top_pos_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "active_learning_samples = valid_tweets[combined_idx] \n",
    "total_tweets = [(1, tweet) for tweet in active_learning_samples] + [(2, tweet) for tweet in random_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle the tweets when labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.shuffle(total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./data/crawled/racism_to_be_labelled.tsv\", \"w\") as f:\n",
    "    for i, tweet in total_tweets:\n",
    "        f.write(\"%s\\t%s\\n\" % (i, tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
