{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from data.word import save_word_cnn\n",
    "from data.preprocess import load_from_file\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"loading pretrained embedding\")\n",
    "pretrained_word2vec = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_format = \"cv-%s\"\n",
    "\n",
    "data = []\n",
    "for i in range(10):\n",
    "    data.append(load_from_file(name_format % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[[\"you're\", 'really', 'fond', 'of', 'lying', 'about', 'everything', 'i', 'said', 'they', \"aren't\", 'isis'], ['tara', 'loves', 'cats', 'oh', 'my', 'god', 'i', 'can', 'not', 'handle', 'one', 'more', 'ep', 'of', 'kat', 'and', 'andre', 'they', 'are', 'cooking', 'my', 'nut', 'big', 'time', 'mkr'], ['if', 'adam', 'wins', 'he', 'can', 'use', 'the', 'money', 'to', 'pay', 'his', 'kid', 'deadbeat', 'mkr'], ['the', 'moron', 'baghdadi', 'thinks', 'he', 'can', 'throw', 'undermanned', 'forces', 'out', 'there', 'on', 'offensives', 'and', 'allah', 'will', 'win', 'it', 'for', 'him', 'kirkuk'], ['i', \"can't\", 'even', 'find', 'the', 'words', 'to', 'describe', 'how', 'much', 'i', 'value', 'the', 'work', 'of', 'all', 'the', 'people', 'ive', 'met', 'during', 'this', 'time', 'how', 'much', 'i', 'love', 'all', 'of', 'them'], ['oh', 'that', 'was', 'a', 'typo', 'guys', 'for', 'dessert', 'you', 'get', 'sorbent', \"you'll\", 'need', 'it', 'after', 'the', 'spatchcock', 'mkr'], ['so', 'i', 'just', 'basically', 'started', 'mkr', 'why', 'are', 'they', 'serving', 'chicken', 'livers', 'vomit'], ['i', 'am', 'allergic', 'to', 'cats', 'but', 'thanks', 'anyways', ':)'], ['i', 'have', 'a', 'bunch', 'of', 'bm', 'already', 'but', 'not', 'swims', '>', '>'], ['your', 'caliph', 'ran', 'from', 'battle', 'at', 'g', 'wer', 'like', 'the', 'pussy', 'he', 'is', 'he', \"can't\", 'possible', 'have', 'faith', 'in', 'allah', 'and', 'be', 'such', 'a', 'coward']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['yup', 'pp', 'jason', \"doesn't\", 'have', 'a', 'lot', 'of', 'experience', 'on', 'twitter', 'it', 'seems', 'or', 'a', 'very', 'healthy', 'world', 'view'], ['and', 'there', 'is', 'good', 'evidence', 'of', 'staging', 'events', 'like', 'the', 'moscow', 'theater', 'takeover', 'and', 'the', 'beslan', 'school', 'attacks'], ['well', 'let', 'me', 'tell', 'you', 'how', 'this', 'looks', 'throwing', 'another', 'woman', 'under', 'the', 'bus', 'to', 'suck', 'up', 'to', 'a', 'shitty', 'dude', 'while', 'also'], ['a', 'little', 'concerned', 'at', 'what', 'is', 'happening', 'here', 'uh', 'this', 'story', 'may', 'not', 'be', 'over'], ['did', 'you', 'mean', 'false', 'equivalence'], ['add', 'a', 'filter', 'to', 'toggle', 'showing', 'phone', 'verified', 'accts', 'in', 'clients', 'requires', 'dev', 'policy', 'update', 'natch'], ['i', \"wouldn't\", 'want', 'to', 'deconstruct', 'my', 'lemon', 'tart', 'mkr'], ['im', 'betting', 'that', 'not', 'a', 'single', 'idiot', 'at', 'salon', 'has', 'ever', 'read', 'the', 'quran', 'and', 'hadiths'], ['are', 'you', 'upset', 'about', 'to', 'nights', 'elimination', 'result', 'mkr'], ['there', 'was', 'no', 'random', 'bombardment', 'with', 'chemical', 'weapons', 'the', 'isis', 'chemical', 'weapons', 'were', 'blown', 'up']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    save_word_cnn(data[i], name_format % i, pretrained_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "aa8682ba1f82413b8d4b497762f25f8d": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
