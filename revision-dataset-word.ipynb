{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from data.word import save_word_cnn\n",
    "from data.preprocess import load_from_file\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"loading pretrained embedding\")\n",
    "pretrained_word2vec = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_format = \"fcv-%s\"\n",
    "\n",
    "data = []\n",
    "for i in range(10):\n",
    "    data.append(load_from_file(name_format % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['haha', 'kate', 'wiped', 'the', 'smile', 'off', 'your', 'face', 'too', 'mkr'], ['mkr', 'mighty', 'killer', 'rats', 'eat', 'out', 'the', 'kitchen', 'a', 'us', 'media', 'oz', 'rant'], ['oh', 'god', 'all', 'these', 'people', 'that', 'i', \"don't\", 'like', 'that', 'id', 'forgotten', 'about', 'go', 'away', '1000', 'teams', 'left', 'mkr'], ['jailed', 'saudi', 'blogger', 'raif', 'badawi', 'faces', '2nd', 'round', 'of', 'public', 'flogging', '50', 'times', 'over', '20', 'weeks', 'for', 'insulting', 'islam', 'https', ':/'], ['all', 'kat', 'and', 'andre', 'did', 'was', 'cut', 'up', 'bread', 'and', 'layer', 'it', 'with', 'ham', 'and', 'cheese', 'and', 'they', \"can't\", 'even', 'get', 'that', 'right', 'seriously', 'they', 'need', 'to', 'go', 'mkr'], ['im', 'sorry', 'wat'], ['pro', 'death', 'penalty', 'anti', 'some', 'provisions', 'of', 'obamacare', 'fewer', 'govt', 'regulations', 'free', 'market', 'better', 'but'], ['because', 'someone', 'here', \"isn't\", 'reading', 'all', 'the', 'tweets', 'and', \"it's\", 'not', 'me'], ['dr', 'asko', 'drongo', 'mkr'], ['completely', 'disgusted', 'with', 'kat', 'and', 'andres', 'scoring', 'really', 'disappointing', 'mkr']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "Tokenizing tweets with tokenize_for_word2vec\n",
      "\n",
      "[['as', 'long', 'as', 'women', 'have', 'a', 'biological', 'advantage', 'that', 'sport', 'or', 'activity', 'will', 'be', 'undervalued', 'and', 'ridiculed'], ['tell', 'her', 'that', \"it's\", 'a', 'gaming', 'convention', 'so', 'im', 'thinking', 'black', 'milks', 'asteroids', 'dress', 'over', 'leggings', 'with', 'some', 'spikey', 'boots'], ['whoever', 'has', 'the', 'oil', 'in', 'the', 'me', 'if', \"it's\", 'a', 'dictator', 'or', 'terrorist', 'or', 'democracy', 'will', 'always', 'sell', 'as', 'much', 'as', 'they', 'can'], ['people', 'think', 'im', 'a', 'feminazi', 'because', 'i', \"don't\", 'laugh', 'at', 'their', 'jokes'], ['in', 'the', 'meantime', 'the', 'kurds', 'are', 'holding', 'three', 'sides', 'of', 'mosul', 'waiting', 'for', 'the', 'iraqis', 'to', 'get', 'off', 'their', 'butts'], ['so', 'this', 'season', 'of', 'mkr', 'is', 'going', 'to', 'finish', 'in', 'which', 'decade', 'such', 'a', 'drag'], ['sleepytime', 'peach', 'tea', 'with', 'lavender', 'infused', 'honey', 'you', 'complete', 'me'], ['where', 'are', 'the', 'food', 'police', 'colin', 'needs', 'to', 'call', '000', 'mkr'], ['mkr', 'bring', 'it', 'on'], ['aw', 'thanks', 'p']]\n",
      "changed data (18351,) into (18351, 36)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    save_word_cnn(data[i], name_format % i, pretrained_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "7cff7ed3a5924c349e9a95e23437a699": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
