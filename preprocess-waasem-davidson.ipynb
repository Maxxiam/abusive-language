{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.preprocess import load_from_file\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded preprocessed tweets for none:3330\n",
      "loaded preprocessed tweets for abusive:16546\n",
      "loaded preprocessed tweets for none:458\n",
      "loaded preprocessed tweets for abusive:2083\n",
      "loaded preprocessed tweets for none:417\n",
      "loaded preprocessed tweets for abusive:2062\n",
      "loaded preprocessed tweets for none:10209\n",
      "loaded preprocessed tweets for sexism:3152\n",
      "loaded preprocessed tweets for racism:1649\n",
      "loaded preprocessed tweets for none:1276\n",
      "loaded preprocessed tweets for sexism:394\n",
      "loaded preprocessed tweets for racism:206\n",
      "loaded preprocessed tweets for none:1277\n",
      "loaded preprocessed tweets for sexism:394\n",
      "loaded preprocessed tweets for racism:207\n"
     ]
    }
   ],
   "source": [
    "data_d = load_from_file(\"davidson\", [\"none\",\"abusive\"])\n",
    "data_w = load_from_file(\"waasem\", [\"none\",\"sexism\", \"racism\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for data in [data_d, data_w]:\n",
    "    for label in data[\"train\"].keys():\n",
    "        for row in data[\"train\"][label]:\n",
    "            vocab.update(row)\n",
    "            if max_len < len(row):\n",
    "                max_len = len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "count = 0\n",
    "for word in vocab.keys():\n",
    "    if vocab[word] >= min_freq:\n",
    "        count += 1\n",
    "vocab_size = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13857"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_vocab = [\"PAD\", \"UNK\"]\n",
    "for word, _ in vocab.most_common(vocab_size):\n",
    "    _vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = {}\n",
    "word2id = {}\n",
    "for i, word in enumerate(_vocab):\n",
    "    id2word[i] = word\n",
    "    word2id[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "def tokens2id(tokens):\n",
    "    idx = []\n",
    "    for t in tokens[:max_len]:\n",
    "        if t not in word2id.keys():\n",
    "            idx.append(word2id[\"UNK\"])\n",
    "        else:\n",
    "            idx.append(word2id[t])\n",
    "    padding_needed = max_len - len(idx) if max_len > len(idx) else 0\n",
    "    for _ in range(padding_needed):\n",
    "        idx.append(word2id[\"PAD\"])\n",
    "    assert len(idx) == max_len\n",
    "    return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"y'all\", 'gots', 'to', 'take', 'da', 'colored', 'vacation', '.', 'kentucky', 'fried', 'chicken', 'but', 'only', 'da', 'white', 'meat']\n",
      "[197, 3951, 8, 171, 259, 456, 6754, 2, 7540, 1851, 867, 30, 112, 259, 170, 1617, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(data_d[\"train\"][\"none\"][0])\n",
    "print(tokens2id(data_d[\"train\"][\"none\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx_dataset(data):\n",
    "    idx_data = {}\n",
    "    for key in [\"train\", \"valid\", \"test\"]:\n",
    "        idx_data[key] = {}\n",
    "        for label in tqdm(data[key].keys()):\n",
    "            idx_data[key][label] = list(map(tokens2id, data[key][label]))\n",
    "    return idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_d = idx_dataset(data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_w = idx_dataset(data_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_npy(data, name):\n",
    "    file_format = \"./data/word_outputs/%s_%s_%s.npy\"\n",
    "    for key in data.keys():\n",
    "        for label in data[key].keys():\n",
    "            file_name = file_format % (key, label, name)\n",
    "            array = np.array(data[key][label])\n",
    "            np.save(file_name, array)\n",
    "            print(\"Saved in %s. %s\" % (file_name, str(array.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in ./data/word_outputs/train_racism_waasem.npy. (1649, 40)\n",
      "Saved in ./data/word_outputs/train_sexism_waasem.npy. (3152, 40)\n",
      "Saved in ./data/word_outputs/train_none_waasem.npy. (10209, 40)\n",
      "Saved in ./data/word_outputs/test_racism_waasem.npy. (207, 40)\n",
      "Saved in ./data/word_outputs/test_sexism_waasem.npy. (394, 40)\n",
      "Saved in ./data/word_outputs/test_none_waasem.npy. (1277, 40)\n",
      "Saved in ./data/word_outputs/valid_racism_waasem.npy. (206, 40)\n",
      "Saved in ./data/word_outputs/valid_sexism_waasem.npy. (394, 40)\n",
      "Saved in ./data/word_outputs/valid_none_waasem.npy. (1276, 40)\n"
     ]
    }
   ],
   "source": [
    "save_npy(idx_w, \"waasem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in ./data/word_outputs/train_abusive_davidson.npy. (16546, 40)\n",
      "Saved in ./data/word_outputs/train_none_davidson.npy. (3330, 40)\n",
      "Saved in ./data/word_outputs/test_abusive_davidson.npy. (2062, 40)\n",
      "Saved in ./data/word_outputs/test_none_davidson.npy. (417, 40)\n",
      "Saved in ./data/word_outputs/valid_abusive_davidson.npy. (2083, 40)\n",
      "Saved in ./data/word_outputs/valid_none_davidson.npy. (458, 40)\n"
     ]
    }
   ],
   "source": [
    "save_npy(idx_d, \"davidson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./data/word_outputs/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"word2id\": word2id, \"id2word\":id2word}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.char import text_to_1hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_idx_dataset(data):\n",
    "    idx_data = {}\n",
    "    for key in [\"train\", \"valid\", \"test\"]:\n",
    "        idx_data[key] = {}\n",
    "        for label in tqdm(data[key].keys()):\n",
    "            idx_data[key][label] = [text_to_1hot_matrix(\" \".join(row)) for row in data[key][label]]\n",
    "    return idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_idx_d = char_idx_dataset(data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_idx_w = char_idx_dataset(data_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_npy(data, name):\n",
    "    file_format = \"./data/char_outputs/%s_%s_%s.npy\"\n",
    "    for key in data.keys():\n",
    "        for label in data[key].keys():\n",
    "            file_name = file_format % (key, label, name)\n",
    "            array = np.array(data[key][label])\n",
    "            np.save(file_name, array)\n",
    "            print(\"Saved in %s. %s\" % (file_name, str(array.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in ./data/char_outputs/train_abusive_davidson.npy. (16546, 140, 70)\n",
      "Saved in ./data/char_outputs/train_none_davidson.npy. (3330, 140, 70)\n",
      "Saved in ./data/char_outputs/valid_abusive_davidson.npy. (2083, 140, 70)\n",
      "Saved in ./data/char_outputs/valid_none_davidson.npy. (458, 140, 70)\n",
      "Saved in ./data/char_outputs/test_abusive_davidson.npy. (2062, 140, 70)\n",
      "Saved in ./data/char_outputs/test_none_davidson.npy. (417, 140, 70)\n"
     ]
    }
   ],
   "source": [
    "save_npy(char_idx_d, \"davidson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in ./data/char_outputs/train_sexism_waasem.npy. (3152, 140, 70)\n",
      "Saved in ./data/char_outputs/train_none_waasem.npy. (10209, 140, 70)\n",
      "Saved in ./data/char_outputs/train_racism_waasem.npy. (1649, 140, 70)\n",
      "Saved in ./data/char_outputs/valid_sexism_waasem.npy. (394, 140, 70)\n",
      "Saved in ./data/char_outputs/valid_none_waasem.npy. (1276, 140, 70)\n",
      "Saved in ./data/char_outputs/valid_racism_waasem.npy. (206, 140, 70)\n",
      "Saved in ./data/char_outputs/test_sexism_waasem.npy. (394, 140, 70)\n",
      "Saved in ./data/char_outputs/test_none_waasem.npy. (1277, 140, 70)\n",
      "Saved in ./data/char_outputs/test_racism_waasem.npy. (207, 140, 70)\n"
     ]
    }
   ],
   "source": [
    "save_npy(char_idx_w, \"waasem\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "15f673f91d644a608588f7cd4783cb86": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "1a1bcdfc1e924dd09425fe2d73ab2900": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "7fe297efeb634aa990105da5b7b6dd39": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "aa9ed7852832418480f5432a847bdfeb": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "b0a9599c779144d8b843a676b76aa948": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "d382b91fc24142d58e4f48b9088f8176": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e0130f7f79054adfb7bc5c4ab24db4d0": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "fb74c0a7974e440788b192018e30c2fb": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
